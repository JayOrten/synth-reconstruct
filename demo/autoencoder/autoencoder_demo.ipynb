{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from scipy.io import wavfile\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics import R2Score\n",
    "import dawdreamer as daw\n",
    "import numpy \n",
    "from scipy.io import wavfile\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"C:\\\\Users\\\\jayor\\\\Documents\\\\repos\\\\synth-reconstruct\\\\demo\\\\autoencoder\\\\checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDS(Dataset):\n",
    "    def __init__(self, presets_csv_path):\n",
    "        self.presets = pd.read_csv(presets_csv_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.presets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        preset = self.presets.iloc[idx]\n",
    "        audio_path = Path(f'../samples/{preset[0]}.wav')\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spectrogram = librosa.power_to_db(spectrogram, ref=numpy.max)\n",
    "\n",
    "        # Reshape to 128x128\n",
    "        # spectrogram = librosa.util.fix_length(spectrogram, size=128, axis=1)\n",
    "        # spectrogram = librosa.util.fix_length(spectrogram, size=128, axis=0)\n",
    "        \n",
    "        # Save spectrogram image for debugging\n",
    "        # fig, ax = plt.subplots()\n",
    "        # img = librosa.display.specshow(spectrogram, y_axis='mel', x_axis='time', ax=ax)\n",
    "        # print('img: ', img)\n",
    "        # fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        # ax.set(title='Mel spectrogram display')\n",
    "        # # Save to disk\n",
    "        # # create folder if not exists\n",
    "        # Path('./sample_images').mkdir(parents=True, exist_ok=True)\n",
    "        # plt.savefig(f'./sample_images/{preset[0]}.png')\n",
    "\n",
    "        # Add channel dimension\n",
    "        spectrogram = numpy.expand_dims(spectrogram, axis=0)\n",
    "        # Transform to tensors\n",
    "        spectrogram = torch.tensor(spectrogram)\n",
    "        preset = torch.tensor(preset[1:-1])\n",
    "\n",
    "        return spectrogram, preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDS('..\\\\presets.csv')\n",
    "num_train = int(0.9 * len(dataset))\n",
    "num_val = len(dataset) - num_train\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [num_train, num_val])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_20308\\91119843.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  audio_path = Path(f'../samples/{preset[0]}.wav')\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_20308\\91119843.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 87])\n",
      "torch.Size([32, 88])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through first batch to check image dimensions\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model. Adapted from: https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image.\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "        #     act_fn(),\n",
    "        #     nn.Flatten(),  # Image grid to single feature vector\n",
    "        #     nn.Linear(2 * (11264) * c_hid, latent_dim),\n",
    "        # )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2)  # 32x32 => 16x16\n",
    "        self.act1 = act_fn()\n",
    "        self.conv2 = nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1)\n",
    "        self.act2 = act_fn()\n",
    "        self.conv3 = nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2)  # 16x16 => 8x8\n",
    "        self.act3 = act_fn()\n",
    "        self.conv4 = nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1)\n",
    "        self.act4 = act_fn()\n",
    "        self.conv5 = nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2)  # 8x8 => 4x4\n",
    "        self.act5 = act_fn()\n",
    "        self.flatten = nn.Flatten()  # Image grid to single feature vector\n",
    "        self.fc = nn.Linear(2 * (16*11) * c_hid, latent_dim) # 16x11 is the size of the image after the last convolution\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.net(x)\n",
    "        print('initial shape: ', x.shape)\n",
    "        x = self.act1(self.conv1(x))\n",
    "        print('conv1: ', x.shape)\n",
    "        x = self.act2(self.conv2(x))\n",
    "        print('conv2: ', x.shape)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        print('conv3: ', x.shape)\n",
    "        x = self.act4(self.conv4(x))\n",
    "        print('conv4: ', x.shape)\n",
    "        x = self.act5(self.conv5(x))\n",
    "        print('conv5: ', x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print('flatten: ', x.shape)\n",
    "        x = self.fc(x)\n",
    "        print('fc: ', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct.\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * (16 * 11) * c_hid), act_fn())\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 4x4 => 8x8\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "        #     act_fn(),\n",
    "        #     nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "        #     act_fn(),\n",
    "        #     nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "        #     act_fn(),\n",
    "        #     nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),  # 16x16 => 32x32\n",
    "        #     nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        # )\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2)  # 4x4 => 8x8\n",
    "        self.act1 = act_fn()\n",
    "        self.conv2 = nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1)\n",
    "        self.act2 = act_fn()\n",
    "        self.conv3 = nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2)\n",
    "        self.act3 = act_fn()\n",
    "        self.conv4 = nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1)\n",
    "        self.act4 = act_fn()\n",
    "        self.conv5 = nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x decoder initially: ', x.shape)\n",
    "        x = self.linear(x)\n",
    "        print('x decoder linear: ', x.shape)\n",
    "        x = x.reshape(x.shape[0], -1, 16, 11)\n",
    "        print('x decoder reshape: ', x.shape)\n",
    "        # x = self.net(x)\n",
    "        x = self.act1(self.conv1(x))\n",
    "        print('x decoder conv1: ', x.shape)\n",
    "        x = self.act2(self.conv2(x))\n",
    "        print('x decoder conv2: ', x.shape)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        print('x decoder conv3: ', x.shape)\n",
    "        x = self.act4(self.conv4(x))\n",
    "        print('x decoder conv4: ', x.shape)\n",
    "        x = self.conv5(x)\n",
    "        print('x decoder conv5: ', x.shape)\n",
    "        x = self.tanh(x)\n",
    "        print('x decoder tanh: ', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "        num_input_channels: int = 1,\n",
    "        width: int = 128,\n",
    "        height: int = 87,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(1, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        # x will be size 128, 87\n",
    "        # x_hat will be size 128, 88\n",
    "        # To fix this, we will crop x_hat to be the same size as x\n",
    "        x_hat = x_hat[:, :, :128, :87]\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        print('Train loss: ', loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GenerateCallback(Callback):\n",
    "#     def __init__(self, input_imgs, every_n_epochs=1):\n",
    "#         super().__init__()\n",
    "#         self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "#         # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "#         self.every_n_epochs = every_n_epochs\n",
    "\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "#             # Reconstruct images\n",
    "#             input_imgs = self.input_imgs.to(pl_module.device)\n",
    "#             with torch.no_grad():\n",
    "#                 pl_module.eval()\n",
    "#                 reconst_imgs = pl_module(input_imgs)\n",
    "#                 pl_module.train()\n",
    "#             imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "#             grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, value_range=(-1, 1))\n",
    "#             # Save image to disk. TODO: fix\n",
    "#             with tempfile.NamedTemporaryFile(suffix=\".png\") as tmp_file:\n",
    "#                 save_image(grid, tmp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(latent_dim):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=\"C:\\\\Users\\\\jayor\\\\Documents\\\\repos\\\\synth-reconstruct\\\\demo\\\\autoencoder\",\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            # GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    # trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    # trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"sr_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode  | In sizes        | Out sizes      \n",
      "--------------------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 23.2 M | train | [1, 1, 128, 87] | [1, 2048]      \n",
      "1 | decoder | Decoder | 23.2 M | train | [1, 2048]       | [1, 1, 128, 88]\n",
      "--------------------------------------------------------------------------------\n",
      "46.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "46.4 M    Total params\n",
      "185.418   Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape:  torch.Size([1, 1, 128, 87])\n",
      "conv1:  torch.Size([1, 32, 64, 44])\n",
      "conv2:  torch.Size([1, 32, 64, 44])\n",
      "conv3:  torch.Size([1, 64, 32, 22])\n",
      "conv4:  torch.Size([1, 64, 32, 22])\n",
      "conv5:  torch.Size([1, 64, 16, 11])\n",
      "flatten:  torch.Size([1, 11264])\n",
      "fc:  torch.Size([1, 2048])\n",
      "x decoder initially:  torch.Size([1, 2048])\n",
      "x decoder linear:  torch.Size([1, 11264])\n",
      "x decoder reshape:  torch.Size([1, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([1, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([1, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([1, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([1, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([1, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([1, 1, 128, 88])\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_20308\\91119843.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  audio_path = Path(f'../samples/{preset[0]}.wav')\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_20308\\91119843.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 12.92it/s]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/844 [00:00<?, ?it/s] initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  13172156.0\n",
      "Epoch 0:   0%|          | 1/844 [00:00<07:14,  1.94it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  23537930.0\n",
      "Epoch 0:   0%|          | 2/844 [00:01<07:19,  1.92it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  14056704.0\n",
      "Epoch 0:   0%|          | 3/844 [00:01<07:06,  1.97it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  14208511.0\n",
      "Epoch 0:   0%|          | 4/844 [00:01<06:50,  2.05it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  15475266.0\n",
      "Epoch 0:   1%|          | 5/844 [00:02<06:35,  2.12it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  16589206.0\n",
      "Epoch 0:   1%|          | 6/844 [00:03<08:36,  1.62it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  15238294.0\n",
      "Epoch 0:   1%|          | 7/844 [00:05<10:40,  1.31it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  15670368.0\n",
      "Epoch 0:   1%|          | 8/844 [00:07<12:27,  1.12it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  15491550.0\n",
      "Epoch 0:   1%|          | 9/844 [00:07<12:07,  1.15it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  13389872.0\n",
      "Epoch 0:   1%|          | 10/844 [00:08<11:40,  1.19it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  13924350.0\n",
      "Epoch 0:   1%|▏         | 11/844 [00:09<11:21,  1.22it/s, v_num=31]initial shape:  torch.Size([32, 1, 128, 87])\n",
      "conv1:  torch.Size([32, 32, 64, 44])\n",
      "conv2:  torch.Size([32, 32, 64, 44])\n",
      "conv3:  torch.Size([32, 64, 32, 22])\n",
      "conv4:  torch.Size([32, 64, 32, 22])\n",
      "conv5:  torch.Size([32, 64, 16, 11])\n",
      "flatten:  torch.Size([32, 11264])\n",
      "fc:  torch.Size([32, 2048])\n",
      "x decoder initially:  torch.Size([32, 2048])\n",
      "x decoder linear:  torch.Size([32, 11264])\n",
      "x decoder reshape:  torch.Size([32, 64, 16, 11])\n",
      "x decoder conv1:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv2:  torch.Size([32, 64, 32, 22])\n",
      "x decoder conv3:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv4:  torch.Size([32, 32, 64, 44])\n",
      "x decoder conv5:  torch.Size([32, 1, 128, 88])\n",
      "x decoder tanh:  torch.Size([32, 1, 128, 88])\n",
      "Train loss:  15523718.0\n",
      "Epoch 0:   1%|▏         | 12/844 [00:09<11:01,  1.26it/s, v_num=31]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[174], line 11\u001b[0m, in \u001b[0;36mAudioDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     10\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../samples/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\librosa\\core\\audio.py:222\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43msf_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[1;32m-> 1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1354\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_dim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2048\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     model_ld, result_ld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     model_dict[latent_dim] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_ld, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_ld}\n",
      "Cell \u001b[1;32mIn[223], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(latent_dim)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m Autoencoder(base_channel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[0;32m     26\u001b[0m val_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(model, dataloaders\u001b[38;5;241m=\u001b[39mval_loader, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "for latent_dim in [2048]:\n",
    "    model_ld, result_ld = train(latent_dim)\n",
    "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-reconstruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
