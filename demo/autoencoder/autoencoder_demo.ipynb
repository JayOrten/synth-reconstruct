{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREDIT: Adapted from: https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import numpy \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "import wave\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "from torcheval.metrics import R2Score\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/audio_test\")\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDS(Dataset):\n",
    "    def __init__(self, presets_csv_path):\n",
    "        self.presets = pd.read_csv(presets_csv_path)\n",
    "        # Take only the first 100 for debugging\n",
    "        self.presets = self.presets.head(2999)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.presets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        preset = self.presets.iloc[idx]\n",
    "        preset_name = preset[0]\n",
    "        spectrogram_path = Path(f'../../data/spectrograms/128x128/{preset_name}.png')\n",
    "\n",
    "        # First, check if the image has already been generated\n",
    "        if not spectrogram_path.exists():\n",
    "            raise FileNotFoundError(spectrogram_path)\n",
    "\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = torchvision.io.read_image(spectrogram_path)\n",
    "        except:\n",
    "            print('error loading image')\n",
    "            print(preset_name)\n",
    "\n",
    "        image = image.float() / 255\n",
    "        # print(image)\n",
    "        preset = torch.tensor(preset[1:-1])\n",
    "\n",
    "        return image, preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "dataset = AudioDS('..\\\\..\\\\data\\\\presets.csv')\n",
    "pl.seed_everything(42)\n",
    "num_train = int(0.9 * len(dataset))\n",
    "num_val = len(dataset) - num_train\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [num_train, num_val])\n",
    "batch_size = 256\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\1055357869.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset_name = preset[0]\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\1055357869.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2196, 0.1255, 0.1255,  ..., 0.0196, 0.0196, 0.0118],\n",
       "          [0.3608, 0.3216, 0.3216,  ..., 0.1922, 0.1922, 0.1412],\n",
       "          [0.3961, 0.3373, 0.3373,  ..., 0.2510, 0.2510, 0.2196],\n",
       "          ...,\n",
       "          [0.9961, 0.9961, 0.9961,  ..., 0.8980, 0.8980, 0.9059],\n",
       "          [0.9961, 0.9961, 0.9961,  ..., 0.9216, 0.9216, 0.8902],\n",
       "          [0.9961, 0.9922, 0.9922,  ..., 0.9569, 0.9569, 0.8784]],\n",
       "\n",
       "         [[0.0627, 0.0667, 0.0667,  ..., 0.0157, 0.0157, 0.0118],\n",
       "          [0.0863, 0.0745, 0.0745,  ..., 0.0667, 0.0667, 0.0706],\n",
       "          [0.1020, 0.0784, 0.0784,  ..., 0.0588, 0.0588, 0.0627],\n",
       "          ...,\n",
       "          [0.7765, 0.6471, 0.6471,  ..., 0.3137, 0.3137, 0.3216],\n",
       "          [0.7569, 0.6392, 0.6392,  ..., 0.3412, 0.3412, 0.3059],\n",
       "          [0.6667, 0.5882, 0.5882,  ..., 0.4039, 0.4039, 0.2980]],\n",
       "\n",
       "         [[0.4235, 0.2941, 0.2941,  ..., 0.0863, 0.0863, 0.0706],\n",
       "          [0.4980, 0.4863, 0.4863,  ..., 0.3961, 0.3961, 0.3255],\n",
       "          [0.5020, 0.4902, 0.4902,  ..., 0.4549, 0.4549, 0.4235],\n",
       "          ...,\n",
       "          [0.5412, 0.4431, 0.4431,  ..., 0.3922, 0.3922, 0.3882],\n",
       "          [0.5216, 0.4353, 0.4353,  ..., 0.3765, 0.3765, 0.3961],\n",
       "          [0.4549, 0.4078, 0.4078,  ..., 0.3608, 0.3608, 0.4039]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_images(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * (16 * 16) * c_hid, latent_dim), # 50, 50\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * (16 * 16) * c_hid), act_fn()) # 50, 50\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 16x16 => 32x32\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 16, 16)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "        num_input_channels: int = 2,\n",
    "        width: int = 128,\n",
    "        height: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(1, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        # print('x.shape: ', x.shape)\n",
    "        # print('x_hat.shape: ', x_hat.shape)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        print('train_loss: ', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "\n",
    "            # print('input_imgs: ', input_imgs)\n",
    "            # print('reconst_imgs: ', reconst_imgs)\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, value_range=(0, 1))\n",
    "            # trainer.logger.experiment.image(\"reconstruction\", grid, global_step=trainer.global_step)\n",
    "            # save image to disk\n",
    "            save_path = os.path.join(trainer.logger.log_dir, f\"reconstruction_{trainer.current_epoch}.png\")\n",
    "            torchvision.utils.save_image(grid, save_path, nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(latent_dim, max_epochs=50):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    print('creating trainer')\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"final_model_dim_%i\" % latent_dim),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            GenerateCallback(get_train_images(8), every_n_epochs=1),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    # pretrained_filename = os.path.join(CHECKPOINT_PATH, \"cifar10_%i.ckpt\" % latent_dim)\n",
    "    # if os.path.isfile(pretrained_filename):\n",
    "    #     print(\"Found pretrained model, loading...\")\n",
    "    #     model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    # else:\n",
    "    print(\"creating model\")\n",
    "    model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "    print(\"training model\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    print(\"testing model\")\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    # test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    # result = {\"test\": test_result, \"val\": val_result}\n",
    "    result = {\"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\2580750985.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset_name = preset[0]\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\2580750985.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trainer\n",
      "creating model\n",
      "training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params | Mode  | In sizes         | Out sizes       \n",
      "----------------------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 1.2 M  | train | [1, 2, 128, 128] | [1, 64]         \n",
      "1 | decoder | Decoder | 1.2 M  | train | [1, 64]          | [1, 2, 128, 128]\n",
      "----------------------------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.272     Total estimated model params size (MB)\n",
      "29        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/11 [00:00<?, ?it/s] train_loss:  tensor(1.8394e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:   9%|▉         | 1/11 [00:03<00:30,  0.33it/s, v_num=1]train_loss:  tensor(1.4750e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  18%|█▊        | 2/11 [00:05<00:25,  0.35it/s, v_num=1]train_loss:  tensor(2.2661e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  27%|██▋       | 3/11 [00:08<00:22,  0.36it/s, v_num=1]train_loss:  tensor(2.8979e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  36%|███▋      | 4/11 [00:10<00:18,  0.37it/s, v_num=1]train_loss:  tensor(8.8532e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  45%|████▌     | 5/11 [00:13<00:16,  0.37it/s, v_num=1]train_loss:  tensor(1.0614e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  55%|█████▍    | 6/11 [00:16<00:13,  0.37it/s, v_num=1]train_loss:  tensor(3.7974e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  64%|██████▎   | 7/11 [00:18<00:10,  0.38it/s, v_num=1]train_loss:  tensor(1.0334e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  73%|███████▎  | 8/11 [00:21<00:07,  0.38it/s, v_num=1]train_loss:  tensor(6.8320e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  82%|████████▏ | 9/11 [00:23<00:05,  0.38it/s, v_num=1]train_loss:  tensor(6.6857e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:26<00:02,  0.38it/s, v_num=1]train_loss:  tensor(3.0164e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(3.8438e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:   9%|▉         | 1/11 [00:00<00:09,  1.08it/s, v_num=1]train_loss:  tensor(3.5834e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  18%|█▊        | 2/11 [00:01<00:08,  1.06it/s, v_num=1]train_loss:  tensor(1.6927e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=1]train_loss:  tensor(2.0576e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  36%|███▋      | 4/11 [00:03<00:06,  1.08it/s, v_num=1]train_loss:  tensor(2.3594e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s, v_num=1]train_loss:  tensor(1.0861e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  55%|█████▍    | 6/11 [00:05<00:04,  1.08it/s, v_num=1]train_loss:  tensor(8.7848e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  64%|██████▎   | 7/11 [00:06<00:03,  1.08it/s, v_num=1]train_loss:  tensor(5.4185e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  73%|███████▎  | 8/11 [00:07<00:02,  1.08it/s, v_num=1]train_loss:  tensor(2.1436e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  82%|████████▏ | 9/11 [00:08<00:01,  1.08it/s, v_num=1]train_loss:  tensor(2.9753e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:09<00:00,  1.08it/s, v_num=1]train_loss:  tensor(7.2872e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(7.4586e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:   9%|▉         | 1/11 [00:01<00:10,  0.99it/s, v_num=1]train_loss:  tensor(5.9769e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  18%|█▊        | 2/11 [00:01<00:08,  1.01it/s, v_num=1]train_loss:  tensor(2.3456e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  27%|██▋       | 3/11 [00:03<00:08,  1.00it/s, v_num=1]train_loss:  tensor(1.0486e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  36%|███▋      | 4/11 [00:04<00:07,  0.98it/s, v_num=1]train_loss:  tensor(9.9262e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  45%|████▌     | 5/11 [00:05<00:06,  0.97it/s, v_num=1]train_loss:  tensor(3.1609e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  55%|█████▍    | 6/11 [00:06<00:05,  0.99it/s, v_num=1]train_loss:  tensor(7.8194e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  64%|██████▎   | 7/11 [00:07<00:04,  1.00it/s, v_num=1]train_loss:  tensor(3.6091e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  73%|███████▎  | 8/11 [00:08<00:03,  1.00it/s, v_num=1]train_loss:  tensor(1.6184e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  82%|████████▏ | 9/11 [00:09<00:02,  1.00it/s, v_num=1]train_loss:  tensor(2.2293e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:09<00:00,  1.00it/s, v_num=1]train_loss:  tensor(2.4339e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(2.6458e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:   9%|▉         | 1/11 [00:01<00:10,  0.97it/s, v_num=1]train_loss:  tensor(2.1303e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  18%|█▊        | 2/11 [00:02<00:09,  0.96it/s, v_num=1]train_loss:  tensor(9.6574e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  27%|██▋       | 3/11 [00:03<00:08,  0.95it/s, v_num=1]train_loss:  tensor(4.3748e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  36%|███▋      | 4/11 [00:04<00:07,  0.97it/s, v_num=1]train_loss:  tensor(2.2997e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  45%|████▌     | 5/11 [00:05<00:06,  0.98it/s, v_num=1]train_loss:  tensor(1.6319e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  55%|█████▍    | 6/11 [00:06<00:05,  0.98it/s, v_num=1]train_loss:  tensor(3.4881e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  64%|██████▎   | 7/11 [00:07<00:04,  0.99it/s, v_num=1]train_loss:  tensor(5.2581e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  73%|███████▎  | 8/11 [00:08<00:03,  0.99it/s, v_num=1]train_loss:  tensor(3.6785e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  82%|████████▏ | 9/11 [00:09<00:02,  1.00it/s, v_num=1]train_loss:  tensor(6.9107e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:09<00:00,  1.00it/s, v_num=1]train_loss:  tensor(8.7753e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(1.1922e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:   9%|▉         | 1/11 [00:01<00:12,  0.80it/s, v_num=1]train_loss:  tensor(3.1832e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  18%|█▊        | 2/11 [00:02<00:09,  0.90it/s, v_num=1]train_loss:  tensor(2.7015e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  27%|██▋       | 3/11 [00:03<00:08,  0.94it/s, v_num=1]train_loss:  tensor(7.0272e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  36%|███▋      | 4/11 [00:04<00:07,  0.97it/s, v_num=1]train_loss:  tensor(2.2319e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  45%|████▌     | 5/11 [00:05<00:06,  0.96it/s, v_num=1]train_loss:  tensor(8.2023e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  55%|█████▍    | 6/11 [00:06<00:05,  0.97it/s, v_num=1]train_loss:  tensor(3.3847e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  64%|██████▎   | 7/11 [00:07<00:04,  0.96it/s, v_num=1]train_loss:  tensor(8.9377e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  73%|███████▎  | 8/11 [00:08<00:03,  0.97it/s, v_num=1]train_loss:  tensor(1.6150e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  82%|████████▏ | 9/11 [00:09<00:02,  0.97it/s, v_num=1]train_loss:  tensor(2.3315e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:10<00:01,  0.97it/s, v_num=1]train_loss:  tensor(7.4778e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(3.1548e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:   9%|▉         | 1/11 [00:01<00:10,  0.98it/s, v_num=1]train_loss:  tensor(3.0551e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  18%|█▊        | 2/11 [00:02<00:09,  0.98it/s, v_num=1]train_loss:  tensor(4.7533e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  27%|██▋       | 3/11 [00:03<00:08,  0.93it/s, v_num=1]train_loss:  tensor(9.3246e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  36%|███▋      | 4/11 [00:04<00:07,  0.94it/s, v_num=1]train_loss:  tensor(1.2447e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  45%|████▌     | 5/11 [00:05<00:06,  0.96it/s, v_num=1]train_loss:  tensor(2.4509e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  55%|█████▍    | 6/11 [00:06<00:05,  0.97it/s, v_num=1]train_loss:  tensor(2.2579e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  64%|██████▎   | 7/11 [00:07<00:04,  0.97it/s, v_num=1]train_loss:  tensor(5.1340e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  73%|███████▎  | 8/11 [00:08<00:03,  0.97it/s, v_num=1]train_loss:  tensor(9.0110e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  82%|████████▏ | 9/11 [00:09<00:02,  0.97it/s, v_num=1]train_loss:  tensor(6.6421e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  91%|█████████ | 10/11 [00:10<00:01,  0.96it/s, v_num=1]train_loss:  tensor(3.3355e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(2.1356e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:   9%|▉         | 1/11 [00:01<00:11,  0.87it/s, v_num=1]train_loss:  tensor(2.3719e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  18%|█▊        | 2/11 [00:02<00:09,  0.90it/s, v_num=1]train_loss:  tensor(2.3963e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  27%|██▋       | 3/11 [00:03<00:08,  0.93it/s, v_num=1]train_loss:  tensor(9.1169e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  36%|███▋      | 4/11 [00:04<00:07,  0.94it/s, v_num=1]train_loss:  tensor(2.7441e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  45%|████▌     | 5/11 [00:05<00:06,  0.96it/s, v_num=1]train_loss:  tensor(1.6284e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  55%|█████▍    | 6/11 [00:06<00:05,  0.95it/s, v_num=1]train_loss:  tensor(2.9131e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  64%|██████▎   | 7/11 [00:07<00:04,  0.94it/s, v_num=1]train_loss:  tensor(2.0511e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  73%|███████▎  | 8/11 [00:08<00:03,  0.95it/s, v_num=1]train_loss:  tensor(3.9268e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  82%|████████▏ | 9/11 [00:09<00:02,  0.95it/s, v_num=1]train_loss:  tensor(3.7148e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  91%|█████████ | 10/11 [00:10<00:01,  0.96it/s, v_num=1]train_loss:  tensor(9.1751e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(1.9390e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:   9%|▉         | 1/11 [00:01<00:10,  0.96it/s, v_num=1]train_loss:  tensor(7.2701e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  18%|█▊        | 2/11 [00:02<00:10,  0.84it/s, v_num=1]train_loss:  tensor(1.9682e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  27%|██▋       | 3/11 [00:03<00:08,  0.89it/s, v_num=1]train_loss:  tensor(6.5145e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  36%|███▋      | 4/11 [00:04<00:07,  0.89it/s, v_num=1]train_loss:  tensor(8.9170e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  45%|████▌     | 5/11 [00:05<00:06,  0.91it/s, v_num=1]train_loss:  tensor(2.0951e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  55%|█████▍    | 6/11 [00:06<00:05,  0.93it/s, v_num=1]train_loss:  tensor(8.5233e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  64%|██████▎   | 7/11 [00:07<00:04,  0.92it/s, v_num=1]train_loss:  tensor(8.9693e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  73%|███████▎  | 8/11 [00:08<00:03,  0.92it/s, v_num=1]train_loss:  tensor(3.3827e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  82%|████████▏ | 9/11 [00:09<00:02,  0.91it/s, v_num=1]train_loss:  tensor(4.6967e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  91%|█████████ | 10/11 [00:10<00:01,  0.91it/s, v_num=1]train_loss:  tensor(4.5664e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(2.3309e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:   9%|▉         | 1/11 [00:01<00:14,  0.70it/s, v_num=1]train_loss:  tensor(3.3404e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  18%|█▊        | 2/11 [00:02<00:10,  0.82it/s, v_num=1]train_loss:  tensor(2.2876e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  27%|██▋       | 3/11 [00:03<00:09,  0.87it/s, v_num=1]train_loss:  tensor(1.5039e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  36%|███▋      | 4/11 [00:04<00:07,  0.89it/s, v_num=1]train_loss:  tensor(3.2452e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  45%|████▌     | 5/11 [00:05<00:06,  0.88it/s, v_num=1]train_loss:  tensor(7.2984e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  55%|█████▍    | 6/11 [00:06<00:05,  0.88it/s, v_num=1]train_loss:  tensor(2.3099e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  64%|██████▎   | 7/11 [00:07<00:04,  0.89it/s, v_num=1]train_loss:  tensor(1.7677e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  73%|███████▎  | 8/11 [00:08<00:03,  0.90it/s, v_num=1]train_loss:  tensor(4.5176e+13, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  82%|████████▏ | 9/11 [00:10<00:02,  0.89it/s, v_num=1]train_loss:  tensor(1.0438e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  91%|█████████ | 10/11 [00:11<00:01,  0.86it/s, v_num=1]train_loss:  tensor(2.0505e+12, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:   0%|          | 0/11 [00:00<?, ?it/s, v_num=1]         train_loss:  tensor(2.5806e+15, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:   9%|▉         | 1/11 [00:01<00:11,  0.87it/s, v_num=1]train_loss:  tensor(2.3730e+14, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  18%|█▊        | 2/11 [00:02<00:10,  0.82it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[21], line 30\u001b[0m, in \u001b[0;36mAudioDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m sig, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(audio_path)\n\u001b[1;32m---> 30\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMelSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Trim the last dimension (time or frames) to 128 to have a nice square image\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:619\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;124;03m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torchaudio\\functional\\functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[1;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_length_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\functional.py:666\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_dim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m384\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     model_ld, result_ld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     model_dict[latent_dim] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_ld, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_ld}\n",
      "Cell \u001b[1;32mIn[33], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(latent_dim, max_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m Autoencoder(base_channel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "for latent_dim in [64, 128, 256, 384]:\n",
    "    model_ld, result_ld = train(latent_dim)\n",
    "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{64: {'model': Autoencoder(\n",
       "    (encoder): Encoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Flatten(start_dim=1, end_dim=-1)\n",
       "        (11): Linear(in_features=16384, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=16384, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): ConvTranspose2d(32, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (9): Tanh()\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'result': {'val': [{'test_loss': 457.75244140625}]}},\n",
       " 128: {'model': Autoencoder(\n",
       "    (encoder): Encoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Flatten(start_dim=1, end_dim=-1)\n",
       "        (11): Linear(in_features=16384, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=16384, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): ConvTranspose2d(32, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (9): Tanh()\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'result': {'val': [{'test_loss': 459.4795837402344}]}},\n",
       " 256: {'model': Autoencoder(\n",
       "    (encoder): Encoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Flatten(start_dim=1, end_dim=-1)\n",
       "        (11): Linear(in_features=16384, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=16384, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): ConvTranspose2d(32, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (9): Tanh()\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'result': {'val': [{'test_loss': 465.7685546875}]}},\n",
       " 384: {'model': Autoencoder(\n",
       "    (encoder): Encoder(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Flatten(start_dim=1, end_dim=-1)\n",
       "        (11): Linear(in_features=16384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=16384, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (net): Sequential(\n",
       "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): GELU(approximate='none')\n",
       "        (8): ConvTranspose2d(32, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (9): Tanh()\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'result': {'val': [{'test_loss': 434.2647705078125}]}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"392.025469pt\" height=\"286.855781pt\" viewBox=\"0 0 392.025469 286.855781\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-12T15:54:26.600841</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 286.855781 \n",
       "L 392.025469 286.855781 \n",
       "L 392.025469 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.025469 244.980938 \n",
       "L 384.825469 244.980938 \n",
       "L 384.825469 23.220937 \n",
       "L 50.025469 23.220937 \n",
       "z\n",
       "\" style=\"fill: #eaeaf2\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 65.243651 244.980938 \n",
       "L 65.243651 23.220937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 64 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(59.126619 262.354531) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-36\" d=\"M 3184 3459 \n",
       "L 2625 3416 \n",
       "Q 2550 3747 2413 3897 \n",
       "Q 2184 4138 1850 4138 \n",
       "Q 1581 4138 1378 3988 \n",
       "Q 1113 3794 959 3422 \n",
       "Q 806 3050 800 2363 \n",
       "Q 1003 2672 1297 2822 \n",
       "Q 1591 2972 1913 2972 \n",
       "Q 2475 2972 2870 2558 \n",
       "Q 3266 2144 3266 1488 \n",
       "Q 3266 1056 3080 686 \n",
       "Q 2894 316 2569 119 \n",
       "Q 2244 -78 1831 -78 \n",
       "Q 1128 -78 684 439 \n",
       "Q 241 956 241 2144 \n",
       "Q 241 3472 731 4075 \n",
       "Q 1159 4600 1884 4600 \n",
       "Q 2425 4600 2770 4297 \n",
       "Q 3116 3994 3184 3459 \n",
       "z\n",
       "M 888 1484 \n",
       "Q 888 1194 1011 928 \n",
       "Q 1134 663 1356 523 \n",
       "Q 1578 384 1822 384 \n",
       "Q 2178 384 2434 671 \n",
       "Q 2691 959 2691 1453 \n",
       "Q 2691 1928 2437 2201 \n",
       "Q 2184 2475 1800 2475 \n",
       "Q 1419 2475 1153 2201 \n",
       "Q 888 1928 888 1484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-34\" d=\"M 2069 0 \n",
       "L 2069 1097 \n",
       "L 81 1097 \n",
       "L 81 1613 \n",
       "L 2172 4581 \n",
       "L 2631 4581 \n",
       "L 2631 1613 \n",
       "L 3250 1613 \n",
       "L 3250 1097 \n",
       "L 2631 1097 \n",
       "L 2631 0 \n",
       "L 2069 0 \n",
       "z\n",
       "M 2069 1613 \n",
       "L 2069 3678 \n",
       "L 634 1613 \n",
       "L 2069 1613 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-36\"/>\n",
       "       <use xlink:href=\"#ArialMT-34\" x=\"55.615234\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <path d=\"M 182.987578 244.980938 \n",
       "L 182.987578 23.220937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 128 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(173.812031 262.354531) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-32\" d=\"M 3222 541 \n",
       "L 3222 0 \n",
       "L 194 0 \n",
       "Q 188 203 259 391 \n",
       "Q 375 700 629 1000 \n",
       "Q 884 1300 1366 1694 \n",
       "Q 2113 2306 2375 2664 \n",
       "Q 2638 3022 2638 3341 \n",
       "Q 2638 3675 2398 3904 \n",
       "Q 2159 4134 1775 4134 \n",
       "Q 1369 4134 1125 3890 \n",
       "Q 881 3647 878 3216 \n",
       "L 300 3275 \n",
       "Q 359 3922 746 4261 \n",
       "Q 1134 4600 1788 4600 \n",
       "Q 2447 4600 2831 4234 \n",
       "Q 3216 3869 3216 3328 \n",
       "Q 3216 3053 3103 2787 \n",
       "Q 2991 2522 2730 2228 \n",
       "Q 2469 1934 1863 1422 \n",
       "Q 1356 997 1212 845 \n",
       "Q 1069 694 975 541 \n",
       "L 3222 541 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"ArialMT-38\" d=\"M 1131 2484 \n",
       "Q 781 2613 612 2850 \n",
       "Q 444 3088 444 3419 \n",
       "Q 444 3919 803 4259 \n",
       "Q 1163 4600 1759 4600 \n",
       "Q 2359 4600 2725 4251 \n",
       "Q 3091 3903 3091 3403 \n",
       "Q 3091 3084 2923 2848 \n",
       "Q 2756 2613 2416 2484 \n",
       "Q 2838 2347 3058 2040 \n",
       "Q 3278 1734 3278 1309 \n",
       "Q 3278 722 2862 322 \n",
       "Q 2447 -78 1769 -78 \n",
       "Q 1091 -78 675 323 \n",
       "Q 259 725 259 1325 \n",
       "Q 259 1772 486 2073 \n",
       "Q 713 2375 1131 2484 \n",
       "z\n",
       "M 1019 3438 \n",
       "Q 1019 3113 1228 2906 \n",
       "Q 1438 2700 1772 2700 \n",
       "Q 2097 2700 2305 2904 \n",
       "Q 2513 3109 2513 3406 \n",
       "Q 2513 3716 2298 3927 \n",
       "Q 2084 4138 1766 4138 \n",
       "Q 1444 4138 1231 3931 \n",
       "Q 1019 3725 1019 3438 \n",
       "z\n",
       "M 838 1322 \n",
       "Q 838 1081 952 856 \n",
       "Q 1066 631 1291 507 \n",
       "Q 1516 384 1775 384 \n",
       "Q 2178 384 2440 643 \n",
       "Q 2703 903 2703 1303 \n",
       "Q 2703 1709 2433 1975 \n",
       "Q 2163 2241 1756 2241 \n",
       "Q 1359 2241 1098 1978 \n",
       "Q 838 1716 838 1322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-32\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-38\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 300.731505 244.980938 \n",
       "L 300.731505 23.220937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 256 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(291.555958 262.354531) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-35\" d=\"M 266 1200 \n",
       "L 856 1250 \n",
       "Q 922 819 1161 601 \n",
       "Q 1400 384 1738 384 \n",
       "Q 2144 384 2425 690 \n",
       "Q 2706 997 2706 1503 \n",
       "Q 2706 1984 2436 2262 \n",
       "Q 2166 2541 1728 2541 \n",
       "Q 1456 2541 1237 2417 \n",
       "Q 1019 2294 894 2097 \n",
       "L 366 2166 \n",
       "L 809 4519 \n",
       "L 3088 4519 \n",
       "L 3088 3981 \n",
       "L 1259 3981 \n",
       "L 1013 2750 \n",
       "Q 1425 3038 1878 3038 \n",
       "Q 2478 3038 2890 2622 \n",
       "Q 3303 2206 3303 1553 \n",
       "Q 3303 931 2941 478 \n",
       "Q 2500 -78 1738 -78 \n",
       "Q 1113 -78 717 272 \n",
       "Q 322 622 266 1200 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-36\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <path d=\"M 369.607287 244.980938 \n",
       "L 369.607287 23.220937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 384 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(360.43174 262.354531) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-33\" d=\"M 269 1209 \n",
       "L 831 1284 \n",
       "Q 928 806 1161 595 \n",
       "Q 1394 384 1728 384 \n",
       "Q 2125 384 2398 659 \n",
       "Q 2672 934 2672 1341 \n",
       "Q 2672 1728 2419 1979 \n",
       "Q 2166 2231 1775 2231 \n",
       "Q 1616 2231 1378 2169 \n",
       "L 1441 2663 \n",
       "Q 1497 2656 1531 2656 \n",
       "Q 1891 2656 2178 2843 \n",
       "Q 2466 3031 2466 3422 \n",
       "Q 2466 3731 2256 3934 \n",
       "Q 2047 4138 1716 4138 \n",
       "Q 1388 4138 1169 3931 \n",
       "Q 950 3725 888 3313 \n",
       "L 325 3413 \n",
       "Q 428 3978 793 4289 \n",
       "Q 1159 4600 1703 4600 \n",
       "Q 2078 4600 2393 4439 \n",
       "Q 2709 4278 2876 4000 \n",
       "Q 3044 3722 3044 3409 \n",
       "Q 3044 3113 2884 2869 \n",
       "Q 2725 2625 2413 2481 \n",
       "Q 2819 2388 3044 2092 \n",
       "Q 3269 1797 3269 1353 \n",
       "Q 3269 753 2831 336 \n",
       "Q 2394 -81 1725 -81 \n",
       "Q 1122 -81 723 278 \n",
       "Q 325 638 269 1209 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-33\"/>\n",
       "       <use xlink:href=\"#ArialMT-38\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-34\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- Latent dimensionality -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(161.062031 277.130156) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-4c\" d=\"M 469 0 \n",
       "L 469 4581 \n",
       "L 1075 4581 \n",
       "L 1075 541 \n",
       "L 3331 541 \n",
       "L 3331 0 \n",
       "L 469 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-61\" d=\"M 2588 409 \n",
       "Q 2275 144 1986 34 \n",
       "Q 1697 -75 1366 -75 \n",
       "Q 819 -75 525 192 \n",
       "Q 231 459 231 875 \n",
       "Q 231 1119 342 1320 \n",
       "Q 453 1522 633 1644 \n",
       "Q 813 1766 1038 1828 \n",
       "Q 1203 1872 1538 1913 \n",
       "Q 2219 1994 2541 2106 \n",
       "Q 2544 2222 2544 2253 \n",
       "Q 2544 2597 2384 2738 \n",
       "Q 2169 2928 1744 2928 \n",
       "Q 1347 2928 1158 2789 \n",
       "Q 969 2650 878 2297 \n",
       "L 328 2372 \n",
       "Q 403 2725 575 2942 \n",
       "Q 747 3159 1072 3276 \n",
       "Q 1397 3394 1825 3394 \n",
       "Q 2250 3394 2515 3294 \n",
       "Q 2781 3194 2906 3042 \n",
       "Q 3031 2891 3081 2659 \n",
       "Q 3109 2516 3109 2141 \n",
       "L 3109 1391 \n",
       "Q 3109 606 3145 398 \n",
       "Q 3181 191 3288 0 \n",
       "L 2700 0 \n",
       "Q 2613 175 2588 409 \n",
       "z\n",
       "M 2541 1666 \n",
       "Q 2234 1541 1622 1453 \n",
       "Q 1275 1403 1131 1340 \n",
       "Q 988 1278 909 1158 \n",
       "Q 831 1038 831 891 \n",
       "Q 831 666 1001 516 \n",
       "Q 1172 366 1500 366 \n",
       "Q 1825 366 2078 508 \n",
       "Q 2331 650 2450 897 \n",
       "Q 2541 1088 2541 1459 \n",
       "L 2541 1666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-74\" d=\"M 1650 503 \n",
       "L 1731 6 \n",
       "Q 1494 -44 1306 -44 \n",
       "Q 1000 -44 831 53 \n",
       "Q 663 150 594 308 \n",
       "Q 525 466 525 972 \n",
       "L 525 2881 \n",
       "L 113 2881 \n",
       "L 113 3319 \n",
       "L 525 3319 \n",
       "L 525 4141 \n",
       "L 1084 4478 \n",
       "L 1084 3319 \n",
       "L 1650 3319 \n",
       "L 1650 2881 \n",
       "L 1084 2881 \n",
       "L 1084 941 \n",
       "Q 1084 700 1114 631 \n",
       "Q 1144 563 1211 522 \n",
       "Q 1278 481 1403 481 \n",
       "Q 1497 481 1650 503 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-65\" d=\"M 2694 1069 \n",
       "L 3275 997 \n",
       "Q 3138 488 2766 206 \n",
       "Q 2394 -75 1816 -75 \n",
       "Q 1088 -75 661 373 \n",
       "Q 234 822 234 1631 \n",
       "Q 234 2469 665 2931 \n",
       "Q 1097 3394 1784 3394 \n",
       "Q 2450 3394 2872 2941 \n",
       "Q 3294 2488 3294 1666 \n",
       "Q 3294 1616 3291 1516 \n",
       "L 816 1516 \n",
       "Q 847 969 1125 678 \n",
       "Q 1403 388 1819 388 \n",
       "Q 2128 388 2347 550 \n",
       "Q 2566 713 2694 1069 \n",
       "z\n",
       "M 847 1978 \n",
       "L 2700 1978 \n",
       "Q 2663 2397 2488 2606 \n",
       "Q 2219 2931 1791 2931 \n",
       "Q 1403 2931 1139 2672 \n",
       "Q 875 2413 847 1978 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6e\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 928 3319 \n",
       "L 928 2847 \n",
       "Q 1294 3394 1984 3394 \n",
       "Q 2284 3394 2536 3286 \n",
       "Q 2788 3178 2913 3003 \n",
       "Q 3038 2828 3088 2588 \n",
       "Q 3119 2431 3119 2041 \n",
       "L 3119 0 \n",
       "L 2556 0 \n",
       "L 2556 2019 \n",
       "Q 2556 2363 2490 2533 \n",
       "Q 2425 2703 2258 2804 \n",
       "Q 2091 2906 1866 2906 \n",
       "Q 1506 2906 1245 2678 \n",
       "Q 984 2450 984 1813 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-64\" d=\"M 2575 0 \n",
       "L 2575 419 \n",
       "Q 2259 -75 1647 -75 \n",
       "Q 1250 -75 917 144 \n",
       "Q 584 363 401 755 \n",
       "Q 219 1147 219 1656 \n",
       "Q 219 2153 384 2558 \n",
       "Q 550 2963 881 3178 \n",
       "Q 1213 3394 1622 3394 \n",
       "Q 1922 3394 2156 3267 \n",
       "Q 2391 3141 2538 2938 \n",
       "L 2538 4581 \n",
       "L 3097 4581 \n",
       "L 3097 0 \n",
       "L 2575 0 \n",
       "z\n",
       "M 797 1656 \n",
       "Q 797 1019 1065 703 \n",
       "Q 1334 388 1700 388 \n",
       "Q 2069 388 2326 689 \n",
       "Q 2584 991 2584 1609 \n",
       "Q 2584 2291 2321 2609 \n",
       "Q 2059 2928 1675 2928 \n",
       "Q 1300 2928 1048 2622 \n",
       "Q 797 2316 797 1656 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-69\" d=\"M 425 3934 \n",
       "L 425 4581 \n",
       "L 988 4581 \n",
       "L 988 3934 \n",
       "L 425 3934 \n",
       "z\n",
       "M 425 0 \n",
       "L 425 3319 \n",
       "L 988 3319 \n",
       "L 988 0 \n",
       "L 425 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6d\" d=\"M 422 0 \n",
       "L 422 3319 \n",
       "L 925 3319 \n",
       "L 925 2853 \n",
       "Q 1081 3097 1340 3245 \n",
       "Q 1600 3394 1931 3394 \n",
       "Q 2300 3394 2536 3241 \n",
       "Q 2772 3088 2869 2813 \n",
       "Q 3263 3394 3894 3394 \n",
       "Q 4388 3394 4653 3120 \n",
       "Q 4919 2847 4919 2278 \n",
       "L 4919 0 \n",
       "L 4359 0 \n",
       "L 4359 2091 \n",
       "Q 4359 2428 4304 2576 \n",
       "Q 4250 2725 4106 2815 \n",
       "Q 3963 2906 3769 2906 \n",
       "Q 3419 2906 3187 2673 \n",
       "Q 2956 2441 2956 1928 \n",
       "L 2956 0 \n",
       "L 2394 0 \n",
       "L 2394 2156 \n",
       "Q 2394 2531 2256 2718 \n",
       "Q 2119 2906 1806 2906 \n",
       "Q 1569 2906 1367 2781 \n",
       "Q 1166 2656 1075 2415 \n",
       "Q 984 2175 984 1722 \n",
       "L 984 0 \n",
       "L 422 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-73\" d=\"M 197 991 \n",
       "L 753 1078 \n",
       "Q 800 744 1014 566 \n",
       "Q 1228 388 1613 388 \n",
       "Q 2000 388 2187 545 \n",
       "Q 2375 703 2375 916 \n",
       "Q 2375 1106 2209 1216 \n",
       "Q 2094 1291 1634 1406 \n",
       "Q 1016 1563 777 1677 \n",
       "Q 538 1791 414 1992 \n",
       "Q 291 2194 291 2438 \n",
       "Q 291 2659 392 2848 \n",
       "Q 494 3038 669 3163 \n",
       "Q 800 3259 1026 3326 \n",
       "Q 1253 3394 1513 3394 \n",
       "Q 1903 3394 2198 3281 \n",
       "Q 2494 3169 2634 2976 \n",
       "Q 2775 2784 2828 2463 \n",
       "L 2278 2388 \n",
       "Q 2241 2644 2061 2787 \n",
       "Q 1881 2931 1553 2931 \n",
       "Q 1166 2931 1000 2803 \n",
       "Q 834 2675 834 2503 \n",
       "Q 834 2394 903 2306 \n",
       "Q 972 2216 1119 2156 \n",
       "Q 1203 2125 1616 2013 \n",
       "Q 2213 1853 2448 1751 \n",
       "Q 2684 1650 2818 1456 \n",
       "Q 2953 1263 2953 975 \n",
       "Q 2953 694 2789 445 \n",
       "Q 2625 197 2315 61 \n",
       "Q 2006 -75 1616 -75 \n",
       "Q 969 -75 630 194 \n",
       "Q 291 463 197 991 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6f\" d=\"M 213 1659 \n",
       "Q 213 2581 725 3025 \n",
       "Q 1153 3394 1769 3394 \n",
       "Q 2453 3394 2887 2945 \n",
       "Q 3322 2497 3322 1706 \n",
       "Q 3322 1066 3130 698 \n",
       "Q 2938 331 2570 128 \n",
       "Q 2203 -75 1769 -75 \n",
       "Q 1072 -75 642 372 \n",
       "Q 213 819 213 1659 \n",
       "z\n",
       "M 791 1659 \n",
       "Q 791 1022 1069 705 \n",
       "Q 1347 388 1769 388 \n",
       "Q 2188 388 2466 706 \n",
       "Q 2744 1025 2744 1678 \n",
       "Q 2744 2294 2464 2611 \n",
       "Q 2184 2928 1769 2928 \n",
       "Q 1347 2928 1069 2612 \n",
       "Q 791 2297 791 1659 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-6c\" d=\"M 409 0 \n",
       "L 409 4581 \n",
       "L 972 4581 \n",
       "L 972 0 \n",
       "L 409 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-79\" d=\"M 397 -1278 \n",
       "L 334 -750 \n",
       "Q 519 -800 656 -800 \n",
       "Q 844 -800 956 -737 \n",
       "Q 1069 -675 1141 -563 \n",
       "Q 1194 -478 1313 -144 \n",
       "Q 1328 -97 1363 -6 \n",
       "L 103 3319 \n",
       "L 709 3319 \n",
       "L 1400 1397 \n",
       "Q 1534 1031 1641 628 \n",
       "Q 1738 1016 1872 1384 \n",
       "L 2581 3319 \n",
       "L 3144 3319 \n",
       "L 1881 -56 \n",
       "Q 1678 -603 1566 -809 \n",
       "Q 1416 -1088 1222 -1217 \n",
       "Q 1028 -1347 759 -1347 \n",
       "Q 597 -1347 397 -1278 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-4c\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"55.615234\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"111.230469\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"139.013672\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"194.628906\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"250.244141\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"278.027344\"/>\n",
       "      <use xlink:href=\"#ArialMT-64\" x=\"305.810547\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"361.425781\"/>\n",
       "      <use xlink:href=\"#ArialMT-6d\" x=\"383.642578\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"466.943359\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"522.558594\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"578.173828\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"628.173828\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"650.390625\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"706.005859\"/>\n",
       "      <use xlink:href=\"#ArialMT-61\" x=\"761.621094\"/>\n",
       "      <use xlink:href=\"#ArialMT-6c\" x=\"817.236328\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"839.453125\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"861.669922\"/>\n",
       "      <use xlink:href=\"#ArialMT-79\" x=\"889.453125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 50.025469 244.980938 \n",
       "L 384.825469 244.980938 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(34.408438 248.917734) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#ArialMT-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <path d=\"M 50.025469 200.628938 \n",
       "L 384.825469 200.628938 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 100 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.174375 204.565734) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-31\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 50.025469 156.276938 \n",
       "L 384.825469 156.276938 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 200 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.174375 160.213734) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-32\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <path d=\"M 50.025469 111.924937 \n",
       "L 384.825469 111.924937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 300 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.174375 115.861734) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-33\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 50.025469 67.572937 \n",
       "L 384.825469 67.572937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 400 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.174375 71.509734) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-34\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <path d=\"M 50.025469 23.220937 \n",
       "L 384.825469 23.220937 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 500 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(22.174375 27.157734) scale(0.11 -0.11)\">\n",
       "       <use xlink:href=\"#ArialMT-35\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n",
       "       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- Reconstruction error -->\n",
       "     <g style=\"fill: #262626\" transform=\"translate(15.789375 188.451562) rotate(-90) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-52\" d=\"M 503 0 \n",
       "L 503 4581 \n",
       "L 2534 4581 \n",
       "Q 3147 4581 3465 4457 \n",
       "Q 3784 4334 3975 4021 \n",
       "Q 4166 3709 4166 3331 \n",
       "Q 4166 2844 3850 2509 \n",
       "Q 3534 2175 2875 2084 \n",
       "Q 3116 1969 3241 1856 \n",
       "Q 3506 1613 3744 1247 \n",
       "L 4541 0 \n",
       "L 3778 0 \n",
       "L 3172 953 \n",
       "Q 2906 1366 2734 1584 \n",
       "Q 2563 1803 2427 1890 \n",
       "Q 2291 1978 2150 2013 \n",
       "Q 2047 2034 1813 2034 \n",
       "L 1109 2034 \n",
       "L 1109 0 \n",
       "L 503 0 \n",
       "z\n",
       "M 1109 2559 \n",
       "L 2413 2559 \n",
       "Q 2828 2559 3062 2645 \n",
       "Q 3297 2731 3419 2920 \n",
       "Q 3541 3109 3541 3331 \n",
       "Q 3541 3656 3305 3865 \n",
       "Q 3069 4075 2559 4075 \n",
       "L 1109 4075 \n",
       "L 1109 2559 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-63\" d=\"M 2588 1216 \n",
       "L 3141 1144 \n",
       "Q 3050 572 2676 248 \n",
       "Q 2303 -75 1759 -75 \n",
       "Q 1078 -75 664 370 \n",
       "Q 250 816 250 1647 \n",
       "Q 250 2184 428 2587 \n",
       "Q 606 2991 970 3192 \n",
       "Q 1334 3394 1763 3394 \n",
       "Q 2303 3394 2647 3120 \n",
       "Q 2991 2847 3088 2344 \n",
       "L 2541 2259 \n",
       "Q 2463 2594 2264 2762 \n",
       "Q 2066 2931 1784 2931 \n",
       "Q 1359 2931 1093 2626 \n",
       "Q 828 2322 828 1663 \n",
       "Q 828 994 1084 691 \n",
       "Q 1341 388 1753 388 \n",
       "Q 2084 388 2306 591 \n",
       "Q 2528 794 2588 1216 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-72\" d=\"M 416 0 \n",
       "L 416 3319 \n",
       "L 922 3319 \n",
       "L 922 2816 \n",
       "Q 1116 3169 1280 3281 \n",
       "Q 1444 3394 1641 3394 \n",
       "Q 1925 3394 2219 3213 \n",
       "L 2025 2691 \n",
       "Q 1819 2813 1613 2813 \n",
       "Q 1428 2813 1281 2702 \n",
       "Q 1134 2591 1072 2394 \n",
       "Q 978 2094 978 1738 \n",
       "L 978 0 \n",
       "L 416 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"ArialMT-75\" d=\"M 2597 0 \n",
       "L 2597 488 \n",
       "Q 2209 -75 1544 -75 \n",
       "Q 1250 -75 995 37 \n",
       "Q 741 150 617 320 \n",
       "Q 494 491 444 738 \n",
       "Q 409 903 409 1263 \n",
       "L 409 3319 \n",
       "L 972 3319 \n",
       "L 972 1478 \n",
       "Q 972 1038 1006 884 \n",
       "Q 1059 663 1231 536 \n",
       "Q 1403 409 1656 409 \n",
       "Q 1909 409 2131 539 \n",
       "Q 2353 669 2445 892 \n",
       "Q 2538 1116 2538 1541 \n",
       "L 2538 3319 \n",
       "L 3100 3319 \n",
       "L 3100 0 \n",
       "L 2597 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-52\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"72.216797\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"127.832031\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"177.832031\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"233.447266\"/>\n",
       "      <use xlink:href=\"#ArialMT-73\" x=\"289.0625\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"339.0625\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"366.845703\"/>\n",
       "      <use xlink:href=\"#ArialMT-75\" x=\"400.146484\"/>\n",
       "      <use xlink:href=\"#ArialMT-63\" x=\"455.761719\"/>\n",
       "      <use xlink:href=\"#ArialMT-74\" x=\"505.761719\"/>\n",
       "      <use xlink:href=\"#ArialMT-69\" x=\"533.544922\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"555.761719\"/>\n",
       "      <use xlink:href=\"#ArialMT-6e\" x=\"611.376953\"/>\n",
       "      <use xlink:href=\"#ArialMT-20\" x=\"666.992188\"/>\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"694.775391\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"750.390625\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"783.691406\"/>\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"816.992188\"/>\n",
       "      <use xlink:href=\"#ArialMT-72\" x=\"872.607422\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_11\">\n",
       "    <path d=\"M 65.243651 41.958575 \n",
       "L 182.987578 41.192553 \n",
       "L 300.731505 38.403268 \n",
       "L 369.607287 52.375826 \n",
       "\" clip-path=\"url(#pe3355aa441)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n",
       "    <defs>\n",
       "     <path id=\"meace97c0f1\" d=\"M 0 -8 \n",
       "L -1.796112 -2.472136 \n",
       "L -7.608452 -2.472136 \n",
       "L -2.90617 0.944272 \n",
       "L -4.702282 6.472136 \n",
       "L -0 3.055728 \n",
       "L 4.702282 6.472136 \n",
       "L 2.90617 0.944272 \n",
       "L 7.608452 -2.472136 \n",
       "L 1.796112 -2.472136 \n",
       "z\n",
       "\" style=\"stroke: #000000; stroke-linejoin: bevel\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pe3355aa441)\">\n",
       "     <use xlink:href=\"#meace97c0f1\" x=\"65.243651\" y=\"41.958575\" style=\"fill: #ccb974; stroke: #000000; stroke-linejoin: bevel\"/>\n",
       "     <use xlink:href=\"#meace97c0f1\" x=\"182.987578\" y=\"41.192553\" style=\"fill: #ccb974; stroke: #000000; stroke-linejoin: bevel\"/>\n",
       "     <use xlink:href=\"#meace97c0f1\" x=\"300.731505\" y=\"38.403268\" style=\"fill: #ccb974; stroke: #000000; stroke-linejoin: bevel\"/>\n",
       "     <use xlink:href=\"#meace97c0f1\" x=\"369.607287\" y=\"52.375826\" style=\"fill: #ccb974; stroke: #000000; stroke-linejoin: bevel\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.025469 244.980938 \n",
       "L 50.025469 23.220937 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.825469 244.980938 \n",
       "L 384.825469 23.220937 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.025469 244.980938 \n",
       "L 384.825469 244.980938 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.025469 23.220937 \n",
       "L 384.825469 23.220937 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- Reconstruction error over latent dimensionality -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(73.090937 17.220937) scale(0.14 -0.14)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-76\" d=\"M 1344 0 \n",
       "L 81 3319 \n",
       "L 675 3319 \n",
       "L 1388 1331 \n",
       "Q 1503 1009 1600 663 \n",
       "Q 1675 925 1809 1294 \n",
       "L 2547 3319 \n",
       "L 3125 3319 \n",
       "L 1869 0 \n",
       "L 1344 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-52\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"72.216797\"/>\n",
       "     <use xlink:href=\"#ArialMT-63\" x=\"127.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"177.832031\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"233.447266\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"289.0625\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"339.0625\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"366.845703\"/>\n",
       "     <use xlink:href=\"#ArialMT-75\" x=\"400.146484\"/>\n",
       "     <use xlink:href=\"#ArialMT-63\" x=\"455.761719\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"505.761719\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"533.544922\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"555.761719\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"611.376953\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"666.992188\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"694.775391\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"750.390625\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"783.691406\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"816.992188\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"872.607422\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"905.908203\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"933.691406\"/>\n",
       "     <use xlink:href=\"#ArialMT-76\" x=\"989.306641\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1039.306641\"/>\n",
       "     <use xlink:href=\"#ArialMT-72\" x=\"1094.921875\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1128.222656\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"1156.005859\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"1178.222656\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1233.837891\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1261.621094\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"1317.236328\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1372.851562\"/>\n",
       "     <use xlink:href=\"#ArialMT-20\" x=\"1400.634766\"/>\n",
       "     <use xlink:href=\"#ArialMT-64\" x=\"1428.417969\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"1484.033203\"/>\n",
       "     <use xlink:href=\"#ArialMT-6d\" x=\"1506.25\"/>\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"1589.550781\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"1645.166016\"/>\n",
       "     <use xlink:href=\"#ArialMT-73\" x=\"1700.78125\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"1750.78125\"/>\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"1772.998047\"/>\n",
       "     <use xlink:href=\"#ArialMT-6e\" x=\"1828.613281\"/>\n",
       "     <use xlink:href=\"#ArialMT-61\" x=\"1884.228516\"/>\n",
       "     <use xlink:href=\"#ArialMT-6c\" x=\"1939.84375\"/>\n",
       "     <use xlink:href=\"#ArialMT-69\" x=\"1962.060547\"/>\n",
       "     <use xlink:href=\"#ArialMT-74\" x=\"1984.277344\"/>\n",
       "     <use xlink:href=\"#ArialMT-79\" x=\"2012.060547\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe3355aa441\">\n",
       "   <rect x=\"50.025469\" y=\"23.220937\" width=\"334.8\" height=\"221.76\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dims = sorted(k for k in model_dict)\n",
    "val_scores = [model_dict[k][\"result\"][\"val\"][0][\"test_loss\"] for k in latent_dims]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    latent_dims, val_scores, \"--\", color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(latent_dims, labels=latent_dims)\n",
    "plt.title(\"Reconstruction error over latent dimensionality\", fontsize=14)\n",
    "plt.xlabel(\"Latent dimensionality\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.minorticks_off()\n",
    "plt.ylim(0, 500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_17660\\986052091.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset_name = preset[0]\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_17660\\986052091.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode  | In sizes         | Out sizes       \n",
      "----------------------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 2.2 M  | train | [1, 4, 128, 128] | [1, 128]        \n",
      "1 | decoder | Decoder | 2.2 M  | train | [1, 128]         | [1, 4, 128, 128]\n",
      "----------------------------------------------------------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.665    Total estimated model params size (MB)\n",
      "29        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trainer\n",
      "creating model\n",
      "training model\n",
      "Epoch 0:   0%|          | 0/11 [00:00<?, ?it/s]                             train_loss:  tensor(32920.5938, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:   9%|▉         | 1/11 [00:00<00:06,  1.53it/s, v_num=0]train_loss:  tensor(32202.6172, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  18%|█▊        | 2/11 [00:01<00:05,  1.53it/s, v_num=0]train_loss:  tensor(31856.9883, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  27%|██▋       | 3/11 [00:01<00:05,  1.53it/s, v_num=0]train_loss:  tensor(31395.3477, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  36%|███▋      | 4/11 [00:02<00:04,  1.54it/s, v_num=0]train_loss:  tensor(30280.8398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  45%|████▌     | 5/11 [00:03<00:03,  1.55it/s, v_num=0]train_loss:  tensor(26305.8633, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  55%|█████▍    | 6/11 [00:03<00:03,  1.54it/s, v_num=0]train_loss:  tensor(22365.3398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  64%|██████▎   | 7/11 [00:04<00:02,  1.55it/s, v_num=0]train_loss:  tensor(23031.7344, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  73%|███████▎  | 8/11 [00:05<00:01,  1.55it/s, v_num=0]train_loss:  tensor(18874.2930, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  82%|████████▏ | 9/11 [00:05<00:01,  1.56it/s, v_num=0]train_loss:  tensor(17487.1523, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, v_num=0]train_loss:  tensor(16187.9844, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(14514.0332, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(12561.3789, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  18%|█▊        | 2/11 [00:01<00:06,  1.37it/s, v_num=0]train_loss:  tensor(11500.9199, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  27%|██▋       | 3/11 [00:02<00:05,  1.42it/s, v_num=0]train_loss:  tensor(9951.5117, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  36%|███▋      | 4/11 [00:02<00:04,  1.46it/s, v_num=0]train_loss:  tensor(8647.4453, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  45%|████▌     | 5/11 [00:03<00:04,  1.46it/s, v_num=0]train_loss:  tensor(7491.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  55%|█████▍    | 6/11 [00:04<00:03,  1.44it/s, v_num=0]train_loss:  tensor(6647.3447, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  64%|██████▎   | 7/11 [00:04<00:02,  1.43it/s, v_num=0]train_loss:  tensor(5806.0107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  73%|███████▎  | 8/11 [00:05<00:02,  1.44it/s, v_num=0]train_loss:  tensor(4899.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  82%|████████▏ | 9/11 [00:06<00:01,  1.44it/s, v_num=0]train_loss:  tensor(4404.3096, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:06<00:00,  1.45it/s, v_num=0]train_loss:  tensor(3835.3804, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(3578.5400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:   9%|▉         | 1/11 [00:00<00:06,  1.45it/s, v_num=0]train_loss:  tensor(3172.4324, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  18%|█▊        | 2/11 [00:01<00:06,  1.48it/s, v_num=0]train_loss:  tensor(3117.7668, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  27%|██▋       | 3/11 [00:02<00:05,  1.48it/s, v_num=0]train_loss:  tensor(2912.2563, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  36%|███▋      | 4/11 [00:02<00:05,  1.40it/s, v_num=0]train_loss:  tensor(2800.7456, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  45%|████▌     | 5/11 [00:03<00:04,  1.40it/s, v_num=0]train_loss:  tensor(2703.4219, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  55%|█████▍    | 6/11 [00:04<00:03,  1.40it/s, v_num=0]train_loss:  tensor(2766.7412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  64%|██████▎   | 7/11 [00:05<00:02,  1.39it/s, v_num=0]train_loss:  tensor(2582.1697, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  73%|███████▎  | 8/11 [00:05<00:02,  1.37it/s, v_num=0]train_loss:  tensor(2501.0679, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  82%|████████▏ | 9/11 [00:06<00:01,  1.37it/s, v_num=0]train_loss:  tensor(2622.2341, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(2599.0532, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(2534.3516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:   9%|▉         | 1/11 [00:00<00:08,  1.16it/s, v_num=0]train_loss:  tensor(2524.9004, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(2254.2561, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  27%|██▋       | 3/11 [00:02<00:05,  1.35it/s, v_num=0]train_loss:  tensor(2351.9636, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  36%|███▋      | 4/11 [00:02<00:05,  1.38it/s, v_num=0]train_loss:  tensor(2197.9814, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  45%|████▌     | 5/11 [00:03<00:04,  1.39it/s, v_num=0]train_loss:  tensor(2175.0857, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  55%|█████▍    | 6/11 [00:04<00:03,  1.39it/s, v_num=0]train_loss:  tensor(2184.5181, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  64%|██████▎   | 7/11 [00:05<00:02,  1.38it/s, v_num=0]train_loss:  tensor(2136.6179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  73%|███████▎  | 8/11 [00:05<00:02,  1.37it/s, v_num=0]train_loss:  tensor(2227.0664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  82%|████████▏ | 9/11 [00:06<00:01,  1.37it/s, v_num=0]train_loss:  tensor(2035.2854, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:07<00:00,  1.38it/s, v_num=0]train_loss:  tensor(2076.4221, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(2215.9023, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:   9%|▉         | 1/11 [00:01<00:19,  0.53it/s, v_num=0]train_loss:  tensor(2052.8716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  18%|█▊        | 2/11 [00:03<00:14,  0.62it/s, v_num=0]train_loss:  tensor(1982.9554, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  27%|██▋       | 3/11 [00:04<00:10,  0.74it/s, v_num=0]train_loss:  tensor(1906.7358, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  36%|███▋      | 4/11 [00:04<00:08,  0.83it/s, v_num=0]train_loss:  tensor(1921.5659, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  45%|████▌     | 5/11 [00:05<00:06,  0.89it/s, v_num=0]train_loss:  tensor(1858.5404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  55%|█████▍    | 6/11 [00:06<00:05,  0.94it/s, v_num=0]train_loss:  tensor(1820.6790, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  64%|██████▎   | 7/11 [00:07<00:04,  0.95it/s, v_num=0]train_loss:  tensor(1806.0229, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  73%|███████▎  | 8/11 [00:08<00:03,  0.93it/s, v_num=0]train_loss:  tensor(1718.5867, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  82%|████████▏ | 9/11 [00:09<00:02,  0.94it/s, v_num=0]train_loss:  tensor(1769.5867, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:10<00:01,  0.95it/s, v_num=0]train_loss:  tensor(1739.7731, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(1796.9841, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:   9%|▉         | 1/11 [00:00<00:09,  1.00it/s, v_num=0]train_loss:  tensor(1682.0192, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  18%|█▊        | 2/11 [00:01<00:08,  1.05it/s, v_num=0]train_loss:  tensor(1797.2603, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(1659.5173, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  36%|███▋      | 4/11 [00:03<00:06,  1.11it/s, v_num=0]train_loss:  tensor(1598.6250, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=0]train_loss:  tensor(1638.1709, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(1649.5035, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(1652.9211, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(1620.3170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(1671.5146, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 5:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(1553.4526, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(1689.1581, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:   9%|▉         | 1/11 [00:00<00:08,  1.18it/s, v_num=0]train_loss:  tensor(1477.8805, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(1511.8069, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(1495.3691, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(1519.4550, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(1521.4452, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(1546.3716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(1524.1205, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(1457.4081, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(1530.6050, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 6:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(1471.6394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(1515.1133, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(1331.0526, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(1457.8201, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(1394.0938, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(1294.6357, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(1304.7350, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(1271.0261, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(1216.7435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  73%|███████▎  | 8/11 [00:06<00:02,  1.22it/s, v_num=0]train_loss:  tensor(1252.7269, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(1214.8142, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 7:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(1193.3448, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(1190.9443, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:   9%|▉         | 1/11 [00:01<00:11,  0.88it/s, v_num=0]train_loss:  tensor(1158.3335, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  18%|█▊        | 2/11 [00:01<00:08,  1.03it/s, v_num=0]train_loss:  tensor(1161.3275, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  27%|██▋       | 3/11 [00:02<00:07,  1.09it/s, v_num=0]train_loss:  tensor(1156.0743, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(1155.7338, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(1066.0747, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(1134.0697, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(1142.8658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(1149.2786, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(1144.5608, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 8:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(1070.9280, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(1061.2839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s, v_num=0]train_loss:  tensor(1054.4934, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  18%|█▊        | 2/11 [00:01<00:08,  1.03it/s, v_num=0]train_loss:  tensor(1048.1658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=0]train_loss:  tensor(1044.9973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(1079.1688, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  45%|████▌     | 5/11 [00:05<00:06,  0.98it/s, v_num=0]train_loss:  tensor(1065.3442, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  55%|█████▍    | 6/11 [00:06<00:05,  0.99it/s, v_num=0]train_loss:  tensor(1074.1582, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  64%|██████▎   | 7/11 [00:07<00:04,  1.00it/s, v_num=0]train_loss:  tensor(1055.4423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  73%|███████▎  | 8/11 [00:07<00:02,  1.02it/s, v_num=0]train_loss:  tensor(1018.2706, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  82%|████████▏ | 9/11 [00:08<00:01,  1.04it/s, v_num=0]train_loss:  tensor(975.7435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 9:  91%|█████████ | 10/11 [00:09<00:00,  1.06it/s, v_num=0]train_loss:  tensor(934.8713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]        train_loss:  tensor(941.0540, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:   9%|▉         | 1/11 [00:01<00:12,  0.79it/s, v_num=0]train_loss:  tensor(995.3768, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  18%|█▊        | 2/11 [00:02<00:09,  0.92it/s, v_num=0]train_loss:  tensor(939.8702, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  27%|██▋       | 3/11 [00:03<00:08,  0.97it/s, v_num=0]train_loss:  tensor(953.8766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  36%|███▋      | 4/11 [00:03<00:06,  1.00it/s, v_num=0]train_loss:  tensor(962.8092, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  45%|████▌     | 5/11 [00:04<00:05,  1.02it/s, v_num=0]train_loss:  tensor(942.1956, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  55%|█████▍    | 6/11 [00:05<00:04,  1.03it/s, v_num=0]train_loss:  tensor(943.7935, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  64%|██████▎   | 7/11 [00:06<00:03,  1.05it/s, v_num=0]train_loss:  tensor(912.1830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  73%|███████▎  | 8/11 [00:07<00:02,  1.08it/s, v_num=0]train_loss:  tensor(985.2421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  82%|████████▏ | 9/11 [00:08<00:01,  1.10it/s, v_num=0]train_loss:  tensor(951.1799, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 10:  91%|█████████ | 10/11 [00:08<00:00,  1.11it/s, v_num=0]train_loss:  tensor(921.2322, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(919.2593, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(898.8453, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(841.5869, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(870.6495, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(888.0327, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(870.3411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(864.0536, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(847.7715, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(833.3074, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(817.5339, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 11:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(883.7145, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(864.0345, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:   9%|▉         | 1/11 [00:00<00:08,  1.19it/s, v_num=0]train_loss:  tensor(865.5361, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(831.2130, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(771.9607, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(802.7396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(781.4413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(817.6265, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(784.4976, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(754.8078, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(774.1111, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 12:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(753.6497, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(739.1011, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(758.0151, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(734.2579, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(734.5841, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(761.5367, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  45%|████▌     | 5/11 [00:04<00:05,  1.19it/s, v_num=0]train_loss:  tensor(717.9561, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  55%|█████▍    | 6/11 [00:05<00:04,  1.20it/s, v_num=0]train_loss:  tensor(741.1411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(723.2404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  73%|███████▎  | 8/11 [00:07<00:02,  1.14it/s, v_num=0]train_loss:  tensor(697.8702, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  82%|████████▏ | 9/11 [00:08<00:01,  1.11it/s, v_num=0]train_loss:  tensor(728.4832, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 13:  91%|█████████ | 10/11 [00:09<00:00,  1.11it/s, v_num=0]train_loss:  tensor(713.3542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(730.8701, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:   9%|▉         | 1/11 [00:00<00:09,  1.06it/s, v_num=0]train_loss:  tensor(745.4386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(770.2937, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(808.2590, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  36%|███▋      | 4/11 [00:04<00:07,  0.98it/s, v_num=0]train_loss:  tensor(770.5178, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  45%|████▌     | 5/11 [00:05<00:06,  0.99it/s, v_num=0]train_loss:  tensor(743.7089, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  55%|█████▍    | 6/11 [00:06<00:05,  0.99it/s, v_num=0]train_loss:  tensor(677.7382, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  64%|██████▎   | 7/11 [00:07<00:04,  0.99it/s, v_num=0]train_loss:  tensor(675.2712, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  73%|███████▎  | 8/11 [00:07<00:02,  1.01it/s, v_num=0]train_loss:  tensor(765.5499, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  82%|████████▏ | 9/11 [00:08<00:01,  1.02it/s, v_num=0]train_loss:  tensor(712.8586, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 14:  91%|█████████ | 10/11 [00:09<00:00,  1.02it/s, v_num=0]train_loss:  tensor(684.1954, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(671.7914, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:   9%|▉         | 1/11 [00:01<00:11,  0.89it/s, v_num=0]train_loss:  tensor(690.8760, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  18%|█▊        | 2/11 [00:01<00:08,  1.01it/s, v_num=0]train_loss:  tensor(679.0870, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  27%|██▋       | 3/11 [00:02<00:07,  1.04it/s, v_num=0]train_loss:  tensor(687.7288, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  36%|███▋      | 4/11 [00:03<00:06,  1.03it/s, v_num=0]train_loss:  tensor(691.3389, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  45%|████▌     | 5/11 [00:04<00:05,  1.06it/s, v_num=0]train_loss:  tensor(660.5109, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  55%|█████▍    | 6/11 [00:05<00:04,  1.09it/s, v_num=0]train_loss:  tensor(693.9791, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  64%|██████▎   | 7/11 [00:06<00:03,  1.11it/s, v_num=0]train_loss:  tensor(664.2899, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  73%|███████▎  | 8/11 [00:07<00:02,  1.12it/s, v_num=0]train_loss:  tensor(649.9487, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s, v_num=0]train_loss:  tensor(667.3745, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 15:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(664.9458, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(649.3044, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:   9%|▉         | 1/11 [00:01<00:12,  0.81it/s, v_num=0]train_loss:  tensor(653.3192, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  18%|█▊        | 2/11 [00:02<00:11,  0.81it/s, v_num=0]train_loss:  tensor(667.4814, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  27%|██▋       | 3/11 [00:03<00:09,  0.88it/s, v_num=0]train_loss:  tensor(685.7544, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  36%|███▋      | 4/11 [00:04<00:07,  0.94it/s, v_num=0]train_loss:  tensor(634.5018, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  45%|████▌     | 5/11 [00:05<00:06,  0.98it/s, v_num=0]train_loss:  tensor(670.7155, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  55%|█████▍    | 6/11 [00:05<00:04,  1.02it/s, v_num=0]train_loss:  tensor(676.3693, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  64%|██████▎   | 7/11 [00:06<00:03,  1.05it/s, v_num=0]train_loss:  tensor(588.3309, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  73%|███████▎  | 8/11 [00:07<00:02,  1.07it/s, v_num=0]train_loss:  tensor(623.2568, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  82%|████████▏ | 9/11 [00:08<00:01,  1.09it/s, v_num=0]train_loss:  tensor(617.7191, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 16:  91%|█████████ | 10/11 [00:09<00:00,  1.10it/s, v_num=0]train_loss:  tensor(673.1792, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(632.9778, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=0]train_loss:  tensor(639.2040, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(648.0262, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(626.6943, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(642.8362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(648.9392, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(623.7770, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  64%|██████▎   | 7/11 [00:06<00:03,  1.14it/s, v_num=0]train_loss:  tensor(607.5494, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  73%|███████▎  | 8/11 [00:07<00:02,  1.14it/s, v_num=0]train_loss:  tensor(612.0262, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(612.3848, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 17:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(633.9234, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(633.2272, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(595.3777, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  18%|█▊        | 2/11 [00:01<00:07,  1.16it/s, v_num=0]train_loss:  tensor(635.4175, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  27%|██▋       | 3/11 [00:02<00:07,  1.06it/s, v_num=0]train_loss:  tensor(586.4277, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(604.8317, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(566.1129, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(623.7069, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(606.7758, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(665.0270, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(651.6946, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 18:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(712.8167, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(643.8122, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(591.3516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  18%|█▊        | 2/11 [00:01<00:06,  1.32it/s, v_num=0]train_loss:  tensor(591.0120, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  27%|██▋       | 3/11 [00:02<00:06,  1.33it/s, v_num=0]train_loss:  tensor(584.9024, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  36%|███▋      | 4/11 [00:02<00:05,  1.34it/s, v_num=0]train_loss:  tensor(578.0035, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  45%|████▌     | 5/11 [00:03<00:04,  1.34it/s, v_num=0]train_loss:  tensor(592.3011, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  55%|█████▍    | 6/11 [00:04<00:03,  1.34it/s, v_num=0]train_loss:  tensor(593.6247, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  64%|██████▎   | 7/11 [00:05<00:02,  1.35it/s, v_num=0]train_loss:  tensor(574.0492, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  73%|███████▎  | 8/11 [00:05<00:02,  1.35it/s, v_num=0]train_loss:  tensor(552.6765, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  82%|████████▏ | 9/11 [00:06<00:01,  1.35it/s, v_num=0]train_loss:  tensor(563.5944, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 19:  91%|█████████ | 10/11 [00:07<00:00,  1.35it/s, v_num=0]train_loss:  tensor(578.9367, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(594.4494, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s, v_num=0]train_loss:  tensor(558.9017, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  18%|█▊        | 2/11 [00:01<00:06,  1.33it/s, v_num=0]train_loss:  tensor(555.9810, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  27%|██▋       | 3/11 [00:02<00:05,  1.33it/s, v_num=0]train_loss:  tensor(563.7694, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(564.9454, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  45%|████▌     | 5/11 [00:03<00:04,  1.29it/s, v_num=0]train_loss:  tensor(570.4482, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(534.7316, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(578.2257, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(566.0335, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(538.7515, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 20:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(548.0853, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(546.1017, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:   9%|▉         | 1/11 [00:00<00:08,  1.12it/s, v_num=0]train_loss:  tensor(532.2328, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(535.8398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(548.7235, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(524.1991, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(575.4364, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(541.6853, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  64%|██████▎   | 7/11 [00:05<00:03,  1.31it/s, v_num=0]train_loss:  tensor(569.0115, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  73%|███████▎  | 8/11 [00:06<00:02,  1.31it/s, v_num=0]train_loss:  tensor(555.0069, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  82%|████████▏ | 9/11 [00:06<00:01,  1.32it/s, v_num=0]train_loss:  tensor(560.6669, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 21:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(534.5359, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(521.0272, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(532.4288, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(526.7383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(555.8280, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(567.5833, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(584.6716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(598.0254, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(668.3782, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(655.5703, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(655.9066, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 22:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(608.2892, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(506.5977, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:   9%|▉         | 1/11 [00:01<00:10,  0.96it/s, v_num=0]train_loss:  tensor(556.9434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(571.2457, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  27%|██▋       | 3/11 [00:02<00:07,  1.12it/s, v_num=0]train_loss:  tensor(585.1512, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  36%|███▋      | 4/11 [00:03<00:06,  1.16it/s, v_num=0]train_loss:  tensor(546.0670, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(525.6917, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(560.8683, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(572.6865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(562.0272, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(532.5417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 23:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(532.5797, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(551.4524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(534.0597, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(536.1574, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(550.8107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(546.8615, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(504.8770, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(512.9839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(522.9672, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  73%|███████▎  | 8/11 [00:06<00:02,  1.30it/s, v_num=0]train_loss:  tensor(501.1663, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(515.9543, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 24:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(483.6979, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(504.3247, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(542.2786, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(506.3885, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(494.9176, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(492.4083, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(531.8027, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(519.7941, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(511.5251, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  73%|███████▎  | 8/11 [00:06<00:02,  1.31it/s, v_num=0]train_loss:  tensor(495.0143, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  82%|████████▏ | 9/11 [00:06<00:01,  1.31it/s, v_num=0]train_loss:  tensor(496.6469, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 25:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(517.7748, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(513.1675, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:   9%|▉         | 1/11 [00:01<00:11,  0.90it/s, v_num=0]train_loss:  tensor(495.5581, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  18%|█▊        | 2/11 [00:01<00:08,  1.02it/s, v_num=0]train_loss:  tensor(510.8907, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  27%|██▋       | 3/11 [00:02<00:07,  1.11it/s, v_num=0]train_loss:  tensor(503.1057, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=0]train_loss:  tensor(504.9550, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(475.8361, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(483.3300, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(507.9129, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(496.7629, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(487.6658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 26:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(474.0242, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(505.4976, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s, v_num=0]train_loss:  tensor(474.0983, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(492.2653, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  27%|██▋       | 3/11 [00:02<00:06,  1.28it/s, v_num=0]train_loss:  tensor(502.3935, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(549.2008, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(539.7589, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(602.2789, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(550.0178, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(519.9045, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(535.1901, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 27:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(478.0286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(484.2290, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:   9%|▉         | 1/11 [00:00<00:09,  1.06it/s, v_num=0]train_loss:  tensor(511.6868, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(536.3018, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(524.1974, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(505.4946, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(505.1438, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(480.0968, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(506.0667, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(493.1973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(492.1652, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 28:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(487.1472, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(476.9351, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(497.7426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(496.6673, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(493.8477, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  36%|███▋      | 4/11 [00:03<00:05,  1.31it/s, v_num=0]train_loss:  tensor(491.8195, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  45%|████▌     | 5/11 [00:03<00:04,  1.32it/s, v_num=0]train_loss:  tensor(477.6567, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  55%|█████▍    | 6/11 [00:04<00:03,  1.33it/s, v_num=0]train_loss:  tensor(467.7000, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  64%|██████▎   | 7/11 [00:05<00:03,  1.33it/s, v_num=0]train_loss:  tensor(503.7454, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  73%|███████▎  | 8/11 [00:06<00:02,  1.33it/s, v_num=0]train_loss:  tensor(471.5048, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  82%|████████▏ | 9/11 [00:06<00:01,  1.33it/s, v_num=0]train_loss:  tensor(474.0992, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 29:  91%|█████████ | 10/11 [00:07<00:00,  1.33it/s, v_num=0]train_loss:  tensor(474.1428, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(471.0207, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:   9%|▉         | 1/11 [00:00<00:09,  1.07it/s, v_num=0]train_loss:  tensor(479.8716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  18%|█▊        | 2/11 [00:01<00:07,  1.15it/s, v_num=0]train_loss:  tensor(479.8998, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(489.7981, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(459.9327, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(467.4494, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(445.8732, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(450.2993, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(508.2132, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(487.6170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 30:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(523.2576, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(491.2097, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:   9%|▉         | 1/11 [00:00<00:07,  1.31it/s, v_num=0]train_loss:  tensor(476.9597, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(533.5451, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(516.3952, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  36%|███▋      | 4/11 [00:03<00:05,  1.31it/s, v_num=0]train_loss:  tensor(507.5369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  45%|████▌     | 5/11 [00:03<00:04,  1.32it/s, v_num=0]train_loss:  tensor(491.1882, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  55%|█████▍    | 6/11 [00:04<00:03,  1.32it/s, v_num=0]train_loss:  tensor(475.7719, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  64%|██████▎   | 7/11 [00:05<00:03,  1.32it/s, v_num=0]train_loss:  tensor(479.6799, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  73%|███████▎  | 8/11 [00:06<00:02,  1.33it/s, v_num=0]train_loss:  tensor(463.9064, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  82%|████████▏ | 9/11 [00:06<00:01,  1.33it/s, v_num=0]train_loss:  tensor(468.4170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 31:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(444.1639, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(435.6887, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(496.3873, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  18%|█▊        | 2/11 [00:01<00:07,  1.15it/s, v_num=0]train_loss:  tensor(475.8028, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(450.5714, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(473.1119, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(468.8632, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(469.1737, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(476.3404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(465.2267, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(479.1215, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 32:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(439.8565, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(474.1006, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:   9%|▉         | 1/11 [00:00<00:09,  1.10it/s, v_num=0]train_loss:  tensor(449.8911, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  18%|█▊        | 2/11 [00:01<00:07,  1.15it/s, v_num=0]train_loss:  tensor(447.7734, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(449.8695, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(462.8168, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(481.1152, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(462.8616, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(488.8821, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(509.2996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(571.7163, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 33:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(679.4857, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(736.8255, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(658.0326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(509.3084, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(487.2297, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(570.2549, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(552.9592, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(503.7975, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(520.1973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(530.7345, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(492.3884, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 34:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(517.4246, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(538.2294, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s, v_num=0]train_loss:  tensor(522.9559, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  18%|█▊        | 2/11 [00:01<00:06,  1.32it/s, v_num=0]train_loss:  tensor(523.3108, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  27%|██▋       | 3/11 [00:02<00:06,  1.33it/s, v_num=0]train_loss:  tensor(534.2628, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(490.5280, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(519.5443, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  55%|█████▍    | 6/11 [00:04<00:03,  1.31it/s, v_num=0]train_loss:  tensor(528.0803, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(418.2953, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  73%|███████▎  | 8/11 [00:06<00:02,  1.31it/s, v_num=0]train_loss:  tensor(466.4326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  82%|████████▏ | 9/11 [00:06<00:01,  1.31it/s, v_num=0]train_loss:  tensor(483.9292, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 35:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(478.6796, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(473.4833, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:   9%|▉         | 1/11 [00:00<00:09,  1.09it/s, v_num=0]train_loss:  tensor(507.6876, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  18%|█▊        | 2/11 [00:01<00:07,  1.15it/s, v_num=0]train_loss:  tensor(470.8889, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(465.5185, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(461.0476, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(478.1524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(476.8859, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(447.5118, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(436.3099, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(441.5920, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 36:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(474.6035, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(446.8070, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(443.6038, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=0]train_loss:  tensor(465.6282, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  27%|██▋       | 3/11 [00:02<00:06,  1.32it/s, v_num=0]train_loss:  tensor(446.9534, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(474.5681, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  45%|████▌     | 5/11 [00:03<00:04,  1.29it/s, v_num=0]train_loss:  tensor(452.2988, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(456.6833, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(459.9863, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  73%|███████▎  | 8/11 [00:06<00:02,  1.31it/s, v_num=0]train_loss:  tensor(423.2781, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  82%|████████▏ | 9/11 [00:06<00:01,  1.31it/s, v_num=0]train_loss:  tensor(464.9680, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 37:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(418.8888, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(429.0975, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(417.7407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(463.4922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(445.4869, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(430.5184, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(478.8369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(464.6949, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(444.2723, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(454.2606, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(424.8314, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 38:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(445.6858, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(444.0356, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(430.7703, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(444.5065, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(439.2099, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  36%|███▋      | 4/11 [00:03<00:05,  1.30it/s, v_num=0]train_loss:  tensor(435.5862, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  45%|████▌     | 5/11 [00:03<00:04,  1.32it/s, v_num=0]train_loss:  tensor(427.5732, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  55%|█████▍    | 6/11 [00:04<00:03,  1.32it/s, v_num=0]train_loss:  tensor(461.7561, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  64%|██████▎   | 7/11 [00:05<00:03,  1.32it/s, v_num=0]train_loss:  tensor(473.0295, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  73%|███████▎  | 8/11 [00:06<00:02,  1.30it/s, v_num=0]train_loss:  tensor(415.4478, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(440.6066, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 39:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(451.7180, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(419.8899, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:   9%|▉         | 1/11 [00:00<00:09,  1.07it/s, v_num=0]train_loss:  tensor(436.2061, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(450.4490, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(430.1101, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(435.9412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(439.3997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(473.2261, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(435.7785, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(434.6248, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(424.8902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 40:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(451.2663, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(444.9484, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(430.3113, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(426.0525, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  27%|██▋       | 3/11 [00:02<00:06,  1.29it/s, v_num=0]train_loss:  tensor(418.5462, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(408.7016, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(452.7023, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(448.1154, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(450.2622, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(426.8320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(444.0110, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 41:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(431.0457, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(453.5664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:   9%|▉         | 1/11 [00:01<00:10,  0.98it/s, v_num=0]train_loss:  tensor(420.9455, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(423.8881, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(446.5555, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(450.6275, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(434.0217, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(442.1091, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(438.1710, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(451.7972, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(435.3856, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 42:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(424.2624, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(461.6676, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(489.7458, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  18%|█▊        | 2/11 [00:01<00:06,  1.31it/s, v_num=0]train_loss:  tensor(466.8900, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  27%|██▋       | 3/11 [00:02<00:06,  1.33it/s, v_num=0]train_loss:  tensor(438.4269, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  36%|███▋      | 4/11 [00:03<00:05,  1.33it/s, v_num=0]train_loss:  tensor(461.8582, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  45%|████▌     | 5/11 [00:03<00:04,  1.31it/s, v_num=0]train_loss:  tensor(460.4118, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  55%|█████▍    | 6/11 [00:04<00:03,  1.32it/s, v_num=0]train_loss:  tensor(430.2629, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  64%|██████▎   | 7/11 [00:05<00:03,  1.32it/s, v_num=0]train_loss:  tensor(423.7938, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  73%|███████▎  | 8/11 [00:06<00:02,  1.33it/s, v_num=0]train_loss:  tensor(444.2112, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  82%|████████▏ | 9/11 [00:06<00:01,  1.33it/s, v_num=0]train_loss:  tensor(446.1649, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 43:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(452.9928, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(422.4920, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(428.0695, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(447.1551, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  27%|██▋       | 3/11 [00:02<00:07,  1.12it/s, v_num=0]train_loss:  tensor(417.4856, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(426.5590, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(464.1000, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(465.9245, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(430.5223, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(429.5726, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(428.1885, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 44:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(452.6324, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(415.4022, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=0]train_loss:  tensor(435.8463, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(439.4258, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  27%|██▋       | 3/11 [00:02<00:06,  1.19it/s, v_num=0]train_loss:  tensor(435.7869, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=0]train_loss:  tensor(407.3659, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(419.2867, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(443.6901, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(410.8334, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(415.7904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(430.3654, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 45:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(430.7517, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(410.1649, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(431.3417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=0]train_loss:  tensor(434.7303, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(433.3107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(431.2874, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(427.2541, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(414.9997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(403.5513, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(433.9639, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(419.0142, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 46:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(425.6867, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(406.0273, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(449.2282, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  18%|█▊        | 2/11 [00:01<00:08,  1.10it/s, v_num=0]train_loss:  tensor(422.4633, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(420.0844, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(450.3123, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(428.2697, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(426.1050, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(454.5018, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(441.3198, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(519.0383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 47:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(552.5573, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(552.9505, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(478.6572, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  18%|█▊        | 2/11 [00:01<00:06,  1.32it/s, v_num=0]train_loss:  tensor(409.7301, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  27%|██▋       | 3/11 [00:02<00:06,  1.32it/s, v_num=0]train_loss:  tensor(435.7577, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(528.0233, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(487.4125, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(409.5355, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  64%|██████▎   | 7/11 [00:05<00:03,  1.21it/s, v_num=0]train_loss:  tensor(435.7087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  73%|███████▎  | 8/11 [00:06<00:02,  1.22it/s, v_num=0]train_loss:  tensor(456.1083, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(442.8123, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 48:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(427.2423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(448.3153, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(446.7562, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  18%|█▊        | 2/11 [00:01<00:06,  1.31it/s, v_num=0]train_loss:  tensor(439.4594, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  27%|██▋       | 3/11 [00:02<00:06,  1.32it/s, v_num=0]train_loss:  tensor(424.6579, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  36%|███▋      | 4/11 [00:03<00:05,  1.33it/s, v_num=0]train_loss:  tensor(467.5399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  45%|████▌     | 5/11 [00:03<00:04,  1.33it/s, v_num=0]train_loss:  tensor(430.3451, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  55%|█████▍    | 6/11 [00:04<00:03,  1.32it/s, v_num=0]train_loss:  tensor(396.2968, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  64%|██████▎   | 7/11 [00:05<00:03,  1.32it/s, v_num=0]train_loss:  tensor(425.6218, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  73%|███████▎  | 8/11 [00:06<00:02,  1.32it/s, v_num=0]train_loss:  tensor(420.2707, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  82%|████████▏ | 9/11 [00:06<00:01,  1.32it/s, v_num=0]train_loss:  tensor(418.1846, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 49:  91%|█████████ | 10/11 [00:07<00:00,  1.32it/s, v_num=0]train_loss:  tensor(433.2386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(427.7041, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(448.2154, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(409.7763, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(445.1585, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(400.6508, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(420.2895, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  55%|█████▍    | 6/11 [00:05<00:04,  1.20it/s, v_num=0]train_loss:  tensor(426.8881, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  64%|██████▎   | 7/11 [00:05<00:03,  1.21it/s, v_num=0]train_loss:  tensor(418.9549, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(398.4481, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(404.2783, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 50:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(430.7884, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(437.1419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(409.9439, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(385.1480, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(449.1450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  36%|███▋      | 4/11 [00:03<00:06,  1.05it/s, v_num=0]train_loss:  tensor(411.6031, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s, v_num=0]train_loss:  tensor(423.4904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  55%|█████▍    | 6/11 [00:05<00:04,  1.10it/s, v_num=0]train_loss:  tensor(429.5662, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  64%|██████▎   | 7/11 [00:06<00:03,  1.12it/s, v_num=0]train_loss:  tensor(402.0340, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(399.1778, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(402.7230, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 51:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(408.7227, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(403.0699, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(422.1177, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(399.5303, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  27%|██▋       | 3/11 [00:02<00:06,  1.17it/s, v_num=0]train_loss:  tensor(419.6556, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(430.6188, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(399.5109, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(424.7672, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(397.4807, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(411.7095, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(409.8205, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 52:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(424.8628, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(404.8562, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(422.7941, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(418.5544, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(396.9740, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(396.5970, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(405.4899, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(405.3452, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(432.8627, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(408.3937, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(405.3231, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 53:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(413.4573, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(416.9117, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(405.3840, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(402.9830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  27%|██▋       | 3/11 [00:02<00:07,  1.13it/s, v_num=0]train_loss:  tensor(376.7990, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  36%|███▋      | 4/11 [00:03<00:06,  1.04it/s, v_num=0]train_loss:  tensor(414.9902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s, v_num=0]train_loss:  tensor(425.3067, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  55%|█████▍    | 6/11 [00:05<00:04,  1.10it/s, v_num=0]train_loss:  tensor(417.1041, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  64%|██████▎   | 7/11 [00:06<00:03,  1.08it/s, v_num=0]train_loss:  tensor(395.0807, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  73%|███████▎  | 8/11 [00:07<00:02,  1.08it/s, v_num=0]train_loss:  tensor(431.4194, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  82%|████████▏ | 9/11 [00:08<00:01,  1.10it/s, v_num=0]train_loss:  tensor(397.7515, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 54:  91%|█████████ | 10/11 [00:08<00:00,  1.12it/s, v_num=0]train_loss:  tensor(407.1295, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(411.2028, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(390.8405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(415.4772, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(421.8220, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(413.0377, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  45%|████▌     | 5/11 [00:03<00:04,  1.25it/s, v_num=0]train_loss:  tensor(406.5180, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(412.4296, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(412.6855, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(387.0089, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(392.0025, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 55:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(416.3369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(398.3456, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(393.9434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(383.7105, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(396.2080, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(391.7830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(411.1094, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(405.2648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(402.3476, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(444.0109, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(423.0272, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 56:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(430.1212, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(404.9634, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(422.1628, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  18%|█▊        | 2/11 [00:01<00:08,  1.01it/s, v_num=0]train_loss:  tensor(416.6423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  27%|██▋       | 3/11 [00:02<00:07,  1.05it/s, v_num=0]train_loss:  tensor(399.1292, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  36%|███▋      | 4/11 [00:03<00:06,  1.09it/s, v_num=0]train_loss:  tensor(401.2719, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  45%|████▌     | 5/11 [00:04<00:05,  1.13it/s, v_num=0]train_loss:  tensor(408.1270, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(396.2482, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(412.7020, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=0]train_loss:  tensor(430.6961, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(394.3818, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 57:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(365.4054, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(419.2363, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=0]train_loss:  tensor(405.5450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(389.5584, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(430.4916, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(387.9134, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(429.5165, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(416.4816, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(411.7338, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(395.7732, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(394.8922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 58:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(383.8998, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(396.9222, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:   9%|▉         | 1/11 [00:01<00:10,  0.94it/s, v_num=0]train_loss:  tensor(397.0210, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  18%|█▊        | 2/11 [00:02<00:09,  0.94it/s, v_num=0]train_loss:  tensor(399.1916, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  27%|██▋       | 3/11 [00:02<00:07,  1.03it/s, v_num=0]train_loss:  tensor(405.6282, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  36%|███▋      | 4/11 [00:03<00:06,  1.09it/s, v_num=0]train_loss:  tensor(404.3125, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  45%|████▌     | 5/11 [00:04<00:05,  1.02it/s, v_num=0]train_loss:  tensor(401.4022, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  55%|█████▍    | 6/11 [00:05<00:04,  1.05it/s, v_num=0]train_loss:  tensor(397.6277, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  64%|██████▎   | 7/11 [00:06<00:03,  1.08it/s, v_num=0]train_loss:  tensor(374.6241, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  73%|███████▎  | 8/11 [00:07<00:02,  1.09it/s, v_num=0]train_loss:  tensor(415.9398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  82%|████████▏ | 9/11 [00:08<00:01,  1.11it/s, v_num=0]train_loss:  tensor(420.0587, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 59:  91%|█████████ | 10/11 [00:08<00:00,  1.12it/s, v_num=0]train_loss:  tensor(418.1759, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(380.4362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(421.9588, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(403.9175, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  27%|██▋       | 3/11 [00:02<00:07,  1.13it/s, v_num=0]train_loss:  tensor(417.8791, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  36%|███▋      | 4/11 [00:03<00:06,  1.16it/s, v_num=0]train_loss:  tensor(424.1744, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(427.5520, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(407.9634, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  64%|██████▎   | 7/11 [00:05<00:03,  1.21it/s, v_num=0]train_loss:  tensor(434.0648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(426.3326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(410.2834, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 60:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(396.3179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(400.5530, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(406.8599, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(412.6700, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(412.6277, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(422.7431, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(417.2910, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(395.4808, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(400.4151, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(381.0107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(377.1859, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 61:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(400.3279, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(401.9433, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:   9%|▉         | 1/11 [00:00<00:09,  1.09it/s, v_num=0]train_loss:  tensor(418.3265, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(391.6303, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(368.3618, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(403.6519, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  45%|████▌     | 5/11 [00:03<00:04,  1.25it/s, v_num=0]train_loss:  tensor(421.3396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(382.7432, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(402.9193, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(408.1597, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  82%|████████▏ | 9/11 [00:07<00:01,  1.29it/s, v_num=0]train_loss:  tensor(376.6476, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 62:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(422.0366, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(388.9526, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(389.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(402.7164, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(398.5724, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(387.9399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(408.5373, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=0]train_loss:  tensor(411.2686, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(426.1735, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(377.9278, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(411.1639, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 63:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(388.1573, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(406.9029, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:   9%|▉         | 1/11 [00:01<00:10,  0.99it/s, v_num=0]train_loss:  tensor(419.4865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  18%|█▊        | 2/11 [00:01<00:08,  1.12it/s, v_num=0]train_loss:  tensor(392.4333, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(396.2662, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(393.1516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(416.5729, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(388.2138, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(369.8564, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(400.8056, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(393.9623, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 64:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(395.6699, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(393.9848, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(405.8412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(378.3636, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(403.5966, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(411.8974, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(388.3525, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(372.7764, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(403.0732, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(400.9313, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(399.7675, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 65:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(395.8019, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(398.4367, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s, v_num=0]train_loss:  tensor(410.5948, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  18%|█▊        | 2/11 [00:01<00:07,  1.13it/s, v_num=0]train_loss:  tensor(394.1675, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(414.9362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(440.5794, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(422.7247, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(430.9385, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(384.6362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(411.2570, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(429.7614, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 66:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(398.4260, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(384.5397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(419.1320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(401.0790, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(408.4430, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(404.1001, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(405.7246, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(399.5992, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(408.0916, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(393.8038, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(416.1795, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 67:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(384.5977, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(400.5052, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:   9%|▉         | 1/11 [00:00<00:09,  1.09it/s, v_num=0]train_loss:  tensor(409.9822, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(386.8029, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=0]train_loss:  tensor(380.7023, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  36%|███▋      | 4/11 [00:03<00:06,  1.14it/s, v_num=0]train_loss:  tensor(378.1232, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(383.4164, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  55%|█████▍    | 6/11 [00:05<00:04,  1.09it/s, v_num=0]train_loss:  tensor(419.5838, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  64%|██████▎   | 7/11 [00:06<00:03,  1.11it/s, v_num=0]train_loss:  tensor(388.6950, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  73%|███████▎  | 8/11 [00:07<00:02,  1.12it/s, v_num=0]train_loss:  tensor(417.1489, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s, v_num=0]train_loss:  tensor(375.9807, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 68:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(367.8661, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(395.6796, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(394.1816, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(373.6341, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(382.3423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(390.3478, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(387.5781, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(414.0549, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(393.9091, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  73%|███████▎  | 8/11 [00:06<00:02,  1.22it/s, v_num=0]train_loss:  tensor(396.5139, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(370.1255, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 69:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(391.4291, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(404.0813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:   9%|▉         | 1/11 [00:01<00:11,  0.90it/s, v_num=0]train_loss:  tensor(372.3671, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(393.1388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  27%|██▋       | 3/11 [00:02<00:07,  1.11it/s, v_num=0]train_loss:  tensor(399.5690, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  36%|███▋      | 4/11 [00:03<00:06,  1.14it/s, v_num=0]train_loss:  tensor(402.5124, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(381.8631, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=0]train_loss:  tensor(400.9742, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(398.1466, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(385.8794, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(362.6551, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 70:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(373.1281, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(385.8953, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(394.3571, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(390.8204, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(393.3905, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(380.6808, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(386.7997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(374.8499, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(374.2686, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(400.1912, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(374.2531, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 71:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(402.0485, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(403.0702, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(360.5112, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(393.1289, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(382.2448, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(391.4036, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(377.3452, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(402.5239, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(395.0666, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(381.9517, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s, v_num=0]train_loss:  tensor(372.4606, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 72:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(383.4221, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(408.6600, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(406.8813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(380.6722, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(394.3122, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(411.0285, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(393.0614, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(417.3737, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(435.8242, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(419.6190, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(442.2214, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 73:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(440.5277, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(416.7831, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=0]train_loss:  tensor(391.1279, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  18%|█▊        | 2/11 [00:01<00:08,  1.10it/s, v_num=0]train_loss:  tensor(395.6693, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(406.5053, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(399.7492, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(401.8068, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(373.8615, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(396.7719, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(391.5102, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(418.1171, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 74:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(396.4307, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(396.3090, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(377.1790, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(410.3908, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(426.8947, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(388.2586, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(353.3584, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(398.3777, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(398.9330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(378.4767, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(398.0884, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 75:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(358.3355, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(406.7698, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:   9%|▉         | 1/11 [00:01<00:10,  0.93it/s, v_num=0]train_loss:  tensor(406.5274, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  18%|█▊        | 2/11 [00:01<00:08,  1.05it/s, v_num=0]train_loss:  tensor(378.5136, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  27%|██▋       | 3/11 [00:02<00:07,  1.11it/s, v_num=0]train_loss:  tensor(382.5819, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=0]train_loss:  tensor(375.3570, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(384.8040, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  55%|█████▍    | 6/11 [00:05<00:04,  1.09it/s, v_num=0]train_loss:  tensor(390.6766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  64%|██████▎   | 7/11 [00:06<00:03,  1.11it/s, v_num=0]train_loss:  tensor(365.6943, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  73%|███████▎  | 8/11 [00:07<00:02,  1.12it/s, v_num=0]train_loss:  tensor(381.8387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s, v_num=0]train_loss:  tensor(383.5569, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 76:  91%|█████████ | 10/11 [00:08<00:00,  1.14it/s, v_num=0]train_loss:  tensor(374.0270, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(374.3140, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(383.7159, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(375.0294, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(390.4373, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(361.2769, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(379.9601, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(391.6026, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(389.1494, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(407.1163, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(371.5226, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 77:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(376.7188, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(405.7626, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(390.6036, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(361.5654, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(389.7541, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(377.7412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(370.5235, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(384.0113, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(379.7902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(373.2630, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(377.9427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 78:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(359.2794, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(387.1971, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=0]train_loss:  tensor(382.3778, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  18%|█▊        | 2/11 [00:01<00:08,  1.12it/s, v_num=0]train_loss:  tensor(387.8464, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  27%|██▋       | 3/11 [00:02<00:06,  1.17it/s, v_num=0]train_loss:  tensor(392.6490, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(370.7243, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  45%|████▌     | 5/11 [00:04<00:05,  1.19it/s, v_num=0]train_loss:  tensor(374.8554, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(372.2388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(403.0562, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(360.0455, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(381.4115, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 79:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(354.5478, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(383.5072, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:   9%|▉         | 1/11 [00:01<00:13,  0.76it/s, v_num=0]train_loss:  tensor(381.5794, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  18%|█▊        | 2/11 [00:02<00:09,  0.95it/s, v_num=0]train_loss:  tensor(371.0450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  27%|██▋       | 3/11 [00:02<00:07,  1.04it/s, v_num=0]train_loss:  tensor(381.5162, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  36%|███▋      | 4/11 [00:03<00:06,  1.10it/s, v_num=0]train_loss:  tensor(383.6454, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=0]train_loss:  tensor(370.5132, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(372.9649, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(355.7527, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(397.8387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(364.0514, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 80:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(406.7908, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(388.8530, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(368.3566, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(399.0013, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(393.8464, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(361.7603, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(405.2090, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(385.1468, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(367.7267, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(392.1902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(345.1508, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 81:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(348.5388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(362.1068, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(397.9026, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(372.0062, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  27%|██▋       | 3/11 [00:02<00:07,  1.02it/s, v_num=0]train_loss:  tensor(383.1276, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  36%|███▋      | 4/11 [00:03<00:06,  1.08it/s, v_num=0]train_loss:  tensor(383.2300, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  45%|████▌     | 5/11 [00:04<00:05,  1.09it/s, v_num=0]train_loss:  tensor(366.4995, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  55%|█████▍    | 6/11 [00:05<00:04,  1.11it/s, v_num=0]train_loss:  tensor(368.7862, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  64%|██████▎   | 7/11 [00:06<00:03,  1.13it/s, v_num=0]train_loss:  tensor(394.5192, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(375.1346, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(401.6600, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 82:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(376.0666, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(396.0449, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(355.2035, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(362.5307, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(388.9792, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(392.5386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(382.4980, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(368.7896, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(358.8542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(390.6534, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(371.0542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 83:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(387.1472, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(383.0357, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:   9%|▉         | 1/11 [00:01<00:11,  0.89it/s, v_num=0]train_loss:  tensor(361.6673, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  18%|█▊        | 2/11 [00:01<00:08,  1.06it/s, v_num=0]train_loss:  tensor(391.6223, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=0]train_loss:  tensor(374.2060, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  36%|███▋      | 4/11 [00:03<00:06,  1.14it/s, v_num=0]train_loss:  tensor(380.0508, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(374.7375, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  55%|█████▍    | 6/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(375.0761, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(378.2714, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(376.1639, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(388.1940, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 84:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(436.4648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(441.7903, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(458.8908, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(479.6274, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(447.8909, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(396.8055, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(397.5870, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(428.0699, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(439.8107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(393.1458, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(381.5063, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 85:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(374.0766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(443.1685, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s, v_num=0]train_loss:  tensor(389.6870, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(390.4142, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(408.3534, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(415.7691, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=0]train_loss:  tensor(396.0031, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(365.2952, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(375.0756, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(389.0224, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(369.8805, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 86:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(372.3190, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(364.0186, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(386.4378, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(377.5280, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(386.4197, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  36%|███▋      | 4/11 [00:03<00:06,  1.11it/s, v_num=0]train_loss:  tensor(391.5718, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=0]train_loss:  tensor(368.0998, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  55%|█████▍    | 6/11 [00:05<00:04,  1.10it/s, v_num=0]train_loss:  tensor(369.0427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  64%|██████▎   | 7/11 [00:06<00:03,  1.13it/s, v_num=0]train_loss:  tensor(375.6354, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(384.8701, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(378.0460, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 87:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(399.7389, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(398.0477, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(380.8317, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  18%|█▊        | 2/11 [00:01<00:06,  1.31it/s, v_num=0]train_loss:  tensor(364.4791, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  27%|██▋       | 3/11 [00:02<00:06,  1.31it/s, v_num=0]train_loss:  tensor(359.3418, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(338.8809, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(364.4208, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(361.9505, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(398.8286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(409.8936, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(385.5238, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 88:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(350.1286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(375.3907, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:   9%|▉         | 1/11 [00:01<00:11,  0.86it/s, v_num=0]train_loss:  tensor(355.0909, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  18%|█▊        | 2/11 [00:01<00:08,  1.04it/s, v_num=0]train_loss:  tensor(369.0434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  27%|██▋       | 3/11 [00:02<00:07,  1.09it/s, v_num=0]train_loss:  tensor(371.8582, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  36%|███▋      | 4/11 [00:03<00:06,  1.14it/s, v_num=0]train_loss:  tensor(392.3000, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(348.8202, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  55%|█████▍    | 6/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(381.4230, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(377.7333, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(369.9684, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(365.7886, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 89:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(383.7726, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(363.8849, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(351.5406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  18%|█▊        | 2/11 [00:01<00:07,  1.17it/s, v_num=0]train_loss:  tensor(386.5172, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(383.4085, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(352.6912, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s, v_num=0]train_loss:  tensor(394.6138, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  55%|█████▍    | 6/11 [00:05<00:04,  1.06it/s, v_num=0]train_loss:  tensor(366.3931, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  64%|██████▎   | 7/11 [00:06<00:03,  1.01it/s, v_num=0]train_loss:  tensor(357.0326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  73%|███████▎  | 8/11 [00:08<00:03,  1.00it/s, v_num=0]train_loss:  tensor(367.8233, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  82%|████████▏ | 9/11 [00:08<00:01,  1.03it/s, v_num=0]train_loss:  tensor(371.0018, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 90:  91%|█████████ | 10/11 [00:09<00:00,  1.05it/s, v_num=0]train_loss:  tensor(386.5927, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(358.5427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:   9%|▉         | 1/11 [00:01<00:10,  0.98it/s, v_num=0]train_loss:  tensor(348.3642, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(394.0143, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  27%|██▋       | 3/11 [00:02<00:07,  1.14it/s, v_num=0]train_loss:  tensor(378.5542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(362.6575, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(372.3666, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  55%|█████▍    | 6/11 [00:05<00:04,  1.20it/s, v_num=0]train_loss:  tensor(357.2208, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(368.3801, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(370.0786, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(367.8200, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 91:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(386.5526, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(362.4879, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:   9%|▉         | 1/11 [00:00<00:08,  1.12it/s, v_num=0]train_loss:  tensor(403.2902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  18%|█▊        | 2/11 [00:01<00:07,  1.16it/s, v_num=0]train_loss:  tensor(348.0361, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  27%|██▋       | 3/11 [00:02<00:07,  1.04it/s, v_num=0]train_loss:  tensor(356.5255, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  36%|███▋      | 4/11 [00:03<00:06,  1.09it/s, v_num=0]train_loss:  tensor(355.1393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  45%|████▌     | 5/11 [00:04<00:05,  1.12it/s, v_num=0]train_loss:  tensor(383.4739, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(362.2153, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(374.1724, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(367.2367, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(348.6846, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 92:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(394.6603, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(344.1751, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:   9%|▉         | 1/11 [00:01<00:10,  0.97it/s, v_num=0]train_loss:  tensor(364.8016, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(350.3466, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(374.0178, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(381.0839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(363.2750, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(384.3826, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(386.0371, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(365.0851, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(361.7200, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 93:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(359.4275, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(356.2303, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(387.6435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(380.9711, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(366.1885, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(384.5560, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(358.4025, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(368.8273, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(351.7403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(355.0192, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(378.8578, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 94:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(373.3096, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(372.3826, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:   9%|▉         | 1/11 [00:01<00:10,  0.94it/s, v_num=0]train_loss:  tensor(365.2012, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(373.7687, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(398.7399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(348.9183, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(355.2896, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(349.3949, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(350.9426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(398.0016, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(367.2410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 95:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(346.2230, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(377.2579, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:   9%|▉         | 1/11 [00:00<00:09,  1.09it/s, v_num=0]train_loss:  tensor(351.2396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  18%|█▊        | 2/11 [00:01<00:08,  1.00it/s, v_num=0]train_loss:  tensor(372.1423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  27%|██▋       | 3/11 [00:02<00:07,  1.05it/s, v_num=0]train_loss:  tensor(369.7527, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  36%|███▋      | 4/11 [00:03<00:06,  1.09it/s, v_num=0]train_loss:  tensor(361.1237, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  45%|████▌     | 5/11 [00:04<00:05,  1.13it/s, v_num=0]train_loss:  tensor(359.3529, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=0]train_loss:  tensor(352.7381, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(385.9006, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=0]train_loss:  tensor(378.3708, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(369.6102, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 96:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(368.7560, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(371.4236, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(392.7763, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(391.9184, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(414.1240, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(385.1207, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(422.0371, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(356.9685, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  64%|██████▎   | 7/11 [00:05<00:03,  1.21it/s, v_num=0]train_loss:  tensor(351.0443, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(369.1219, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(370.3613, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 97:  91%|█████████ | 10/11 [00:08<00:00,  1.16it/s, v_num=0]train_loss:  tensor(340.7379, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(404.5785, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:   9%|▉         | 1/11 [00:00<00:07,  1.29it/s, v_num=0]train_loss:  tensor(394.1893, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  18%|█▊        | 2/11 [00:01<00:06,  1.32it/s, v_num=0]train_loss:  tensor(358.2297, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  27%|██▋       | 3/11 [00:02<00:06,  1.33it/s, v_num=0]train_loss:  tensor(366.1321, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  36%|███▋      | 4/11 [00:03<00:05,  1.33it/s, v_num=0]train_loss:  tensor(345.0681, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  45%|████▌     | 5/11 [00:03<00:04,  1.32it/s, v_num=0]train_loss:  tensor(380.9046, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(357.1744, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(363.2068, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(347.4023, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(360.0490, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 98:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(395.2619, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(364.9579, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s, v_num=0]train_loss:  tensor(365.2437, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  18%|█▊        | 2/11 [00:01<00:07,  1.17it/s, v_num=0]train_loss:  tensor(356.2646, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(371.3954, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(348.7928, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(377.0595, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(358.1331, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  64%|██████▎   | 7/11 [00:06<00:03,  1.12it/s, v_num=0]train_loss:  tensor(371.1446, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  73%|███████▎  | 8/11 [00:07<00:02,  1.14it/s, v_num=0]train_loss:  tensor(360.5899, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(348.6741, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 99:  91%|█████████ | 10/11 [00:08<00:00,  1.16it/s, v_num=0]train_loss:  tensor(394.1261, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]        train_loss:  tensor(349.9317, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:   9%|▉         | 1/11 [00:00<00:08,  1.17it/s, v_num=0]train_loss:  tensor(369.8548, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(373.1439, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(378.5803, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(349.4909, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(361.3940, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(364.2000, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(346.2621, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(343.3124, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(366.6320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 100:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(377.1771, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(340.5789, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:   9%|▉         | 1/11 [00:00<00:09,  1.06it/s, v_num=0]train_loss:  tensor(369.2129, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  18%|█▊        | 2/11 [00:01<00:08,  1.03it/s, v_num=0]train_loss:  tensor(376.2449, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=0]train_loss:  tensor(340.9505, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=0]train_loss:  tensor(357.5420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(351.1336, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(367.9521, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(378.2894, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(370.9236, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(340.3438, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 101:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(376.2858, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(359.4803, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:   9%|▉         | 1/11 [00:00<00:08,  1.19it/s, v_num=0]train_loss:  tensor(357.3945, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(358.8574, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  27%|██▋       | 3/11 [00:02<00:07,  1.12it/s, v_num=0]train_loss:  tensor(367.8710, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=0]train_loss:  tensor(385.0543, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  45%|████▌     | 5/11 [00:04<00:05,  1.07it/s, v_num=0]train_loss:  tensor(392.0909, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  55%|█████▍    | 6/11 [00:05<00:04,  1.10it/s, v_num=0]train_loss:  tensor(407.0405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  64%|██████▎   | 7/11 [00:06<00:03,  1.11it/s, v_num=0]train_loss:  tensor(390.7958, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  73%|███████▎  | 8/11 [00:07<00:02,  1.11it/s, v_num=0]train_loss:  tensor(468.5378, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  82%|████████▏ | 9/11 [00:08<00:01,  1.12it/s, v_num=0]train_loss:  tensor(431.1439, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 102:  91%|█████████ | 10/11 [00:08<00:00,  1.14it/s, v_num=0]train_loss:  tensor(359.7922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(360.2756, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(360.3260, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(406.1808, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(412.2904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(372.7560, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(376.4375, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(346.7590, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(392.3525, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(379.4378, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(400.3124, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 103:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(345.6626, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(357.7530, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(393.8569, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(370.3340, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(342.6331, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(367.2856, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(379.8766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(347.8140, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(361.7814, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(356.7458, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(366.7216, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 104:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(366.0250, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(352.8432, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=0]train_loss:  tensor(378.7195, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(351.1837, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  27%|██▋       | 3/11 [00:02<00:07,  1.13it/s, v_num=0]train_loss:  tensor(373.9119, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  36%|███▋      | 4/11 [00:03<00:06,  1.17it/s, v_num=0]train_loss:  tensor(342.0013, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  45%|████▌     | 5/11 [00:04<00:05,  1.19it/s, v_num=0]train_loss:  tensor(329.3324, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  55%|█████▍    | 6/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(353.5489, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  64%|██████▎   | 7/11 [00:05<00:03,  1.21it/s, v_num=0]train_loss:  tensor(346.3227, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(373.4142, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(355.9025, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 105:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(381.9413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(368.9991, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(349.8226, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(344.4156, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(356.7814, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  36%|███▋      | 4/11 [00:03<00:05,  1.31it/s, v_num=0]train_loss:  tensor(344.5355, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(355.3970, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(373.4203, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  64%|██████▎   | 7/11 [00:05<00:03,  1.30it/s, v_num=0]train_loss:  tensor(344.7457, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  73%|███████▎  | 8/11 [00:06<00:02,  1.30it/s, v_num=0]train_loss:  tensor(364.3290, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(342.6467, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 106:  91%|█████████ | 10/11 [00:07<00:00,  1.31it/s, v_num=0]train_loss:  tensor(335.6207, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(355.6172, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:   9%|▉         | 1/11 [00:00<00:09,  1.08it/s, v_num=0]train_loss:  tensor(369.0570, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  18%|█▊        | 2/11 [00:01<00:07,  1.13it/s, v_num=0]train_loss:  tensor(339.8427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  27%|██▋       | 3/11 [00:02<00:06,  1.19it/s, v_num=0]train_loss:  tensor(361.5685, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(356.1888, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(350.1122, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(343.5942, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(346.7939, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(342.9239, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(346.9174, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 107:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(338.8788, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(352.2787, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(358.8348, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  18%|█▊        | 2/11 [00:02<00:10,  0.90it/s, v_num=0]train_loss:  tensor(334.8171, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  27%|██▋       | 3/11 [00:02<00:07,  1.00it/s, v_num=0]train_loss:  tensor(314.8204, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(360.4681, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  45%|████▌     | 5/11 [00:04<00:05,  1.11it/s, v_num=0]train_loss:  tensor(349.8914, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(345.9539, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(355.7619, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(349.3377, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(355.7542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 108:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(374.2624, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(359.5608, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s, v_num=0]train_loss:  tensor(348.7773, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  18%|█▊        | 2/11 [00:01<00:07,  1.15it/s, v_num=0]train_loss:  tensor(362.9279, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  27%|██▋       | 3/11 [00:02<00:06,  1.17it/s, v_num=0]train_loss:  tensor(341.4800, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(356.1295, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(359.1696, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(346.0209, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(340.3826, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(356.7060, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(362.9740, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 109:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(371.9042, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(359.6599, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(368.5058, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=0]train_loss:  tensor(352.5536, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(352.5602, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(348.7716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(337.3588, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(338.8193, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(364.7001, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(356.1187, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(337.9503, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 110:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(313.2831, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(329.7266, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:   9%|▉         | 1/11 [00:00<00:07,  1.30it/s, v_num=0]train_loss:  tensor(353.2736, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(348.8723, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(327.3788, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(334.1862, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  45%|████▌     | 5/11 [00:03<00:04,  1.30it/s, v_num=0]train_loss:  tensor(351.0089, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  55%|█████▍    | 6/11 [00:04<00:03,  1.30it/s, v_num=0]train_loss:  tensor(361.6474, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  64%|██████▎   | 7/11 [00:05<00:03,  1.31it/s, v_num=0]train_loss:  tensor(339.7573, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  73%|███████▎  | 8/11 [00:06<00:02,  1.30it/s, v_num=0]train_loss:  tensor(340.1134, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(342.2322, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 111:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(348.4271, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(331.6972, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(343.5836, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  18%|█▊        | 2/11 [00:01<00:08,  1.03it/s, v_num=0]train_loss:  tensor(359.2758, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(337.8225, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(353.1372, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(345.8664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(327.0817, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(341.2405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  73%|███████▎  | 8/11 [00:07<00:02,  1.08it/s, v_num=0]train_loss:  tensor(346.4867, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  82%|████████▏ | 9/11 [00:08<00:01,  1.10it/s, v_num=0]train_loss:  tensor(318.7714, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 112:  91%|█████████ | 10/11 [00:08<00:00,  1.12it/s, v_num=0]train_loss:  tensor(349.5902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(344.9129, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:   9%|▉         | 1/11 [00:00<00:08,  1.19it/s, v_num=0]train_loss:  tensor(343.5308, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(317.5688, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(332.0644, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(327.4767, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(339.7723, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(331.0466, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(337.1160, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  73%|███████▎  | 8/11 [00:06<00:02,  1.22it/s, v_num=0]train_loss:  tensor(351.6368, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(340.8252, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 113:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(338.2571, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(333.1164, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:   9%|▉         | 1/11 [00:01<00:11,  0.84it/s, v_num=0]train_loss:  tensor(320.3902, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  18%|█▊        | 2/11 [00:02<00:09,  0.98it/s, v_num=0]train_loss:  tensor(331.6729, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  27%|██▋       | 3/11 [00:02<00:07,  1.03it/s, v_num=0]train_loss:  tensor(323.2383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  36%|███▋      | 4/11 [00:03<00:06,  1.08it/s, v_num=0]train_loss:  tensor(340.9075, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  45%|████▌     | 5/11 [00:04<00:05,  1.12it/s, v_num=0]train_loss:  tensor(370.6073, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=0]train_loss:  tensor(329.0844, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(335.6179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(347.1330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(333.7345, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 114:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(361.4533, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(381.1408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:   9%|▉         | 1/11 [00:01<00:11,  0.87it/s, v_num=0]train_loss:  tensor(406.1912, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  18%|█▊        | 2/11 [00:01<00:08,  1.01it/s, v_num=0]train_loss:  tensor(377.8139, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  27%|██▋       | 3/11 [00:02<00:07,  1.05it/s, v_num=0]train_loss:  tensor(375.4757, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  36%|███▋      | 4/11 [00:03<00:06,  1.09it/s, v_num=0]train_loss:  tensor(359.1149, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  45%|████▌     | 5/11 [00:04<00:05,  1.13it/s, v_num=0]train_loss:  tensor(308.3787, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=0]train_loss:  tensor(328.4678, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(339.9094, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=0]train_loss:  tensor(354.5342, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(356.5659, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 115:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(372.4018, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(355.0747, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:   9%|▉         | 1/11 [00:01<00:12,  0.77it/s, v_num=0]train_loss:  tensor(333.4344, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  18%|█▊        | 2/11 [00:02<00:09,  0.90it/s, v_num=0]train_loss:  tensor(335.3464, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  27%|██▋       | 3/11 [00:03<00:08,  0.99it/s, v_num=0]train_loss:  tensor(345.2107, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  36%|███▋      | 4/11 [00:03<00:06,  1.06it/s, v_num=0]train_loss:  tensor(325.1450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  45%|████▌     | 5/11 [00:04<00:05,  1.10it/s, v_num=0]train_loss:  tensor(328.0844, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(325.0626, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(337.3170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(323.1750, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(329.7110, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 116:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(354.6099, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(338.8858, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(321.5801, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(322.2548, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  27%|██▋       | 3/11 [00:02<00:06,  1.19it/s, v_num=0]train_loss:  tensor(343.8531, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(345.3613, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(327.1743, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(326.9958, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(319.2587, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(339.4656, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(314.1402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 117:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(338.8013, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(326.8413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:   9%|▉         | 1/11 [00:01<00:12,  0.78it/s, v_num=0]train_loss:  tensor(317.6936, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  18%|█▊        | 2/11 [00:02<00:09,  0.96it/s, v_num=0]train_loss:  tensor(308.6556, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  27%|██▋       | 3/11 [00:03<00:08,  0.98it/s, v_num=0]train_loss:  tensor(326.0664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  36%|███▋      | 4/11 [00:03<00:06,  1.05it/s, v_num=0]train_loss:  tensor(334.9765, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  45%|████▌     | 5/11 [00:04<00:05,  1.09it/s, v_num=0]train_loss:  tensor(318.0669, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  55%|█████▍    | 6/11 [00:05<00:04,  1.12it/s, v_num=0]train_loss:  tensor(317.2838, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(338.1179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(353.3895, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(337.9003, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 118:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(322.6922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(342.0203, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(318.2041, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(337.3766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(329.7577, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(334.8802, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(335.5991, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(328.9330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(333.7817, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(314.9214, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(325.4608, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 119:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(323.5818, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(325.8296, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(318.6448, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(351.9461, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  27%|██▋       | 3/11 [00:02<00:06,  1.28it/s, v_num=0]train_loss:  tensor(328.8875, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(320.8421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(332.9117, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(308.4435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(334.9519, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(316.5264, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(303.3904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 120:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(352.8972, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(336.1445, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s, v_num=0]train_loss:  tensor(315.5564, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(319.7744, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(346.2962, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(326.0083, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(326.2257, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(314.5307, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(305.7097, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(325.8369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(328.5854, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 121:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(344.4553, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(356.4650, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(329.7783, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(330.9700, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(372.8321, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(358.8987, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  45%|████▌     | 5/11 [00:04<00:05,  1.10it/s, v_num=0]train_loss:  tensor(342.4286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(344.5412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(341.3064, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(300.3499, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s, v_num=0]train_loss:  tensor(317.9964, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 122:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(348.1487, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(350.4048, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(360.4150, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(318.1366, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(308.6806, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(333.3600, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(351.4957, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(340.9034, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(326.2025, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  73%|███████▎  | 8/11 [00:06<00:02,  1.30it/s, v_num=0]train_loss:  tensor(313.3274, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(320.5791, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 123:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(325.0450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(350.4322, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(339.6383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(338.5234, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  27%|██▋       | 3/11 [00:02<00:06,  1.17it/s, v_num=0]train_loss:  tensor(335.9451, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(332.0881, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(290.1951, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=0]train_loss:  tensor(331.0558, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(342.9459, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(321.9734, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(309.7559, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 124:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(306.4073, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(308.7422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:   9%|▉         | 1/11 [00:00<00:08,  1.18it/s, v_num=0]train_loss:  tensor(323.5996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(336.6236, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(323.7906, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(312.5648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(313.3052, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(338.8015, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(302.3664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(311.5865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(329.2087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 125:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(322.9706, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(310.1490, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(337.8384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  18%|█▊        | 2/11 [00:01<00:07,  1.22it/s, v_num=0]train_loss:  tensor(316.1688, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(313.3741, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(313.1450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(311.8131, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(295.4095, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(315.7263, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(324.8855, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(337.3698, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 126:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(310.0080, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(324.2318, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(312.9723, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(320.1353, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(298.1173, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(315.3785, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(309.4114, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(325.3306, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(295.4227, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(305.5715, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(348.3885, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 127:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(313.9424, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(305.4444, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(328.7350, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(302.6271, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  27%|██▋       | 3/11 [00:02<00:07,  1.13it/s, v_num=0]train_loss:  tensor(314.1397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(306.4978, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(310.1982, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(311.0174, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(305.1393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(317.5722, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(314.1415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 128:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(337.2258, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(315.8371, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(324.7925, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(311.4806, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(322.0532, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(318.2101, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(313.3131, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(300.6254, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(305.3864, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(310.7909, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(295.5408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 129:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(297.0853, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(292.3410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(300.3670, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(289.4581, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  27%|██▋       | 3/11 [00:02<00:07,  1.14it/s, v_num=0]train_loss:  tensor(337.8854, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(300.1800, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(305.8901, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(316.4802, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(318.5085, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(315.9365, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(303.2727, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 130:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(324.7672, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(321.7973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(358.8085, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(343.0884, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(362.1007, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(337.7170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(329.4739, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(305.6994, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(321.0063, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(305.1461, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(311.7190, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 131:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(349.7841, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(330.8441, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(320.7480, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(292.0300, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  27%|██▋       | 3/11 [00:02<00:06,  1.29it/s, v_num=0]train_loss:  tensor(310.1205, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  36%|███▋      | 4/11 [00:03<00:05,  1.29it/s, v_num=0]train_loss:  tensor(335.0882, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  45%|████▌     | 5/11 [00:03<00:04,  1.29it/s, v_num=0]train_loss:  tensor(342.0551, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(314.8060, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(304.2672, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(298.8114, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  82%|████████▏ | 9/11 [00:07<00:01,  1.29it/s, v_num=0]train_loss:  tensor(326.8541, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 132:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(334.7330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(313.9657, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(302.3098, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(299.3778, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(290.9667, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(322.8273, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(318.7003, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(307.2383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(302.8762, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(301.1873, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(323.7533, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 133:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(307.3330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(282.3797, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(290.2484, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(302.2370, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  27%|██▋       | 3/11 [00:02<00:07,  1.14it/s, v_num=0]train_loss:  tensor(317.1103, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(316.6997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(304.8226, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(309.2863, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(301.7173, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(303.3342, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(297.0332, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 134:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(315.4267, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(311.1929, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(306.9003, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(304.8885, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(317.7049, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(297.5343, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(294.0320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  55%|█████▍    | 6/11 [00:04<00:03,  1.25it/s, v_num=0]train_loss:  tensor(299.0034, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(299.4476, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(287.3377, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(295.5115, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 135:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(310.4281, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(316.9047, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(306.4449, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  18%|█▊        | 2/11 [00:01<00:08,  1.04it/s, v_num=0]train_loss:  tensor(303.8007, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  27%|██▋       | 3/11 [00:02<00:07,  1.12it/s, v_num=0]train_loss:  tensor(299.2624, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  36%|███▋      | 4/11 [00:03<00:06,  1.16it/s, v_num=0]train_loss:  tensor(312.1608, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  45%|████▌     | 5/11 [00:04<00:05,  1.13it/s, v_num=0]train_loss:  tensor(287.7031, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=0]train_loss:  tensor(304.2377, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=0]train_loss:  tensor(295.8197, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=0]train_loss:  tensor(297.4917, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  82%|████████▏ | 9/11 [00:07<00:01,  1.13it/s, v_num=0]train_loss:  tensor(297.5857, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 136:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=0]train_loss:  tensor(297.1192, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(301.4313, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(294.2724, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(305.7993, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(280.7654, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(296.8124, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(297.0516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(309.0156, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(305.6286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(301.8644, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(299.5832, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 137:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(312.2506, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(315.6648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(301.7238, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  18%|█▊        | 2/11 [00:01<00:08,  1.08it/s, v_num=0]train_loss:  tensor(289.3447, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(307.0922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(276.0894, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(282.2071, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(294.6699, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(309.2303, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(298.3813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(302.5627, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 138:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(300.8658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(289.4686, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(300.9424, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(313.2479, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(351.9176, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(336.7641, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(339.8422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(298.4160, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(311.9001, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(306.0078, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(303.2323, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 139:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(310.5034, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(320.0248, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(326.8813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  18%|█▊        | 2/11 [00:01<00:08,  1.07it/s, v_num=0]train_loss:  tensor(308.0355, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  27%|██▋       | 3/11 [00:02<00:07,  1.14it/s, v_num=0]train_loss:  tensor(307.6520, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(296.4406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(296.2393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(309.3658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(306.6996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(312.2605, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(302.8066, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 140:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(303.7278, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(309.4312, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(302.0594, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(281.6723, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(293.6910, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(304.0831, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(297.3896, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(292.4351, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(284.2973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(289.1459, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(303.2481, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 141:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(278.6422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(295.2197, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(283.4509, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  18%|█▊        | 2/11 [00:01<00:08,  1.04it/s, v_num=0]train_loss:  tensor(293.4320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  27%|██▋       | 3/11 [00:02<00:07,  1.12it/s, v_num=0]train_loss:  tensor(280.8575, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  36%|███▋      | 4/11 [00:03<00:06,  1.16it/s, v_num=0]train_loss:  tensor(283.5581, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  45%|████▌     | 5/11 [00:04<00:05,  1.19it/s, v_num=0]train_loss:  tensor(295.9663, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  55%|█████▍    | 6/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(301.2029, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(292.5815, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(297.2142, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(282.3591, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 142:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(273.3154, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(294.0988, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:   9%|▉         | 1/11 [00:00<00:09,  1.10it/s, v_num=0]train_loss:  tensor(290.7256, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  18%|█▊        | 2/11 [00:01<00:07,  1.20it/s, v_num=0]train_loss:  tensor(297.7339, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(283.1058, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(285.6896, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(299.5795, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(278.5466, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(282.3046, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(290.0024, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(282.3222, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 143:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(279.8363, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 0:  27%|██▋       | 3/11 [1:35:32<4:14:47,  0.00it/s, v_num=0]\n",
      "train_loss:  tensor(276.0686, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=0]train_loss:  tensor(276.5907, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  18%|█▊        | 2/11 [00:02<00:09,  0.98it/s, v_num=0]train_loss:  tensor(293.9817, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(295.5820, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=0]train_loss:  tensor(290.2022, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(285.4241, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(287.8967, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(286.9722, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(285.2141, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(275.0955, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 144:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(276.7725, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(282.9027, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:   9%|▉         | 1/11 [00:01<00:11,  0.84it/s, v_num=0]train_loss:  tensor(290.2272, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  18%|█▊        | 2/11 [00:01<00:08,  1.03it/s, v_num=0]train_loss:  tensor(289.0176, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(285.5797, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(259.5965, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(286.6299, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(284.9250, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(285.6786, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(275.9566, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(283.1927, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 145:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(270.8326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(280.0685, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(276.8126, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(268.7516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  27%|██▋       | 3/11 [00:02<00:06,  1.28it/s, v_num=0]train_loss:  tensor(293.3253, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(271.8455, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(291.1011, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(283.1462, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(282.8568, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(281.6260, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(297.6807, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 146:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(293.9766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(302.2133, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s, v_num=0]train_loss:  tensor(288.4614, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  18%|█▊        | 2/11 [00:02<00:09,  0.90it/s, v_num=0]train_loss:  tensor(296.7881, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  27%|██▋       | 3/11 [00:02<00:07,  1.01it/s, v_num=0]train_loss:  tensor(301.4483, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(295.8163, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  45%|████▌     | 5/11 [00:04<00:05,  1.11it/s, v_num=0]train_loss:  tensor(298.6317, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(287.9125, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(288.4985, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(282.1105, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(285.3300, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 147:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(258.2451, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(269.1313, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(269.7815, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(285.2863, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(295.7179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(279.6815, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(299.2300, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(279.9360, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(288.2952, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(269.3713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  82%|████████▏ | 9/11 [00:06<00:01,  1.30it/s, v_num=0]train_loss:  tensor(276.6368, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 148:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(274.3946, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(295.2696, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(298.5866, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  18%|█▊        | 2/11 [00:02<00:09,  0.91it/s, v_num=0]train_loss:  tensor(294.4837, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  27%|██▋       | 3/11 [00:02<00:07,  1.01it/s, v_num=0]train_loss:  tensor(280.8220, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(262.0326, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  45%|████▌     | 5/11 [00:04<00:05,  1.11it/s, v_num=0]train_loss:  tensor(271.4464, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(277.5806, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(276.3248, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(282.9720, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(273.9667, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 149:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(287.7959, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(268.2144, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(285.9990, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(286.5524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(272.2721, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  36%|███▋      | 4/11 [00:03<00:05,  1.23it/s, v_num=0]train_loss:  tensor(276.2998, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(285.2803, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(274.8297, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(294.2888, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(263.1293, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(268.8163, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 150:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(287.6304, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(264.2866, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(281.7336, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  18%|█▊        | 2/11 [00:01<00:08,  1.12it/s, v_num=0]train_loss:  tensor(277.2097, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(274.0068, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(276.7839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(280.2330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(276.3426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(270.0997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(269.0120, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(268.5896, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 151:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(268.2175, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(276.9855, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(287.1775, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  18%|█▊        | 2/11 [00:01<00:07,  1.20it/s, v_num=0]train_loss:  tensor(254.7222, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(272.1611, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(277.5839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  45%|████▌     | 5/11 [00:03<00:04,  1.25it/s, v_num=0]train_loss:  tensor(263.7883, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(263.8622, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(259.5830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(276.7869, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(268.5547, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 152:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(283.7314, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(264.3288, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(279.0222, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(280.2170, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(292.0028, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  36%|███▋      | 4/11 [00:03<00:05,  1.24it/s, v_num=0]train_loss:  tensor(292.9689, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  45%|████▌     | 5/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(299.0627, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(300.3022, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(297.7458, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(281.4335, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(289.7589, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 153:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(254.3375, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(288.3651, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(273.5333, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  18%|█▊        | 2/11 [00:01<00:08,  1.05it/s, v_num=0]train_loss:  tensor(260.4416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  27%|██▋       | 3/11 [00:02<00:07,  1.11it/s, v_num=0]train_loss:  tensor(274.8257, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=0]train_loss:  tensor(271.6455, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(272.4568, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=0]train_loss:  tensor(280.7314, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(272.8098, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(280.8542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(276.7225, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 154:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(273.7839, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(278.7224, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(264.5078, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(260.0924, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(269.4252, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(274.3450, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(272.7919, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(268.4803, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(261.3948, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(273.9383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(277.9830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 155:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(269.6510, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(263.1494, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:   9%|▉         | 1/11 [00:01<00:10,  1.00it/s, v_num=0]train_loss:  tensor(260.0219, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  18%|█▊        | 2/11 [00:01<00:08,  1.06it/s, v_num=0]train_loss:  tensor(272.2713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  27%|██▋       | 3/11 [00:02<00:07,  1.13it/s, v_num=0]train_loss:  tensor(269.9918, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  36%|███▋      | 4/11 [00:03<00:05,  1.17it/s, v_num=0]train_loss:  tensor(275.0495, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(261.3606, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(269.7958, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(257.0328, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(273.4545, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(263.9688, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 156:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(274.8500, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(279.8713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(273.0756, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(260.6041, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  27%|██▋       | 3/11 [00:02<00:06,  1.25it/s, v_num=0]train_loss:  tensor(272.4769, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(275.2359, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(276.2669, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(268.1440, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(288.1621, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(286.5012, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(276.8841, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 157:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(271.1868, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(277.2662, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(273.4538, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(276.4283, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(284.8555, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(260.6864, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(275.6428, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(270.4952, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(263.5722, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(255.4533, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(274.1662, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 158:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(260.6332, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(267.6835, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s, v_num=0]train_loss:  tensor(259.4716, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  18%|█▊        | 2/11 [00:02<00:09,  0.99it/s, v_num=0]train_loss:  tensor(267.8489, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  27%|██▋       | 3/11 [00:02<00:07,  1.08it/s, v_num=0]train_loss:  tensor(263.4087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=0]train_loss:  tensor(266.7744, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  45%|████▌     | 5/11 [00:04<00:05,  1.16it/s, v_num=0]train_loss:  tensor(276.8043, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=0]train_loss:  tensor(267.3542, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(266.1045, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(276.8336, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(260.3505, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 159:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(248.4486, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(259.4469, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(247.0087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(266.9537, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(264.8524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(259.5408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(273.6805, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(264.3617, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(263.6948, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(272.9336, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(252.6918, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 160:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(269.6824, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(266.9677, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:   9%|▉         | 1/11 [00:00<00:07,  1.28it/s, v_num=0]train_loss:  tensor(261.9497, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=0]train_loss:  tensor(272.0986, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(253.4565, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  36%|███▋      | 4/11 [00:03<00:05,  1.30it/s, v_num=0]train_loss:  tensor(261.7875, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(255.4865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(256.5818, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(263.6479, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(269.2335, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(263.1565, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 161:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(231.0652, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(254.3669, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=0]train_loss:  tensor(257.4526, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  18%|█▊        | 2/11 [00:01<00:08,  1.09it/s, v_num=0]train_loss:  tensor(252.8756, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(257.8362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(251.4243, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  45%|████▌     | 5/11 [00:04<00:05,  1.09it/s, v_num=0]train_loss:  tensor(265.5813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(261.1034, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(277.2045, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  73%|███████▎  | 8/11 [00:07<00:02,  1.14it/s, v_num=0]train_loss:  tensor(263.5429, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(251.5658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 162:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(253.1454, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(246.9441, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(274.6403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=0]train_loss:  tensor(271.5388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(284.9940, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(283.6805, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(269.0872, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(271.0991, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(285.7923, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(271.9614, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=0]train_loss:  tensor(268.3347, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 163:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(245.2775, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(268.6270, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:   9%|▉         | 1/11 [00:01<00:11,  0.83it/s, v_num=0]train_loss:  tensor(275.5963, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  18%|█▊        | 2/11 [00:01<00:08,  1.01it/s, v_num=0]train_loss:  tensor(277.6273, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(252.9630, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  36%|███▋      | 4/11 [00:03<00:06,  1.11it/s, v_num=0]train_loss:  tensor(255.3110, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=0]train_loss:  tensor(246.0574, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  55%|█████▍    | 6/11 [00:05<00:04,  1.17it/s, v_num=0]train_loss:  tensor(256.1285, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=0]train_loss:  tensor(259.9752, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(260.2860, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(251.8845, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 164:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(275.2211, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(276.5106, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(269.5301, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(262.0076, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  27%|██▋       | 3/11 [00:02<00:06,  1.30it/s, v_num=0]train_loss:  tensor(254.1740, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  36%|███▋      | 4/11 [00:03<00:05,  1.31it/s, v_num=0]train_loss:  tensor(266.3568, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  45%|████▌     | 5/11 [00:03<00:04,  1.29it/s, v_num=0]train_loss:  tensor(253.6143, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(249.5465, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(262.8589, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(270.9570, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(252.3543, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 165:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(250.0904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(260.8102, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:   9%|▉         | 1/11 [00:00<00:09,  1.04it/s, v_num=0]train_loss:  tensor(252.9611, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  18%|█▊        | 2/11 [00:01<00:08,  1.10it/s, v_num=0]train_loss:  tensor(261.3496, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(250.2949, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(262.2153, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(259.2850, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(265.8870, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=0]train_loss:  tensor(260.4237, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=0]train_loss:  tensor(271.8460, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  82%|████████▏ | 9/11 [00:07<00:01,  1.25it/s, v_num=0]train_loss:  tensor(265.5632, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 166:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(250.7771, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(248.7167, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(250.7960, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  18%|█▊        | 2/11 [00:01<00:07,  1.24it/s, v_num=0]train_loss:  tensor(269.3966, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(254.8330, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(269.6087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(265.8713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(255.2602, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(260.0757, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(270.0487, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(248.4982, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 167:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(264.2129, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(259.9774, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(275.7658, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=0]train_loss:  tensor(274.4912, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(263.7798, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  36%|███▋      | 4/11 [00:03<00:06,  1.16it/s, v_num=0]train_loss:  tensor(255.5865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(245.9912, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=0]train_loss:  tensor(248.3800, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  64%|██████▎   | 7/11 [00:05<00:03,  1.17it/s, v_num=0]train_loss:  tensor(257.6179, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(257.1956, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  82%|████████▏ | 9/11 [00:07<00:01,  1.19it/s, v_num=0]train_loss:  tensor(254.8699, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 168:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=0]train_loss:  tensor(244.7835, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(255.4397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(246.5153, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(253.6067, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(266.6621, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(246.0388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(244.2545, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=0]train_loss:  tensor(257.9151, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(256.2099, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  73%|███████▎  | 8/11 [00:06<00:02,  1.20it/s, v_num=0]train_loss:  tensor(269.5789, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(245.9118, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 169:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(255.8536, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(254.4897, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:   9%|▉         | 1/11 [00:00<00:07,  1.26it/s, v_num=0]train_loss:  tensor(250.6797, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(252.1212, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(259.7378, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(247.7149, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(256.7681, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(251.4949, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(244.9274, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(248.1531, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(246.2937, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 170:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(255.4054, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(242.3621, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(255.7944, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  18%|█▊        | 2/11 [00:02<00:10,  0.88it/s, v_num=0]train_loss:  tensor(251.7189, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  27%|██▋       | 3/11 [00:03<00:08,  0.99it/s, v_num=0]train_loss:  tensor(248.2610, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  36%|███▋      | 4/11 [00:03<00:06,  1.05it/s, v_num=0]train_loss:  tensor(257.8035, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  45%|████▌     | 5/11 [00:04<00:05,  1.09it/s, v_num=0]train_loss:  tensor(242.2059, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  55%|█████▍    | 6/11 [00:05<00:04,  1.12it/s, v_num=0]train_loss:  tensor(251.2889, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(251.6031, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(258.2537, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(251.5846, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 171:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(232.5247, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(249.9364, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(267.3817, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(256.3299, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(257.3745, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(254.4742, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(261.2203, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(253.6791, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(258.5134, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(272.0574, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(276.3432, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 172:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(265.4512, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(257.0698, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(259.7477, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(248.0828, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  27%|██▋       | 3/11 [00:02<00:06,  1.29it/s, v_num=0]train_loss:  tensor(253.0907, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  36%|███▋      | 4/11 [00:03<00:05,  1.28it/s, v_num=0]train_loss:  tensor(256.6347, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(255.7357, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(269.4923, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(254.5110, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(240.2462, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(261.5482, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 173:  91%|█████████ | 10/11 [00:07<00:00,  1.30it/s, v_num=0]train_loss:  tensor(243.0890, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(242.7966, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:   9%|▉         | 1/11 [00:00<00:09,  1.07it/s, v_num=0]train_loss:  tensor(258.2491, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(249.1769, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(244.6997, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(260.1393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  45%|████▌     | 5/11 [00:03<00:04,  1.25it/s, v_num=0]train_loss:  tensor(246.3473, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(249.4495, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(269.6021, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=0]train_loss:  tensor(242.2292, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=0]train_loss:  tensor(258.0067, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 174:  91%|█████████ | 10/11 [00:08<00:00,  1.22it/s, v_num=0]train_loss:  tensor(252.9302, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(243.7980, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:   9%|▉         | 1/11 [00:00<00:08,  1.19it/s, v_num=0]train_loss:  tensor(258.2719, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=0]train_loss:  tensor(249.8286, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=0]train_loss:  tensor(242.1006, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(258.6713, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  45%|████▌     | 5/11 [00:03<00:04,  1.29it/s, v_num=0]train_loss:  tensor(250.6480, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(243.1237, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(239.6994, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(248.8512, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(229.4847, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 175:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(257.3774, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(246.3033, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s, v_num=0]train_loss:  tensor(232.6411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  18%|█▊        | 2/11 [00:02<00:10,  0.90it/s, v_num=0]train_loss:  tensor(256.8663, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  27%|██▋       | 3/11 [00:02<00:07,  1.00it/s, v_num=0]train_loss:  tensor(245.0771, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(251.2831, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  45%|████▌     | 5/11 [00:04<00:05,  1.11it/s, v_num=0]train_loss:  tensor(247.6088, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(234.3158, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(250.8078, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(255.1083, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(250.5833, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 176:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(260.9194, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(255.7117, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:   9%|▉         | 1/11 [00:00<00:08,  1.24it/s, v_num=0]train_loss:  tensor(236.1291, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(248.5569, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(233.5221, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(254.9587, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(235.8186, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(247.1508, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(254.1502, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(265.6224, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(273.7242, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 177:  91%|█████████ | 10/11 [00:08<00:00,  1.24it/s, v_num=0]train_loss:  tensor(268.0062, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(263.0483, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s, v_num=0]train_loss:  tensor(244.3246, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  18%|█▊        | 2/11 [00:01<00:07,  1.17it/s, v_num=0]train_loss:  tensor(254.7847, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  27%|██▋       | 3/11 [00:02<00:06,  1.19it/s, v_num=0]train_loss:  tensor(267.0719, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(266.9996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(282.0534, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  55%|█████▍    | 6/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(275.9496, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(241.5309, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(248.4770, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(248.8505, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 178:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=0]train_loss:  tensor(255.2820, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(259.9461, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:   9%|▉         | 1/11 [00:00<00:08,  1.21it/s, v_num=0]train_loss:  tensor(252.6654, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=0]train_loss:  tensor(242.8340, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(258.6353, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=0]train_loss:  tensor(253.4327, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  45%|████▌     | 5/11 [00:04<00:05,  1.17it/s, v_num=0]train_loss:  tensor(265.6227, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  55%|█████▍    | 6/11 [00:05<00:04,  1.19it/s, v_num=0]train_loss:  tensor(261.5024, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  64%|██████▎   | 7/11 [00:05<00:03,  1.20it/s, v_num=0]train_loss:  tensor(242.3484, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  73%|███████▎  | 8/11 [00:06<00:02,  1.21it/s, v_num=0]train_loss:  tensor(247.8360, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  82%|████████▏ | 9/11 [00:07<00:01,  1.22it/s, v_num=0]train_loss:  tensor(247.3950, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 179:  91%|█████████ | 10/11 [00:08<00:00,  1.23it/s, v_num=0]train_loss:  tensor(259.5209, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(246.1100, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(241.2471, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=0]train_loss:  tensor(243.2916, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(252.2503, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(242.8500, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  45%|████▌     | 5/11 [00:03<00:04,  1.27it/s, v_num=0]train_loss:  tensor(241.4420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  55%|█████▍    | 6/11 [00:04<00:03,  1.28it/s, v_num=0]train_loss:  tensor(245.1294, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(232.9352, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(257.1478, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(252.8746, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 180:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(249.0522, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(245.6261, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=0]train_loss:  tensor(241.0845, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  18%|█▊        | 2/11 [00:01<00:08,  1.10it/s, v_num=0]train_loss:  tensor(237.4683, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  27%|██▋       | 3/11 [00:02<00:06,  1.16it/s, v_num=0]train_loss:  tensor(244.9113, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=0]train_loss:  tensor(251.5087, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  45%|████▌     | 5/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(244.2109, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(245.0589, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=0]train_loss:  tensor(243.6012, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(249.5799, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(241.7943, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 181:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=0]train_loss:  tensor(258.2996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(246.6082, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:   9%|▉         | 1/11 [00:00<00:09,  1.11it/s, v_num=0]train_loss:  tensor(236.0319, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  18%|█▊        | 2/11 [00:01<00:07,  1.16it/s, v_num=0]train_loss:  tensor(243.0289, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  27%|██▋       | 3/11 [00:02<00:06,  1.18it/s, v_num=0]train_loss:  tensor(238.9953, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  36%|███▋      | 4/11 [00:03<00:06,  1.08it/s, v_num=0]train_loss:  tensor(241.6321, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  45%|████▌     | 5/11 [00:04<00:05,  1.12it/s, v_num=0]train_loss:  tensor(251.7116, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(234.3104, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(243.9398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=0]train_loss:  tensor(236.7878, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(241.6950, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 182:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(261.9524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(237.1214, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=0]train_loss:  tensor(246.1922, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  18%|█▊        | 2/11 [00:01<00:07,  1.25it/s, v_num=0]train_loss:  tensor(249.5971, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(244.9509, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(257.8874, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(236.9008, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(233.1003, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  64%|██████▎   | 7/11 [00:05<00:03,  1.27it/s, v_num=0]train_loss:  tensor(234.1312, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(222.8435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(246.9339, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 183:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(250.3396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(239.7363, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(242.3091, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  18%|█▊        | 2/11 [00:01<00:08,  1.11it/s, v_num=0]train_loss:  tensor(242.6781, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  27%|██▋       | 3/11 [00:02<00:06,  1.17it/s, v_num=0]train_loss:  tensor(249.0137, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(237.4830, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(237.3522, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  55%|█████▍    | 6/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(254.1248, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  64%|██████▎   | 7/11 [00:05<00:03,  1.25it/s, v_num=0]train_loss:  tensor(235.1446, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(234.5685, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  82%|████████▏ | 9/11 [00:07<00:01,  1.26it/s, v_num=0]train_loss:  tensor(232.7225, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 184:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(226.2265, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(244.1532, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(243.5493, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  18%|█▊        | 2/11 [00:01<00:08,  1.04it/s, v_num=0]train_loss:  tensor(228.6996, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  27%|██▋       | 3/11 [00:03<00:08,  0.97it/s, v_num=0]train_loss:  tensor(243.4398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  36%|███▋      | 4/11 [00:03<00:06,  1.03it/s, v_num=0]train_loss:  tensor(241.5484, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s, v_num=0]train_loss:  tensor(235.3014, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  55%|█████▍    | 6/11 [00:05<00:04,  1.11it/s, v_num=0]train_loss:  tensor(230.4822, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  64%|██████▎   | 7/11 [00:06<00:03,  1.13it/s, v_num=0]train_loss:  tensor(248.5650, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  73%|███████▎  | 8/11 [00:07<00:02,  1.14it/s, v_num=0]train_loss:  tensor(232.6656, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(227.6506, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 185:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=0]train_loss:  tensor(243.0965, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(229.7301, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(230.8353, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  18%|█▊        | 2/11 [00:01<00:07,  1.26it/s, v_num=0]train_loss:  tensor(230.8913, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  27%|██▋       | 3/11 [00:02<00:06,  1.23it/s, v_num=0]train_loss:  tensor(226.9245, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(242.4434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  45%|████▌     | 5/11 [00:04<00:05,  1.20it/s, v_num=0]train_loss:  tensor(240.6687, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(247.9096, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(234.1416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  73%|███████▎  | 8/11 [00:06<00:02,  1.24it/s, v_num=0]train_loss:  tensor(242.4946, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=0]train_loss:  tensor(248.3288, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 186:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(225.5250, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(234.9220, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(244.0934, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(239.4621, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  27%|██▋       | 3/11 [00:02<00:06,  1.27it/s, v_num=0]train_loss:  tensor(258.3800, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=0]train_loss:  tensor(251.2264, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=0]train_loss:  tensor(251.2346, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=0]train_loss:  tensor(266.0119, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  64%|██████▎   | 7/11 [00:05<00:03,  1.29it/s, v_num=0]train_loss:  tensor(256.9809, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  73%|███████▎  | 8/11 [00:06<00:02,  1.29it/s, v_num=0]train_loss:  tensor(233.3185, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  82%|████████▏ | 9/11 [00:06<00:01,  1.29it/s, v_num=0]train_loss:  tensor(236.0436, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 187:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=0]train_loss:  tensor(236.3939, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(239.2481, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:   9%|▉         | 1/11 [00:01<00:14,  0.70it/s, v_num=0]train_loss:  tensor(241.6101, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  18%|█▊        | 2/11 [00:02<00:10,  0.89it/s, v_num=0]train_loss:  tensor(244.4432, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  27%|██▋       | 3/11 [00:03<00:08,  0.99it/s, v_num=0]train_loss:  tensor(252.0205, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  36%|███▋      | 4/11 [00:03<00:06,  1.06it/s, v_num=0]train_loss:  tensor(248.0148, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  45%|████▌     | 5/11 [00:04<00:05,  1.09it/s, v_num=0]train_loss:  tensor(247.1961, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(232.6304, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(233.1606, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(240.2661, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(243.3963, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 188:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(248.4871, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(243.1108, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(243.6225, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  18%|█▊        | 2/11 [00:01<00:08,  1.06it/s, v_num=0]train_loss:  tensor(230.2208, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  27%|██▋       | 3/11 [00:02<00:07,  1.11it/s, v_num=0]train_loss:  tensor(236.7813, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  36%|███▋      | 4/11 [00:03<00:06,  1.14it/s, v_num=0]train_loss:  tensor(239.7767, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=0]train_loss:  tensor(223.4422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  55%|█████▍    | 6/11 [00:05<00:04,  1.10it/s, v_num=0]train_loss:  tensor(236.5398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  64%|██████▎   | 7/11 [00:06<00:03,  1.13it/s, v_num=0]train_loss:  tensor(245.4800, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  73%|███████▎  | 8/11 [00:07<00:02,  1.13it/s, v_num=0]train_loss:  tensor(236.1835, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  82%|████████▏ | 9/11 [00:07<00:01,  1.15it/s, v_num=0]train_loss:  tensor(235.7598, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 189:  91%|█████████ | 10/11 [00:08<00:00,  1.16it/s, v_num=0]train_loss:  tensor(226.3178, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(247.8612, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=0]train_loss:  tensor(220.8930, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  18%|█▊        | 2/11 [00:01<00:07,  1.27it/s, v_num=0]train_loss:  tensor(230.0513, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(219.0475, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(245.7978, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(240.1949, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(239.3250, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(232.3266, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(230.6481, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(229.9643, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 190:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(232.4489, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(247.1931, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:   9%|▉         | 1/11 [00:00<00:09,  1.03it/s, v_num=0]train_loss:  tensor(228.0522, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  18%|█▊        | 2/11 [00:01<00:07,  1.16it/s, v_num=0]train_loss:  tensor(231.1408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  27%|██▋       | 3/11 [00:02<00:06,  1.22it/s, v_num=0]train_loss:  tensor(225.8722, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  36%|███▋      | 4/11 [00:03<00:05,  1.25it/s, v_num=0]train_loss:  tensor(240.6506, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(230.3386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=0]train_loss:  tensor(231.4823, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(221.8627, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=0]train_loss:  tensor(227.6976, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(230.5868, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 191:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(237.7251, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(222.7162, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:   9%|▉         | 1/11 [00:00<00:08,  1.23it/s, v_num=0]train_loss:  tensor(229.9485, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(235.4717, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=0]train_loss:  tensor(222.3385, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  36%|███▋      | 4/11 [00:03<00:05,  1.20it/s, v_num=0]train_loss:  tensor(247.0264, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  45%|████▌     | 5/11 [00:04<00:05,  1.12it/s, v_num=0]train_loss:  tensor(234.5291, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(239.3613, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(227.7999, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=0]train_loss:  tensor(230.2985, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  82%|████████▏ | 9/11 [00:07<00:01,  1.16it/s, v_num=0]train_loss:  tensor(226.1275, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 192:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(222.2176, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(238.3157, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:   9%|▉         | 1/11 [00:00<00:09,  1.01it/s, v_num=0]train_loss:  tensor(247.6020, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  18%|█▊        | 2/11 [00:01<00:07,  1.13it/s, v_num=0]train_loss:  tensor(233.9940, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(252.2482, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  36%|███▋      | 4/11 [00:03<00:05,  1.18it/s, v_num=0]train_loss:  tensor(285.4753, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  45%|████▌     | 5/11 [00:04<00:04,  1.20it/s, v_num=0]train_loss:  tensor(310.5587, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  55%|█████▍    | 6/11 [00:04<00:04,  1.22it/s, v_num=0]train_loss:  tensor(335.7345, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(287.1724, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(244.1063, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(254.7439, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 193:  91%|█████████ | 10/11 [00:07<00:00,  1.25it/s, v_num=0]train_loss:  tensor(264.3379, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(258.6020, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:   9%|▉         | 1/11 [00:00<00:08,  1.22it/s, v_num=0]train_loss:  tensor(242.8251, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  18%|█▊        | 2/11 [00:01<00:07,  1.21it/s, v_num=0]train_loss:  tensor(250.0648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  27%|██▋       | 3/11 [00:02<00:06,  1.21it/s, v_num=0]train_loss:  tensor(251.5052, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  36%|███▋      | 4/11 [00:03<00:05,  1.22it/s, v_num=0]train_loss:  tensor(246.5060, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  45%|████▌     | 5/11 [00:04<00:04,  1.24it/s, v_num=0]train_loss:  tensor(230.6022, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=0]train_loss:  tensor(259.1689, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  64%|██████▎   | 7/11 [00:05<00:03,  1.22it/s, v_num=0]train_loss:  tensor(239.0599, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  73%|███████▎  | 8/11 [00:06<00:02,  1.23it/s, v_num=0]train_loss:  tensor(241.4416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=0]train_loss:  tensor(247.9899, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 194:  91%|█████████ | 10/11 [00:08<00:00,  1.25it/s, v_num=0]train_loss:  tensor(244.3158, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(256.0384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:   9%|▉         | 1/11 [00:01<00:12,  0.83it/s, v_num=0]train_loss:  tensor(233.1842, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  18%|█▊        | 2/11 [00:02<00:09,  0.98it/s, v_num=0]train_loss:  tensor(237.7709, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=0]train_loss:  tensor(241.7397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  36%|███▋      | 4/11 [00:03<00:06,  1.12it/s, v_num=0]train_loss:  tensor(244.0101, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  45%|████▌     | 5/11 [00:04<00:05,  1.15it/s, v_num=0]train_loss:  tensor(229.0200, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(230.8331, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  64%|██████▎   | 7/11 [00:06<00:03,  1.15it/s, v_num=0]train_loss:  tensor(240.4097, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(228.2649, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(232.0504, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 195:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(253.4717, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(229.4753, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:   9%|▉         | 1/11 [00:00<00:09,  1.00it/s, v_num=0]train_loss:  tensor(238.0059, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  18%|█▊        | 2/11 [00:01<00:07,  1.13it/s, v_num=0]train_loss:  tensor(215.3943, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=0]train_loss:  tensor(240.1703, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  36%|███▋      | 4/11 [00:03<00:06,  1.07it/s, v_num=0]train_loss:  tensor(225.9835, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  45%|████▌     | 5/11 [00:04<00:05,  1.10it/s, v_num=0]train_loss:  tensor(227.9835, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  55%|█████▍    | 6/11 [00:05<00:04,  1.13it/s, v_num=0]train_loss:  tensor(241.2429, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(241.1409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  73%|███████▎  | 8/11 [00:06<00:02,  1.17it/s, v_num=0]train_loss:  tensor(234.4435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=0]train_loss:  tensor(229.9868, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 196:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=0]train_loss:  tensor(246.6669, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(224.5402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=0]train_loss:  tensor(232.0216, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=0]train_loss:  tensor(228.9702, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  27%|██▋       | 3/11 [00:02<00:06,  1.28it/s, v_num=0]train_loss:  tensor(235.7582, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=0]train_loss:  tensor(231.2778, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=0]train_loss:  tensor(225.9891, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  55%|█████▍    | 6/11 [00:04<00:03,  1.27it/s, v_num=0]train_loss:  tensor(237.3768, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  64%|██████▎   | 7/11 [00:05<00:03,  1.28it/s, v_num=0]train_loss:  tensor(239.2683, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  73%|███████▎  | 8/11 [00:06<00:02,  1.28it/s, v_num=0]train_loss:  tensor(227.9234, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=0]train_loss:  tensor(224.3351, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 197:  91%|█████████ | 10/11 [00:07<00:00,  1.28it/s, v_num=0]train_loss:  tensor(230.3865, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(224.8100, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:   9%|▉         | 1/11 [00:00<00:08,  1.13it/s, v_num=0]train_loss:  tensor(231.2466, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  18%|█▊        | 2/11 [00:01<00:07,  1.17it/s, v_num=0]train_loss:  tensor(228.2362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=0]train_loss:  tensor(225.5731, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  36%|███▋      | 4/11 [00:03<00:05,  1.21it/s, v_num=0]train_loss:  tensor(216.1279, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  45%|████▌     | 5/11 [00:04<00:04,  1.23it/s, v_num=0]train_loss:  tensor(228.1677, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=0]train_loss:  tensor(226.8105, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=0]train_loss:  tensor(246.9237, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=0]train_loss:  tensor(221.7705, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  82%|████████▏ | 9/11 [00:07<00:01,  1.27it/s, v_num=0]train_loss:  tensor(226.6886, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 198:  91%|█████████ | 10/11 [00:07<00:00,  1.27it/s, v_num=0]train_loss:  tensor(230.2089, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:   0%|          | 0/11 [00:00<?, ?it/s, v_num=0]         train_loss:  tensor(222.7254, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:   9%|▉         | 1/11 [00:00<00:09,  1.06it/s, v_num=0]train_loss:  tensor(224.0317, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  18%|█▊        | 2/11 [00:02<00:09,  0.91it/s, v_num=0]train_loss:  tensor(223.7091, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  27%|██▋       | 3/11 [00:02<00:07,  1.01it/s, v_num=0]train_loss:  tensor(230.3931, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  36%|███▋      | 4/11 [00:03<00:06,  1.06it/s, v_num=0]train_loss:  tensor(234.3895, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  45%|████▌     | 5/11 [00:04<00:05,  1.10it/s, v_num=0]train_loss:  tensor(235.8654, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=0]train_loss:  tensor(235.5277, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=0]train_loss:  tensor(222.1365, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=0]train_loss:  tensor(225.0569, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=0]train_loss:  tensor(216.7137, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=0]train_loss:  tensor(222.0650, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch 199: 100%|██████████| 11/11 [00:10<00:00,  1.10it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 11/11 [00:10<00:00,  1.09it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing model\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 13.83it/s] \n"
     ]
    }
   ],
   "source": [
    "# they all look about the same, so let's just use 128\n",
    "latent_dim = 128\n",
    "model_ld, result_ld = train(latent_dim, max_epochs=200)\n",
    "# Save the model checkpoint. Not strictly necessary because lightning is also saving it, but just to be safe\n",
    "checkpoint_filename = Path(CHECKPOINT_PATH) / f\"final_model_dim_{latent_dim}\" / \"final_checkpoint.ckpt\"\n",
    "torch.save(model_ld.state_dict(), checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting preset from latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self, latent_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 88),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(88, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        num_classes: int,\n",
    "        encoder_model_checkpoint: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Load autoencoder and get the encoder only\n",
    "        self.encoder = Autoencoder.load_from_checkpoint(encoder_model_checkpoint).encoder\n",
    "        # Put the encoder in evaluation mode\n",
    "        self.encoder.eval()\n",
    "        self.feed_forward = Feed_Forward(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        y_hat = self.feed_forward(z)\n",
    "        return y_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, y = batch  # We do not need the labels\n",
    "        y_hat = self.forward(x)\n",
    "        # print('x.shape: ', y.shape)\n",
    "        # print('x_hat.shape: ', y_hat.shape)\n",
    "        # Take the first value of y as the only label for prediction\n",
    "        y = y[:, 2].unsqueeze(1)\n",
    "        y = y.float()\n",
    "        y_hat = y_hat.float()\n",
    "        loss = F.mse_loss(y, y_hat)\n",
    "        print('loss: ', loss) # 256, 88\n",
    "        # loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        r2 = R2Score()\n",
    "        r2.update(y_hat, y)\n",
    "        r2_score = r2.compute().item()\n",
    "        print('r2_score: ', r2_score)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through data in dataloader,\n",
    "# For each data point, pass it through the decoder, and then pass the output through our simple feedforward network\n",
    "def train_classifier(latent_dim, num_classes, max_epochs=50, encoder_model_checkpoint=None):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    print('creating trainer')\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"classifier_%i\" % latent_dim),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    print(\"creating model\")\n",
    "    model = Classifier(latent_dim=latent_dim, num_classes=num_classes, encoder_model_checkpoint=encoder_model_checkpoint)\n",
    "    print(\"training model\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    print(\"testing model\")\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    # test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    # result = {\"test\": test_result, \"val\": val_result}\n",
    "    result = {\"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type         | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | encoder      | Encoder      | 2.2 M  | eval \n",
      "1 | feed_forward | Feed_Forward | 44.5 K | train\n",
      "------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.978     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "14        Modules in eval mode\n",
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:191: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trainer\n",
      "creating model\n",
      "training model\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\1055357869.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset_name = preset[0]\n",
      "C:\\Users\\jayor\\AppData\\Local\\Temp\\ipykernel_7172\\1055357869.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preset = torch.tensor(preset[1:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.2778, device='cuda:0')\n",
      "r2_score:  -2.2079050540924072\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 11.11it/s]y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.2426, device='cuda:0')\n",
      "r2_score:  -1.9556338787078857\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/11 [00:00<?, ?it/s] y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.2685, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -2.2499115467071533\n",
      "Epoch 0:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(3.0839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -35.77628707885742\n",
      "Epoch 0:  18%|█▊        | 2/11 [00:01<00:06,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.2700643539428711\n",
      "Epoch 0:  27%|██▋       | 3/11 [00:02<00:05,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -1.0339221954345703\n",
      "Epoch 0:  36%|███▋      | 4/11 [00:02<00:04,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1522, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.9803268909454346\n",
      "Epoch 0:  45%|████▌     | 5/11 [00:03<00:04,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1171, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.3186788558959961\n",
      "Epoch 0:  55%|█████▍    | 6/11 [00:04<00:03,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0962, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.17650485038757324\n",
      "Epoch 0:  64%|██████▎   | 7/11 [00:04<00:02,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.20987343788146973\n",
      "Epoch 0:  73%|███████▎  | 8/11 [00:05<00:02,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.42315375804901123\n",
      "Epoch 0:  82%|████████▏ | 9/11 [00:06<00:01,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0861, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.10611546039581299\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:07<00:00,  1.42it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0980, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.14793550968170166\n",
      "Epoch 0: 100%|██████████| 11/11 [00:07<00:00,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1128, device='cuda:0')\n",
      "r2_score:  -0.30320632457733154\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1049, device='cuda:0')\n",
      "r2_score:  -0.27811574935913086\n",
      "Epoch 1:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0940, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.20348942279815674\n",
      "Epoch 1:   9%|▉         | 1/11 [00:00<00:05,  1.75it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.22119462490081787\n",
      "Epoch 1:  18%|█▊        | 2/11 [00:01<00:05,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.012130796909332275\n",
      "Epoch 1:  27%|██▋       | 3/11 [00:01<00:04,  1.78it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.022573113441467285\n",
      "Epoch 1:  36%|███▋      | 4/11 [00:02<00:04,  1.74it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.06679844856262207\n",
      "Epoch 1:  45%|████▌     | 5/11 [00:02<00:03,  1.75it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0937, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.03342866897583008\n",
      "Epoch 1:  55%|█████▍    | 6/11 [00:03<00:02,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.006889045238494873\n",
      "Epoch 1:  64%|██████▎   | 7/11 [00:03<00:02,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.09631085395812988\n",
      "Epoch 1:  73%|███████▎  | 8/11 [00:04<00:01,  1.75it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0834, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.07158780097961426\n",
      "Epoch 1:  82%|████████▏ | 9/11 [00:05<00:01,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.010239005088806152\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:05<00:00,  1.76it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.030467867851257324\n",
      "Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.83it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0831, device='cuda:0')\n",
      "r2_score:  0.03979337215423584\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0874, device='cuda:0')\n",
      "r2_score:  -0.06412720680236816\n",
      "Epoch 2:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0879, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0151747465133667\n",
      "Epoch 2:   9%|▉         | 1/11 [00:00<00:05,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0006129741668701172\n",
      "Epoch 2:  18%|█▊        | 2/11 [00:01<00:05,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03268402814865112\n",
      "Epoch 2:  27%|██▋       | 3/11 [00:01<00:04,  1.74it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0881, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.020789146423339844\n",
      "Epoch 2:  36%|███▋      | 4/11 [00:02<00:04,  1.75it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0847, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.028304100036621094\n",
      "Epoch 2:  45%|████▌     | 5/11 [00:02<00:03,  1.75it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.03019571304321289\n",
      "Epoch 2:  55%|█████▍    | 6/11 [00:03<00:02,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0813, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.007853031158447266\n",
      "Epoch 2:  64%|██████▎   | 7/11 [00:03<00:02,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0042694807052612305\n",
      "Epoch 2:  73%|███████▎  | 8/11 [00:04<00:01,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0842, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.004754364490509033\n",
      "Epoch 2:  82%|████████▏ | 9/11 [00:05<00:01,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0835, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04804199934005737\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:05<00:00,  1.77it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.011126875877380371\n",
      "Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.84it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0886, device='cuda:0')\n",
      "r2_score:  -0.0233614444732666\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0850, device='cuda:0')\n",
      "r2_score:  -0.035320281982421875\n",
      "Epoch 3:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0872, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.006258487701416016\n",
      "Epoch 3:   9%|▉         | 1/11 [00:00<00:05,  1.78it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0872, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.009764492511749268\n",
      "Epoch 3:  18%|█▊        | 2/11 [00:01<00:05,  1.74it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.026386737823486328\n",
      "Epoch 3:  27%|██▋       | 3/11 [00:01<00:04,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02502131462097168\n",
      "Epoch 3:  36%|███▋      | 4/11 [00:02<00:03,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05718344449996948\n",
      "Epoch 3:  45%|████▌     | 5/11 [00:02<00:03,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.005022943019866943\n",
      "Epoch 3:  55%|█████▍    | 6/11 [00:03<00:02,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02848142385482788\n",
      "Epoch 3:  64%|██████▎   | 7/11 [00:03<00:02,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.007919669151306152\n",
      "Epoch 3:  73%|███████▎  | 8/11 [00:04<00:01,  1.76it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0857, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.018363356590270996\n",
      "Epoch 3:  82%|████████▏ | 9/11 [00:05<00:01,  1.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0740, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.054672062397003174\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:05<00:00,  1.76it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0075531005859375\n",
      "Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  1.84it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0868, device='cuda:0')\n",
      "r2_score:  -0.002944350242614746\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0841, device='cuda:0')\n",
      "r2_score:  -0.024278879165649414\n",
      "Epoch 4:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02185523509979248\n",
      "Epoch 4:   9%|▉         | 1/11 [00:00<00:07,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0043125152587890625\n",
      "Epoch 4:  18%|█▊        | 2/11 [00:01<00:05,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07160723209381104\n",
      "Epoch 4:  27%|██▋       | 3/11 [00:01<00:05,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02862924337387085\n",
      "Epoch 4:  36%|███▋      | 4/11 [00:02<00:04,  1.62it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.028428375720977783\n",
      "Epoch 4:  45%|████▌     | 5/11 [00:03<00:03,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04972660541534424\n",
      "Epoch 4:  55%|█████▍    | 6/11 [00:03<00:03,  1.66it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0729, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04690134525299072\n",
      "Epoch 4:  64%|██████▎   | 7/11 [00:04<00:02,  1.63it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.019754469394683838\n",
      "Epoch 4:  73%|███████▎  | 8/11 [00:04<00:01,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04165142774581909\n",
      "Epoch 4:  82%|████████▏ | 9/11 [00:05<00:01,  1.65it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.030261099338531494\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:06<00:00,  1.66it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06767463684082031\n",
      "Epoch 4: 100%|██████████| 11/11 [00:06<00:00,  1.72it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0862, device='cuda:0')\n",
      "r2_score:  0.004642844200134277\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0844, device='cuda:0')\n",
      "r2_score:  -0.027660131454467773\n",
      "Epoch 5:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05440318584442139\n",
      "Epoch 5:   9%|▉         | 1/11 [00:00<00:06,  1.56it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05231374502182007\n",
      "Epoch 5:  18%|█▊        | 2/11 [00:01<00:05,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04117298126220703\n",
      "Epoch 5:  27%|██▋       | 3/11 [00:01<00:05,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0827, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.052076637744903564\n",
      "Epoch 5:  36%|███▋      | 4/11 [00:02<00:04,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.016544580459594727\n",
      "Epoch 5:  45%|████▌     | 5/11 [00:03<00:03,  1.62it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02314656972885132\n",
      "Epoch 5:  55%|█████▍    | 6/11 [00:03<00:03,  1.56it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.018051624298095703\n",
      "Epoch 5:  64%|██████▎   | 7/11 [00:04<00:02,  1.57it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.043001532554626465\n",
      "Epoch 5:  73%|███████▎  | 8/11 [00:05<00:01,  1.58it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0854, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05619460344314575\n",
      "Epoch 5:  82%|████████▏ | 9/11 [00:05<00:01,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04969984292984009\n",
      "Epoch 5:  91%|█████████ | 10/11 [00:06<00:00,  1.60it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0009953975677490234\n",
      "Epoch 5: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0854, device='cuda:0')\n",
      "r2_score:  0.013544738292694092\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0837, device='cuda:0')\n",
      "r2_score:  -0.01901400089263916\n",
      "Epoch 6:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0827, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06827670335769653\n",
      "Epoch 6:   9%|▉         | 1/11 [00:00<00:05,  1.67it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02797764539718628\n",
      "Epoch 6:  18%|█▊        | 2/11 [00:01<00:05,  1.66it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.018971025943756104\n",
      "Epoch 6:  27%|██▋       | 3/11 [00:01<00:04,  1.67it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05233651399612427\n",
      "Epoch 6:  36%|███▋      | 4/11 [00:02<00:04,  1.63it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0069380998611450195\n",
      "Epoch 6:  45%|████▌     | 5/11 [00:03<00:03,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0817, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05986505746841431\n",
      "Epoch 6:  55%|█████▍    | 6/11 [00:03<00:03,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03480321168899536\n",
      "Epoch 6:  64%|██████▎   | 7/11 [00:04<00:02,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0720, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03921538591384888\n",
      "Epoch 6:  73%|███████▎  | 8/11 [00:04<00:01,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0740, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09625875949859619\n",
      "Epoch 6:  82%|████████▏ | 9/11 [00:05<00:01,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.011371850967407227\n",
      "Epoch 6:  91%|█████████ | 10/11 [00:06<00:00,  1.64it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0007263422012329102\n",
      "Epoch 6: 100%|██████████| 11/11 [00:06<00:00,  1.71it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0857, device='cuda:0')\n",
      "r2_score:  0.010220170021057129\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0834, device='cuda:0')\n",
      "r2_score:  -0.01641988754272461\n",
      "Epoch 7:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03226053714752197\n",
      "Epoch 7:   9%|▉         | 1/11 [00:00<00:06,  1.64it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04012638330459595\n",
      "Epoch 7:  18%|█▊        | 2/11 [00:01<00:05,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.030039429664611816\n",
      "Epoch 7:  27%|██▋       | 3/11 [00:01<00:04,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0298348069190979\n",
      "Epoch 7:  36%|███▋      | 4/11 [00:02<00:04,  1.61it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.003374040126800537\n",
      "Epoch 7:  45%|████▌     | 5/11 [00:03<00:03,  1.61it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0729, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06464451551437378\n",
      "Epoch 7:  55%|█████▍    | 6/11 [00:03<00:03,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0901, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0027223825454711914\n",
      "Epoch 7:  64%|██████▎   | 7/11 [00:04<00:02,  1.54it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.022148847579956055\n",
      "Epoch 7:  73%|███████▎  | 8/11 [00:05<00:01,  1.55it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05330628156661987\n",
      "Epoch 7:  82%|████████▏ | 9/11 [00:05<00:01,  1.56it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06217050552368164\n",
      "Epoch 7:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0718, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07556921243667603\n",
      "Epoch 7: 100%|██████████| 11/11 [00:06<00:00,  1.61it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0899, device='cuda:0')\n",
      "r2_score:  -0.038823604583740234\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0884, device='cuda:0')\n",
      "r2_score:  -0.07703745365142822\n",
      "Epoch 8:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.015265882015228271\n",
      "Epoch 8:   9%|▉         | 1/11 [00:00<00:06,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07414913177490234\n",
      "Epoch 8:  18%|█▊        | 2/11 [00:01<00:05,  1.56it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.006316423416137695\n",
      "Epoch 8:  27%|██▋       | 3/11 [00:01<00:05,  1.58it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0736, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10009908676147461\n",
      "Epoch 8:  36%|███▋      | 4/11 [00:02<00:04,  1.58it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.015977859497070312\n",
      "Epoch 8:  45%|████▌     | 5/11 [00:03<00:03,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03551912307739258\n",
      "Epoch 8:  55%|█████▍    | 6/11 [00:03<00:03,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0866, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04110223054885864\n",
      "Epoch 8:  64%|██████▎   | 7/11 [00:04<00:02,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0854, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.029994189739227295\n",
      "Epoch 8:  73%|███████▎  | 8/11 [00:05<00:01,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.037074506282806396\n",
      "Epoch 8:  82%|████████▏ | 9/11 [00:05<00:01,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.031118929386138916\n",
      "Epoch 8:  91%|█████████ | 10/11 [00:06<00:00,  1.60it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04089599847793579\n",
      "Epoch 8: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0866, device='cuda:0')\n",
      "r2_score:  -0.0002562999725341797\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0823, device='cuda:0')\n",
      "r2_score:  -0.0027832984924316406\n",
      "Epoch 9:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07018893957138062\n",
      "Epoch 9:   9%|▉         | 1/11 [00:00<00:06,  1.58it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.037148356437683105\n",
      "Epoch 9:  18%|█▊        | 2/11 [00:01<00:06,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.056825101375579834\n",
      "Epoch 9:  27%|██▋       | 3/11 [00:02<00:05,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08448094129562378\n",
      "Epoch 9:  36%|███▋      | 4/11 [00:02<00:04,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.017271101474761963\n",
      "Epoch 9:  45%|████▌     | 5/11 [00:03<00:03,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0772, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0465395450592041\n",
      "Epoch 9:  55%|█████▍    | 6/11 [00:03<00:03,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03479045629501343\n",
      "Epoch 9:  64%|██████▎   | 7/11 [00:04<00:02,  1.54it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05707728862762451\n",
      "Epoch 9:  73%|███████▎  | 8/11 [00:05<00:01,  1.54it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0831, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.00022399425506591797\n",
      "Epoch 9:  82%|████████▏ | 9/11 [00:05<00:01,  1.54it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.010006904602050781\n",
      "Epoch 9:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0887, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.025880813598632812\n",
      "Epoch 9: 100%|██████████| 11/11 [00:06<00:00,  1.61it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0854, device='cuda:0')\n",
      "r2_score:  0.013596117496490479\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0869, device='cuda:0')\n",
      "r2_score:  -0.05844712257385254\n",
      "Epoch 10:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]        y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07835280895233154\n",
      "Epoch 10:   9%|▉         | 1/11 [00:00<00:06,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.01569998264312744\n",
      "Epoch 10:  18%|█▊        | 2/11 [00:01<00:06,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.014297246932983398\n",
      "Epoch 10:  27%|██▋       | 3/11 [00:02<00:05,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0810, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.031829237937927246\n",
      "Epoch 10:  36%|███▋      | 4/11 [00:02<00:04,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.00707930326461792\n",
      "Epoch 10:  45%|████▌     | 5/11 [00:03<00:03,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03144603967666626\n",
      "Epoch 10:  55%|█████▍    | 6/11 [00:03<00:03,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03095221519470215\n",
      "Epoch 10:  64%|██████▎   | 7/11 [00:04<00:02,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0716, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09029513597488403\n",
      "Epoch 10:  73%|███████▎  | 8/11 [00:05<00:01,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04036778211593628\n",
      "Epoch 10:  82%|████████▏ | 9/11 [00:05<00:01,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.027117371559143066\n",
      "Epoch 10:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06265449523925781\n",
      "Epoch 10: 100%|██████████| 11/11 [00:06<00:00,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0891, device='cuda:0')\n",
      "r2_score:  -0.029569029808044434\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0877, device='cuda:0')\n",
      "r2_score:  -0.06874144077301025\n",
      "Epoch 11:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0830, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.00949794054031372\n",
      "Epoch 11:   9%|▉         | 1/11 [00:00<00:06,  1.55it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04389190673828125\n",
      "Epoch 11:  18%|█▊        | 2/11 [00:01<00:06,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.010445952415466309\n",
      "Epoch 11:  27%|██▋       | 3/11 [00:01<00:05,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07046687602996826\n",
      "Epoch 11:  36%|███▋      | 4/11 [00:02<00:04,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07848858833312988\n",
      "Epoch 11:  45%|████▌     | 5/11 [00:03<00:03,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0874, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.062140703201293945\n",
      "Epoch 11:  55%|█████▍    | 6/11 [00:03<00:03,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04483979940414429\n",
      "Epoch 11:  64%|██████▎   | 7/11 [00:04<00:02,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07236486673355103\n",
      "Epoch 11:  73%|███████▎  | 8/11 [00:05<00:01,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0068885087966918945\n",
      "Epoch 11:  82%|████████▏ | 9/11 [00:05<00:01,  1.54it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07856523990631104\n",
      "Epoch 11:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.02249211072921753\n",
      "Epoch 11: 100%|██████████| 11/11 [00:06<00:00,  1.60it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0920, device='cuda:0')\n",
      "r2_score:  -0.06238245964050293\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0856, device='cuda:0')\n",
      "r2_score:  -0.042290329933166504\n",
      "Epoch 12:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.006306469440460205\n",
      "Epoch 12:   9%|▉         | 1/11 [00:00<00:06,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07761490345001221\n",
      "Epoch 12:  18%|█▊        | 2/11 [00:01<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0846, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.040360331535339355\n",
      "Epoch 12:  27%|██▋       | 3/11 [00:02<00:05,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0807, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05711674690246582\n",
      "Epoch 12:  36%|███▋      | 4/11 [00:02<00:04,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0699889063835144\n",
      "Epoch 12:  45%|████▌     | 5/11 [00:03<00:03,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09170186519622803\n",
      "Epoch 12:  55%|█████▍    | 6/11 [00:03<00:03,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0769658088684082\n",
      "Epoch 12:  64%|██████▎   | 7/11 [00:04<00:02,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0761, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07097184658050537\n",
      "Epoch 12:  73%|███████▎  | 8/11 [00:05<00:01,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0786, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04228144884109497\n",
      "Epoch 12:  82%|████████▏ | 9/11 [00:05<00:01,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.012478113174438477\n",
      "Epoch 12:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06646305322647095\n",
      "Epoch 12: 100%|██████████| 11/11 [00:06<00:00,  1.59it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0901, device='cuda:0')\n",
      "r2_score:  -0.04090762138366699\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0909, device='cuda:0')\n",
      "r2_score:  -0.10779905319213867\n",
      "Epoch 13:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.022278964519500732\n",
      "Epoch 13:   9%|▉         | 1/11 [00:00<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0726, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07685387134552002\n",
      "Epoch 13:  18%|█▊        | 2/11 [00:01<00:06,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0727, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09505850076675415\n",
      "Epoch 13:  27%|██▋       | 3/11 [00:02<00:05,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0747, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05486011505126953\n",
      "Epoch 13:  36%|███▋      | 4/11 [00:02<00:04,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.051891446113586426\n",
      "Epoch 13:  45%|████▌     | 5/11 [00:03<00:04,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.01942211389541626\n",
      "Epoch 13:  55%|█████▍    | 6/11 [00:03<00:03,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0902, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05758368968963623\n",
      "Epoch 13:  64%|██████▎   | 7/11 [00:04<00:02,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0718, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08948516845703125\n",
      "Epoch 13:  73%|███████▎  | 8/11 [00:05<00:02,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06444907188415527\n",
      "Epoch 13:  82%|████████▏ | 9/11 [00:06<00:01,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.023334503173828125\n",
      "Epoch 13:  91%|█████████ | 10/11 [00:06<00:00,  1.47it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06380081176757812\n",
      "Epoch 13: 100%|██████████| 11/11 [00:07<00:00,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0896, device='cuda:0')\n",
      "r2_score:  -0.03437459468841553\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0873, device='cuda:0')\n",
      "r2_score:  -0.0633782148361206\n",
      "Epoch 14:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07519131898880005\n",
      "Epoch 14:   9%|▉         | 1/11 [00:00<00:06,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06693804264068604\n",
      "Epoch 14:  18%|█▊        | 2/11 [00:01<00:06,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0669, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1520177125930786\n",
      "Epoch 14:  27%|██▋       | 3/11 [00:02<00:05,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0700, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1169174313545227\n",
      "Epoch 14:  36%|███▋      | 4/11 [00:02<00:04,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.05053281784057617\n",
      "Epoch 14:  45%|████▌     | 5/11 [00:03<00:03,  1.51it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1084369421005249\n",
      "Epoch 14:  55%|█████▍    | 6/11 [00:04<00:03,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10953974723815918\n",
      "Epoch 14:  64%|██████▎   | 7/11 [00:04<00:02,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08756542205810547\n",
      "Epoch 14:  73%|███████▎  | 8/11 [00:05<00:02,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0663, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09296715259552002\n",
      "Epoch 14:  82%|████████▏ | 9/11 [00:06<00:01,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0699, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.03501647710800171\n",
      "Epoch 14:  91%|█████████ | 10/11 [00:06<00:00,  1.50it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0834, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0876917839050293\n",
      "Epoch 14: 100%|██████████| 11/11 [00:07<00:00,  1.56it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0870, device='cuda:0')\n",
      "r2_score:  -0.00480043888092041\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0858, device='cuda:0')\n",
      "r2_score:  -0.04518723487854004\n",
      "Epoch 15:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0676, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0857129693031311\n",
      "Epoch 15:   9%|▉         | 1/11 [00:00<00:08,  1.20it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0772, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13461613655090332\n",
      "Epoch 15:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.041144371032714844\n",
      "Epoch 15:  27%|██▋       | 3/11 [00:02<00:05,  1.37it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07774537801742554\n",
      "Epoch 15:  36%|███▋      | 4/11 [00:02<00:04,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0946892499923706\n",
      "Epoch 15:  45%|████▌     | 5/11 [00:03<00:04,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0842, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04634064435958862\n",
      "Epoch 15:  55%|█████▍    | 6/11 [00:04<00:03,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0755, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13193482160568237\n",
      "Epoch 15:  64%|██████▎   | 7/11 [00:04<00:02,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09576666355133057\n",
      "Epoch 15:  73%|███████▎  | 8/11 [00:05<00:02,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0702, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0633041262626648\n",
      "Epoch 15:  82%|████████▏ | 9/11 [00:06<00:01,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0714, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11306232213973999\n",
      "Epoch 15:  91%|█████████ | 10/11 [00:06<00:00,  1.46it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06611430644989014\n",
      "Epoch 15: 100%|██████████| 11/11 [00:07<00:00,  1.53it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0903, device='cuda:0')\n",
      "r2_score:  -0.04276633262634277\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0891, device='cuda:0')\n",
      "r2_score:  -0.08513879776000977\n",
      "Epoch 16:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07915085554122925\n",
      "Epoch 16:   9%|▉         | 1/11 [00:00<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06612849235534668\n",
      "Epoch 16:  18%|█▊        | 2/11 [00:01<00:06,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0719, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13983649015426636\n",
      "Epoch 16:  27%|██▋       | 3/11 [00:02<00:05,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0910, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.06138956546783447\n",
      "Epoch 16:  36%|███▋      | 4/11 [00:02<00:04,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1324518322944641\n",
      "Epoch 16:  45%|████▌     | 5/11 [00:03<00:04,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07046294212341309\n",
      "Epoch 16:  55%|█████▍    | 6/11 [00:04<00:03,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.059714674949645996\n",
      "Epoch 16:  64%|██████▎   | 7/11 [00:04<00:02,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0841, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10154420137405396\n",
      "Epoch 16:  73%|███████▎  | 8/11 [00:05<00:02,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1096416711807251\n",
      "Epoch 16:  82%|████████▏ | 9/11 [00:06<00:01,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0725, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0878981351852417\n",
      "Epoch 16:  91%|█████████ | 10/11 [00:06<00:00,  1.49it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.0033032894134521484\n",
      "Epoch 16: 100%|██████████| 11/11 [00:07<00:00,  1.55it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0860, device='cuda:0')\n",
      "r2_score:  0.006276607513427734\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0900, device='cuda:0')\n",
      "r2_score:  -0.096396803855896\n",
      "Epoch 17:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11761146783828735\n",
      "Epoch 17:   9%|▉         | 1/11 [00:00<00:06,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11884361505508423\n",
      "Epoch 17:  18%|█▊        | 2/11 [00:01<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06054568290710449\n",
      "Epoch 17:  27%|██▋       | 3/11 [00:02<00:05,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1126636266708374\n",
      "Epoch 17:  36%|███▋      | 4/11 [00:02<00:04,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13916254043579102\n",
      "Epoch 17:  45%|████▌     | 5/11 [00:03<00:04,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.06492221355438232\n",
      "Epoch 17:  55%|█████▍    | 6/11 [00:04<00:03,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0703, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07468342781066895\n",
      "Epoch 17:  64%|██████▎   | 7/11 [00:04<00:02,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.04330110549926758\n",
      "Epoch 17:  73%|███████▎  | 8/11 [00:05<00:02,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.039269447326660156\n",
      "Epoch 17:  82%|████████▏ | 9/11 [00:06<00:01,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08373963832855225\n",
      "Epoch 17:  91%|█████████ | 10/11 [00:07<00:00,  1.42it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0943, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  -0.026027441024780273\n",
      "Epoch 17: 100%|██████████| 11/11 [00:07<00:00,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0960, device='cuda:0')\n",
      "r2_score:  -0.10833823680877686\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0929, device='cuda:0')\n",
      "r2_score:  -0.1319645643234253\n",
      "Epoch 18:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11736160516738892\n",
      "Epoch 18:   9%|▉         | 1/11 [00:00<00:06,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0726, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10692662000656128\n",
      "Epoch 18:  18%|█▊        | 2/11 [00:01<00:06,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.12668824195861816\n",
      "Epoch 18:  27%|██▋       | 3/11 [00:02<00:05,  1.38it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15074509382247925\n",
      "Epoch 18:  36%|███▋      | 4/11 [00:02<00:05,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10467696189880371\n",
      "Epoch 18:  45%|████▌     | 5/11 [00:03<00:04,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15593838691711426\n",
      "Epoch 18:  55%|█████▍    | 6/11 [00:04<00:03,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0716, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11702364683151245\n",
      "Epoch 18:  64%|██████▎   | 7/11 [00:04<00:02,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09996455907821655\n",
      "Epoch 18:  73%|███████▎  | 8/11 [00:05<00:02,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1025039553642273\n",
      "Epoch 18:  82%|████████▏ | 9/11 [00:06<00:01,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.07421523332595825\n",
      "Epoch 18:  91%|█████████ | 10/11 [00:06<00:00,  1.43it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1179155707359314\n",
      "Epoch 18: 100%|██████████| 11/11 [00:07<00:00,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0881, device='cuda:0')\n",
      "r2_score:  -0.01789689064025879\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0943, device='cuda:0')\n",
      "r2_score:  -0.14911842346191406\n",
      "Epoch 19:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.09089541435241699\n",
      "Epoch 19:   9%|▉         | 1/11 [00:00<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0756, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1551342010498047\n",
      "Epoch 19:  18%|█▊        | 2/11 [00:01<00:06,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0955883264541626\n",
      "Epoch 19:  27%|██▋       | 3/11 [00:02<00:05,  1.48it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0720, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08659631013870239\n",
      "Epoch 19:  36%|███▋      | 4/11 [00:02<00:04,  1.47it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0658, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15425139665603638\n",
      "Epoch 19:  45%|████▌     | 5/11 [00:03<00:04,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0701, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.12334352731704712\n",
      "Epoch 19:  55%|█████▍    | 6/11 [00:04<00:03,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11976730823516846\n",
      "Epoch 19:  64%|██████▎   | 7/11 [00:04<00:02,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0699, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1429012417793274\n",
      "Epoch 19:  73%|███████▎  | 8/11 [00:05<00:02,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0674, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1657828688621521\n",
      "Epoch 19:  82%|████████▏ | 9/11 [00:06<00:01,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.08042800426483154\n",
      "Epoch 19:  91%|█████████ | 10/11 [00:06<00:00,  1.46it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0700, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.12408298254013062\n",
      "Epoch 19: 100%|██████████| 11/11 [00:07<00:00,  1.52it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0877, device='cuda:0')\n",
      "r2_score:  -0.013244032859802246\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0882, device='cuda:0')\n",
      "r2_score:  -0.07445085048675537\n",
      "Epoch 20:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.10881209373474121\n",
      "Epoch 20:   9%|▉         | 1/11 [00:00<00:06,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0681, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11072033643722534\n",
      "Epoch 20:  18%|█▊        | 2/11 [00:01<00:06,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.18415242433547974\n",
      "Epoch 20:  27%|██▋       | 3/11 [00:02<00:05,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.159795880317688\n",
      "Epoch 20:  36%|███▋      | 4/11 [00:02<00:04,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0843, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.0898546576499939\n",
      "Epoch 20:  45%|████▌     | 5/11 [00:03<00:04,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0669, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13919639587402344\n",
      "Epoch 20:  55%|█████▍    | 6/11 [00:04<00:03,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0726, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15850698947906494\n",
      "Epoch 20:  64%|██████▎   | 7/11 [00:05<00:02,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0684, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.13807886838912964\n",
      "Epoch 20:  73%|███████▎  | 8/11 [00:05<00:02,  1.37it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0742, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15197980403900146\n",
      "Epoch 20:  82%|████████▏ | 9/11 [00:06<00:01,  1.35it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0684, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.15883415937423706\n",
      "Epoch 20:  91%|█████████ | 10/11 [00:07<00:00,  1.35it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0660, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.21776217222213745\n",
      "Epoch 20: 100%|██████████| 11/11 [00:07<00:00,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0968, device='cuda:0')\n",
      "r2_score:  -0.11755073070526123\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0946, device='cuda:0')\n",
      "r2_score:  -0.15213298797607422\n",
      "Epoch 21:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0743, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11613309383392334\n",
      "Epoch 21:   9%|▉         | 1/11 [00:00<00:07,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0698, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.18942326307296753\n",
      "Epoch 21:  18%|█▊        | 2/11 [00:01<00:07,  1.28it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0649, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.19439876079559326\n",
      "Epoch 21:  27%|██▋       | 3/11 [00:02<00:05,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0712, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.17438870668411255\n",
      "Epoch 21:  36%|███▋      | 4/11 [00:02<00:05,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0595, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.21637028455734253\n",
      "Epoch 21:  45%|████▌     | 5/11 [00:03<00:04,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0715, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.11194515228271484\n",
      "Epoch 21:  55%|█████▍    | 6/11 [00:04<00:03,  1.40it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0728, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1786298155784607\n",
      "Epoch 21:  64%|██████▎   | 7/11 [00:04<00:02,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.19252312183380127\n",
      "Epoch 21:  73%|███████▎  | 8/11 [00:05<00:02,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.19660228490829468\n",
      "Epoch 21:  82%|████████▏ | 9/11 [00:06<00:01,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0669, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1786707043647766\n",
      "Epoch 21:  91%|█████████ | 10/11 [00:06<00:00,  1.43it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1982380747795105\n",
      "Epoch 21: 100%|██████████| 11/11 [00:07<00:00,  1.49it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1021, device='cuda:0')\n",
      "r2_score:  -0.17876052856445312\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0991, device='cuda:0')\n",
      "r2_score:  -0.2068883180618286\n",
      "Epoch 22:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1684918999671936\n",
      "Epoch 22:   9%|▉         | 1/11 [00:00<00:06,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0696, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.17270833253860474\n",
      "Epoch 22:  18%|█▊        | 2/11 [00:01<00:06,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.27768653631210327\n",
      "Epoch 22:  27%|██▋       | 3/11 [00:02<00:05,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0628, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.17829161882400513\n",
      "Epoch 22:  36%|███▋      | 4/11 [00:02<00:04,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0597, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24932044744491577\n",
      "Epoch 22:  45%|████▌     | 5/11 [00:03<00:04,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0664, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24884819984436035\n",
      "Epoch 22:  55%|█████▍    | 6/11 [00:04<00:03,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0644, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1872270703315735\n",
      "Epoch 22:  64%|██████▎   | 7/11 [00:04<00:02,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0680, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.22080332040786743\n",
      "Epoch 22:  73%|███████▎  | 8/11 [00:05<00:02,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.1594793200492859\n",
      "Epoch 22:  82%|████████▏ | 9/11 [00:06<00:01,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2564138174057007\n",
      "Epoch 22:  91%|█████████ | 10/11 [00:06<00:00,  1.43it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.22834235429763794\n",
      "Epoch 22: 100%|██████████| 11/11 [00:07<00:00,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0962, device='cuda:0')\n",
      "r2_score:  -0.11086511611938477\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1012, device='cuda:0')\n",
      "r2_score:  -0.23253512382507324\n",
      "Epoch 23:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0637, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.25613319873809814\n",
      "Epoch 23:   9%|▉         | 1/11 [00:00<00:07,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.258112370967865\n",
      "Epoch 23:  18%|█▊        | 2/11 [00:01<00:06,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24212491512298584\n",
      "Epoch 23:  27%|██▋       | 3/11 [00:02<00:05,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0623, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.26053404808044434\n",
      "Epoch 23:  36%|███▋      | 4/11 [00:02<00:04,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0589, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.31551027297973633\n",
      "Epoch 23:  45%|████▌     | 5/11 [00:03<00:04,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.23699826002120972\n",
      "Epoch 23:  55%|█████▍    | 6/11 [00:04<00:03,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0638, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.25328612327575684\n",
      "Epoch 23:  64%|██████▎   | 7/11 [00:05<00:02,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2857515215873718\n",
      "Epoch 23:  73%|███████▎  | 8/11 [00:05<00:02,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0591, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2474726438522339\n",
      "Epoch 23:  82%|████████▏ | 9/11 [00:06<00:01,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2561342716217041\n",
      "Epoch 23:  91%|█████████ | 10/11 [00:07<00:00,  1.40it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.26312631368637085\n",
      "Epoch 23: 100%|██████████| 11/11 [00:07<00:00,  1.46it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0955, device='cuda:0')\n",
      "r2_score:  -0.10285079479217529\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0954, device='cuda:0')\n",
      "r2_score:  -0.1624133586883545\n",
      "Epoch 24:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.258114218711853\n",
      "Epoch 24:   9%|▉         | 1/11 [00:00<00:07,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0649, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.25797760486602783\n",
      "Epoch 24:  18%|█▊        | 2/11 [00:01<00:06,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2398257851600647\n",
      "Epoch 24:  27%|██▋       | 3/11 [00:02<00:05,  1.38it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0612, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.27030307054519653\n",
      "Epoch 24:  36%|███▋      | 4/11 [00:02<00:04,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0576, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.27493369579315186\n",
      "Epoch 24:  45%|████▌     | 5/11 [00:03<00:04,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0647, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.273063063621521\n",
      "Epoch 24:  55%|█████▍    | 6/11 [00:04<00:03,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0650, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2295238971710205\n",
      "Epoch 24:  64%|██████▎   | 7/11 [00:04<00:02,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.34015941619873047\n",
      "Epoch 24:  73%|███████▎  | 8/11 [00:05<00:02,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0598, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3079717755317688\n",
      "Epoch 24:  82%|████████▏ | 9/11 [00:06<00:01,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.32174861431121826\n",
      "Epoch 24:  91%|█████████ | 10/11 [00:06<00:00,  1.44it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0595, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3302183151245117\n",
      "Epoch 24: 100%|██████████| 11/11 [00:07<00:00,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0960, device='cuda:0')\n",
      "r2_score:  -0.10900342464447021\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0986, device='cuda:0')\n",
      "r2_score:  -0.20045506954193115\n",
      "Epoch 25:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0566, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.28701287508010864\n",
      "Epoch 25:   9%|▉         | 1/11 [00:00<00:07,  1.40it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.27464669942855835\n",
      "Epoch 25:  18%|█▊        | 2/11 [00:01<00:06,  1.39it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0581, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.283122718334198\n",
      "Epoch 25:  27%|██▋       | 3/11 [00:02<00:05,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0579, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2978000044822693\n",
      "Epoch 25:  36%|███▋      | 4/11 [00:02<00:04,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0583, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3103641867637634\n",
      "Epoch 25:  45%|████▌     | 5/11 [00:03<00:04,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.29244452714920044\n",
      "Epoch 25:  55%|█████▍    | 6/11 [00:04<00:03,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0567, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3136554956436157\n",
      "Epoch 25:  64%|██████▎   | 7/11 [00:04<00:02,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0615, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.25807106494903564\n",
      "Epoch 25:  73%|███████▎  | 8/11 [00:05<00:02,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0594, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3284795880317688\n",
      "Epoch 25:  82%|████████▏ | 9/11 [00:06<00:01,  1.45it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.29049426317214966\n",
      "Epoch 25:  91%|█████████ | 10/11 [00:06<00:00,  1.44it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0590, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3027935028076172\n",
      "Epoch 25: 100%|██████████| 11/11 [00:07<00:00,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0980, device='cuda:0')\n",
      "r2_score:  -0.13137400150299072\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.0981, device='cuda:0')\n",
      "r2_score:  -0.19443416595458984\n",
      "Epoch 26:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0614, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2915264368057251\n",
      "Epoch 26:   9%|▉         | 1/11 [00:00<00:07,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3177791237831116\n",
      "Epoch 26:  18%|█▊        | 2/11 [00:01<00:06,  1.38it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2686383128166199\n",
      "Epoch 26:  27%|██▋       | 3/11 [00:02<00:05,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24426889419555664\n",
      "Epoch 26:  36%|███▋      | 4/11 [00:02<00:04,  1.42it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0547, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.34011709690093994\n",
      "Epoch 26:  45%|████▌     | 5/11 [00:03<00:04,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0547, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3284085988998413\n",
      "Epoch 26:  55%|█████▍    | 6/11 [00:04<00:03,  1.43it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0621, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.312555730342865\n",
      "Epoch 26:  64%|██████▎   | 7/11 [00:04<00:02,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.289787232875824\n",
      "Epoch 26:  73%|███████▎  | 8/11 [00:05<00:02,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0575, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.31394386291503906\n",
      "Epoch 26:  82%|████████▏ | 9/11 [00:06<00:01,  1.44it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.30652374029159546\n",
      "Epoch 26:  91%|█████████ | 10/11 [00:06<00:00,  1.44it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0565, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.33921998739242554\n",
      "Epoch 26: 100%|██████████| 11/11 [00:07<00:00,  1.50it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1006, device='cuda:0')\n",
      "r2_score:  -0.16199421882629395\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1040, device='cuda:0')\n",
      "r2_score:  -0.2664809226989746\n",
      "Epoch 27:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3662978410720825\n",
      "Epoch 27:   9%|▉         | 1/11 [00:00<00:07,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3988661766052246\n",
      "Epoch 27:  18%|█▊        | 2/11 [00:01<00:06,  1.29it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0625, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.26726651191711426\n",
      "Epoch 27:  27%|██▋       | 3/11 [00:02<00:06,  1.20it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0588, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3463290333747864\n",
      "Epoch 27:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.30072635412216187\n",
      "Epoch 27:  45%|████▌     | 5/11 [00:04<00:05,  1.11it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.39238882064819336\n",
      "Epoch 27:  55%|█████▍    | 6/11 [00:05<00:04,  1.14it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0570, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.32772761583328247\n",
      "Epoch 27:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24998700618743896\n",
      "Epoch 27:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3127882480621338\n",
      "Epoch 27:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0585, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2854093313217163\n",
      "Epoch 27:  91%|█████████ | 10/11 [00:08<00:00,  1.19it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.24555248022079468\n",
      "Epoch 27: 100%|██████████| 11/11 [00:08<00:00,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1001, device='cuda:0')\n",
      "r2_score:  -0.15588891506195068\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1031, device='cuda:0')\n",
      "r2_score:  -0.2559698820114136\n",
      "Epoch 28:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0506, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38688868284225464\n",
      "Epoch 28:   9%|▉         | 1/11 [00:00<00:07,  1.33it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0607, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3388358950614929\n",
      "Epoch 28:  18%|█▊        | 2/11 [00:01<00:06,  1.35it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.30562806129455566\n",
      "Epoch 28:  27%|██▋       | 3/11 [00:02<00:05,  1.37it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.40501654148101807\n",
      "Epoch 28:  36%|███▋      | 4/11 [00:03<00:05,  1.32it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.39115405082702637\n",
      "Epoch 28:  45%|████▌     | 5/11 [00:03<00:04,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3025975823402405\n",
      "Epoch 28:  55%|█████▍    | 6/11 [00:04<00:03,  1.35it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.35190635919570923\n",
      "Epoch 28:  64%|██████▎   | 7/11 [00:05<00:03,  1.33it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3252015709877014\n",
      "Epoch 28:  73%|███████▎  | 8/11 [00:05<00:02,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3535536527633667\n",
      "Epoch 28:  82%|████████▏ | 9/11 [00:06<00:01,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.27953338623046875\n",
      "Epoch 28:  91%|█████████ | 10/11 [00:07<00:00,  1.35it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.23680120706558228\n",
      "Epoch 28: 100%|██████████| 11/11 [00:07<00:00,  1.41it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0979, device='cuda:0')\n",
      "r2_score:  -0.13022863864898682\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1023, device='cuda:0')\n",
      "r2_score:  -0.2458939552307129\n",
      "Epoch 29:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0628, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.29494577646255493\n",
      "Epoch 29:   9%|▉         | 1/11 [00:00<00:08,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.33839166164398193\n",
      "Epoch 29:  18%|█▊        | 2/11 [00:01<00:06,  1.30it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38687586784362793\n",
      "Epoch 29:  27%|██▋       | 3/11 [00:02<00:05,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3147727847099304\n",
      "Epoch 29:  36%|███▋      | 4/11 [00:03<00:05,  1.30it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0537, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4129188656806946\n",
      "Epoch 29:  45%|████▌     | 5/11 [00:03<00:04,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0575, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3291553854942322\n",
      "Epoch 29:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.306316614151001\n",
      "Epoch 29:  64%|██████▎   | 7/11 [00:05<00:03,  1.23it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3627404570579529\n",
      "Epoch 29:  73%|███████▎  | 8/11 [00:06<00:02,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0530, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.35853761434555054\n",
      "Epoch 29:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0502, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3791201710700989\n",
      "Epoch 29:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0440, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.37557482719421387\n",
      "Epoch 29: 100%|██████████| 11/11 [00:08<00:00,  1.31it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1027, device='cuda:0')\n",
      "r2_score:  -0.1864151954650879\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1054, device='cuda:0')\n",
      "r2_score:  -0.2834665775299072\n",
      "Epoch 30:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.32349133491516113\n",
      "Epoch 30:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.32115209102630615\n",
      "Epoch 30:  18%|█▊        | 2/11 [00:01<00:06,  1.31it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3462715744972229\n",
      "Epoch 30:  27%|██▋       | 3/11 [00:02<00:06,  1.33it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4022967219352722\n",
      "Epoch 30:  36%|███▋      | 4/11 [00:03<00:05,  1.30it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0519, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3852154016494751\n",
      "Epoch 30:  45%|████▌     | 5/11 [00:03<00:04,  1.33it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0511, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.370319128036499\n",
      "Epoch 30:  55%|█████▍    | 6/11 [00:04<00:03,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.388855516910553\n",
      "Epoch 30:  64%|██████▎   | 7/11 [00:05<00:02,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0593, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.30070167779922485\n",
      "Epoch 30:  73%|███████▎  | 8/11 [00:05<00:02,  1.36it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.36730825901031494\n",
      "Epoch 30:  82%|████████▏ | 9/11 [00:06<00:01,  1.34it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0548, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4223777651786804\n",
      "Epoch 30:  91%|█████████ | 10/11 [00:07<00:00,  1.34it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4164412021636963\n",
      "Epoch 30: 100%|██████████| 11/11 [00:07<00:00,  1.40it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1008, device='cuda:0')\n",
      "r2_score:  -0.16359806060791016\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1065, device='cuda:0')\n",
      "r2_score:  -0.2972452640533447\n",
      "Epoch 31:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3441397547721863\n",
      "Epoch 31:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0475, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.37964141368865967\n",
      "Epoch 31:  18%|█▊        | 2/11 [00:01<00:07,  1.14it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0511, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38066357374191284\n",
      "Epoch 31:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0554, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3654285669326782\n",
      "Epoch 31:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0555, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3523032069206238\n",
      "Epoch 31:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0532, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3913232088088989\n",
      "Epoch 31:  55%|█████▍    | 6/11 [00:05<00:04,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4698385000228882\n",
      "Epoch 31:  64%|██████▎   | 7/11 [00:05<00:03,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.35264909267425537\n",
      "Epoch 31:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0474, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4195178747177124\n",
      "Epoch 31:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0558, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3467170000076294\n",
      "Epoch 31:  91%|█████████ | 10/11 [00:08<00:00,  1.17it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0429, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3855917453765869\n",
      "Epoch 31: 100%|██████████| 11/11 [00:10<00:00,  1.09it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1040, device='cuda:0')\n",
      "r2_score:  -0.20066726207733154\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1013, device='cuda:0')\n",
      "r2_score:  -0.234472393989563\n",
      "Epoch 32:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38276124000549316\n",
      "Epoch 32:   9%|▉         | 1/11 [00:00<00:08,  1.11it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0492, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.39562326669692993\n",
      "Epoch 32:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.41583478450775146\n",
      "Epoch 32:  27%|██▋       | 3/11 [00:02<00:07,  1.07it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3499630093574524\n",
      "Epoch 32:  36%|███▋      | 4/11 [00:04<00:08,  0.82it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0591, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.34275370836257935\n",
      "Epoch 32:  45%|████▌     | 5/11 [00:06<00:07,  0.82it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0477, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.41151905059814453\n",
      "Epoch 32:  55%|█████▍    | 6/11 [00:07<00:05,  0.85it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0468, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4009774327278137\n",
      "Epoch 32:  64%|██████▎   | 7/11 [00:08<00:04,  0.87it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0523, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38712626695632935\n",
      "Epoch 32:  73%|███████▎  | 8/11 [00:09<00:03,  0.87it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3663177490234375\n",
      "Epoch 32:  82%|████████▏ | 9/11 [00:10<00:02,  0.87it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0501, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3829127550125122\n",
      "Epoch 32:  91%|█████████ | 10/11 [00:11<00:01,  0.90it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0504, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3950355648994446\n",
      "Epoch 32: 100%|██████████| 11/11 [00:11<00:00,  0.96it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1057, device='cuda:0')\n",
      "r2_score:  -0.2210625410079956\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1082, device='cuda:0')\n",
      "r2_score:  -0.3176685571670532\n",
      "Epoch 33:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0504, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.40669918060302734\n",
      "Epoch 33:   9%|▉         | 1/11 [00:00<00:09,  1.00it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3703639507293701\n",
      "Epoch 33:  18%|█▊        | 2/11 [00:02<00:09,  0.93it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0573, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3376094698905945\n",
      "Epoch 33:  27%|██▋       | 3/11 [00:02<00:07,  1.03it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.48563051223754883\n",
      "Epoch 33:  36%|███▋      | 4/11 [00:03<00:06,  1.04it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.37700921297073364\n",
      "Epoch 33:  45%|████▌     | 5/11 [00:04<00:05,  1.07it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.46276628971099854\n",
      "Epoch 33:  55%|█████▍    | 6/11 [00:05<00:04,  1.11it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.46548032760620117\n",
      "Epoch 33:  64%|██████▎   | 7/11 [00:06<00:03,  1.14it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.40197592973709106\n",
      "Epoch 33:  73%|███████▎  | 8/11 [00:06<00:02,  1.16it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0477, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.39004063606262207\n",
      "Epoch 33:  82%|████████▏ | 9/11 [00:07<00:01,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0481, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.40664780139923096\n",
      "Epoch 33:  91%|█████████ | 10/11 [00:08<00:00,  1.20it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0544, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.2893450856208801\n",
      "Epoch 33: 100%|██████████| 11/11 [00:08<00:00,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1039, device='cuda:0')\n",
      "r2_score:  -0.19948875904083252\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1055, device='cuda:0')\n",
      "r2_score:  -0.28466737270355225\n",
      "Epoch 34:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0514, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.38871318101882935\n",
      "Epoch 34:   9%|▉         | 1/11 [00:00<00:07,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4501863718032837\n",
      "Epoch 34:  18%|█▊        | 2/11 [00:01<00:06,  1.32it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4959166646003723\n",
      "Epoch 34:  27%|██▋       | 3/11 [00:02<00:06,  1.15it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4042818546295166\n",
      "Epoch 34:  36%|███▋      | 4/11 [00:03<00:05,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0484, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4046379327774048\n",
      "Epoch 34:  45%|████▌     | 5/11 [00:04<00:04,  1.22it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0507, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.42486822605133057\n",
      "Epoch 34:  55%|█████▍    | 6/11 [00:04<00:04,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.39036285877227783\n",
      "Epoch 34:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.42931807041168213\n",
      "Epoch 34:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4160749912261963\n",
      "Epoch 34:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0475, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4130655527114868\n",
      "Epoch 34:  91%|█████████ | 10/11 [00:07<00:00,  1.29it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0502, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4063863754272461\n",
      "Epoch 34: 100%|██████████| 11/11 [00:08<00:00,  1.35it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1051, device='cuda:0')\n",
      "r2_score:  -0.2141636610031128\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1042, device='cuda:0')\n",
      "r2_score:  -0.2690829038619995\n",
      "Epoch 35:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0462, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.43201178312301636\n",
      "Epoch 35:   9%|▉         | 1/11 [00:00<00:08,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0435, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4607298970222473\n",
      "Epoch 35:  18%|█▊        | 2/11 [00:01<00:07,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0444, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4988887906074524\n",
      "Epoch 35:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0515, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.41797083616256714\n",
      "Epoch 35:  36%|███▋      | 4/11 [00:03<00:05,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0429, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.44237715005874634\n",
      "Epoch 35:  45%|████▌     | 5/11 [00:03<00:04,  1.28it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4803870916366577\n",
      "Epoch 35:  55%|█████▍    | 6/11 [00:04<00:03,  1.29it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4396355152130127\n",
      "Epoch 35:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.42326945066452026\n",
      "Epoch 35:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.477364718914032\n",
      "Epoch 35:  82%|████████▏ | 9/11 [00:07<00:01,  1.23it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0459, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4543488621711731\n",
      "Epoch 35:  91%|█████████ | 10/11 [00:08<00:00,  1.13it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4981836676597595\n",
      "Epoch 35: 100%|██████████| 11/11 [00:09<00:00,  1.13it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1051, device='cuda:0')\n",
      "r2_score:  -0.21383166313171387\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1088, device='cuda:0')\n",
      "r2_score:  -0.32578253746032715\n",
      "Epoch 36:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4831897020339966\n",
      "Epoch 36:   9%|▉         | 1/11 [00:00<00:07,  1.25it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0434, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.45210158824920654\n",
      "Epoch 36:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0363, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5192116498947144\n",
      "Epoch 36:  27%|██▋       | 3/11 [00:02<00:07,  1.10it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4421655535697937\n",
      "Epoch 36:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4975287914276123\n",
      "Epoch 36:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0507, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.37479376792907715\n",
      "Epoch 36:  55%|█████▍    | 6/11 [00:05<00:04,  1.12it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4186422824859619\n",
      "Epoch 36:  64%|██████▎   | 7/11 [00:06<00:03,  1.10it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.47203248739242554\n",
      "Epoch 36:  73%|███████▎  | 8/11 [00:07<00:02,  1.12it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4456794261932373\n",
      "Epoch 36:  82%|████████▏ | 9/11 [00:07<00:01,  1.13it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.3740236759185791\n",
      "Epoch 36:  91%|█████████ | 10/11 [00:08<00:00,  1.15it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.506035566329956\n",
      "Epoch 36: 100%|██████████| 11/11 [00:09<00:00,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1101, device='cuda:0')\n",
      "r2_score:  -0.2713881731033325\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1082, device='cuda:0')\n",
      "r2_score:  -0.3180985450744629\n",
      "Epoch 37:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5000853538513184\n",
      "Epoch 37:   9%|▉         | 1/11 [00:00<00:09,  1.10it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0452, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4071868062019348\n",
      "Epoch 37:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0515, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4367039203643799\n",
      "Epoch 37:  27%|██▋       | 3/11 [00:02<00:06,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.48349690437316895\n",
      "Epoch 37:  36%|███▋      | 4/11 [00:03<00:05,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4733496904373169\n",
      "Epoch 37:  45%|████▌     | 5/11 [00:03<00:04,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.41463911533355713\n",
      "Epoch 37:  55%|█████▍    | 6/11 [00:04<00:03,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.47112327814102173\n",
      "Epoch 37:  64%|██████▎   | 7/11 [00:05<00:03,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.46336615085601807\n",
      "Epoch 37:  73%|███████▎  | 8/11 [00:06<00:02,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.45413583517074585\n",
      "Epoch 37:  82%|████████▏ | 9/11 [00:07<00:01,  1.28it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.47680890560150146\n",
      "Epoch 37:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0384, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.48547106981277466\n",
      "Epoch 37: 100%|██████████| 11/11 [00:08<00:00,  1.30it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1069, device='cuda:0')\n",
      "r2_score:  -0.23426783084869385\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1076, device='cuda:0')\n",
      "r2_score:  -0.31068670749664307\n",
      "Epoch 38:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4414304494857788\n",
      "Epoch 38:   9%|▉         | 1/11 [00:01<00:10,  0.98it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0342, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5711191892623901\n",
      "Epoch 38:  18%|█▊        | 2/11 [00:02<00:09,  0.99it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4578930139541626\n",
      "Epoch 38:  27%|██▋       | 3/11 [00:02<00:07,  1.02it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5148385763168335\n",
      "Epoch 38:  36%|███▋      | 4/11 [00:03<00:06,  1.08it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4356895685195923\n",
      "Epoch 38:  45%|████▌     | 5/11 [00:04<00:05,  1.13it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5208636522293091\n",
      "Epoch 38:  55%|█████▍    | 6/11 [00:05<00:04,  1.16it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4891008734703064\n",
      "Epoch 38:  64%|██████▎   | 7/11 [00:05<00:03,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0440, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5041404366493225\n",
      "Epoch 38:  73%|███████▎  | 8/11 [00:06<00:02,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5083125829696655\n",
      "Epoch 38:  82%|████████▏ | 9/11 [00:07<00:01,  1.21it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0425, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5120495557785034\n",
      "Epoch 38:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5082243084907532\n",
      "Epoch 38: 100%|██████████| 11/11 [00:09<00:00,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1100, device='cuda:0')\n",
      "r2_score:  -0.27008235454559326\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1072, device='cuda:0')\n",
      "r2_score:  -0.30630946159362793\n",
      "Epoch 39:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0365, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5485494136810303\n",
      "Epoch 39:   9%|▉         | 1/11 [00:00<00:08,  1.12it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5064824819564819\n",
      "Epoch 39:  18%|█▊        | 2/11 [00:01<00:07,  1.23it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4861079454421997\n",
      "Epoch 39:  27%|██▋       | 3/11 [00:02<00:06,  1.28it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5583698749542236\n",
      "Epoch 39:  36%|███▋      | 4/11 [00:03<00:05,  1.30it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4582839012145996\n",
      "Epoch 39:  45%|████▌     | 5/11 [00:03<00:04,  1.31it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.47610920667648315\n",
      "Epoch 39:  55%|█████▍    | 6/11 [00:04<00:03,  1.32it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5197274684906006\n",
      "Epoch 39:  64%|██████▎   | 7/11 [00:05<00:03,  1.31it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0400, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4856570363044739\n",
      "Epoch 39:  73%|███████▎  | 8/11 [00:06<00:02,  1.32it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0326, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.6013926267623901\n",
      "Epoch 39:  82%|████████▏ | 9/11 [00:06<00:01,  1.33it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0372, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5626420974731445\n",
      "Epoch 39:  91%|█████████ | 10/11 [00:07<00:00,  1.33it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5102480053901672\n",
      "Epoch 39: 100%|██████████| 11/11 [00:07<00:00,  1.38it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1137, device='cuda:0')\n",
      "r2_score:  -0.3130531311035156\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1067, device='cuda:0')\n",
      "r2_score:  -0.2992117404937744\n",
      "Epoch 40:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5145657062530518\n",
      "Epoch 40:   9%|▉         | 1/11 [00:01<00:13,  0.77it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5150530934333801\n",
      "Epoch 40:  18%|█▊        | 2/11 [00:02<00:10,  0.89it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0353, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5645156502723694\n",
      "Epoch 40:  27%|██▋       | 3/11 [00:03<00:08,  0.89it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4978798031806946\n",
      "Epoch 40:  36%|███▋      | 4/11 [00:04<00:07,  0.97it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5723438262939453\n",
      "Epoch 40:  45%|████▌     | 5/11 [00:04<00:05,  1.03it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5949039459228516\n",
      "Epoch 40:  55%|█████▍    | 6/11 [00:05<00:04,  1.08it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.49327218532562256\n",
      "Epoch 40:  64%|██████▎   | 7/11 [00:06<00:03,  1.12it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0402, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5308630466461182\n",
      "Epoch 40:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5004695057868958\n",
      "Epoch 40:  82%|████████▏ | 9/11 [00:07<00:01,  1.17it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0361, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5645740628242493\n",
      "Epoch 40:  91%|█████████ | 10/11 [00:08<00:00,  1.18it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0424, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.4876841902732849\n",
      "Epoch 40: 100%|██████████| 11/11 [00:09<00:00,  1.19it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1164, device='cuda:0')\n",
      "r2_score:  -0.3439594507217407\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1059, device='cuda:0')\n",
      "r2_score:  -0.28957080841064453\n",
      "Epoch 41:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5383872985839844\n",
      "Epoch 41:   9%|▉         | 1/11 [00:01<00:11,  0.84it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5217667818069458\n",
      "Epoch 41:  18%|█▊        | 2/11 [00:02<00:09,  0.99it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0365, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5364087224006653\n",
      "Epoch 41:  27%|██▋       | 3/11 [00:02<00:07,  1.09it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5320215225219727\n",
      "Epoch 41:  36%|███▋      | 4/11 [00:03<00:06,  1.15it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5271817445755005\n",
      "Epoch 41:  45%|████▌     | 5/11 [00:04<00:05,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0380, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5139087438583374\n",
      "Epoch 41:  55%|█████▍    | 6/11 [00:04<00:04,  1.21it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5283330082893372\n",
      "Epoch 41:  64%|██████▎   | 7/11 [00:05<00:03,  1.24it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0428, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5491970777511597\n",
      "Epoch 41:  73%|███████▎  | 8/11 [00:06<00:02,  1.26it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.574572741985321\n",
      "Epoch 41:  82%|████████▏ | 9/11 [00:07<00:01,  1.24it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0361, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5876399278640747\n",
      "Epoch 41:  91%|█████████ | 10/11 [00:07<00:00,  1.26it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5449576377868652\n",
      "Epoch 41: 100%|██████████| 11/11 [00:08<00:00,  1.31it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1151, device='cuda:0')\n",
      "r2_score:  -0.3295893669128418\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1099, device='cuda:0')\n",
      "r2_score:  -0.3380934000015259\n",
      "Epoch 42:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5660758018493652\n",
      "Epoch 42:   9%|▉         | 1/11 [00:01<00:12,  0.78it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0380, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5551254749298096\n",
      "Epoch 42:  18%|█▊        | 2/11 [00:02<00:09,  0.95it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.572635293006897\n",
      "Epoch 42:  27%|██▋       | 3/11 [00:02<00:07,  1.05it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5868522524833679\n",
      "Epoch 42:  36%|███▋      | 4/11 [00:03<00:06,  1.10it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5508236885070801\n",
      "Epoch 42:  45%|████▌     | 5/11 [00:04<00:05,  1.14it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5724523067474365\n",
      "Epoch 42:  55%|█████▍    | 6/11 [00:05<00:04,  1.15it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5300559997558594\n",
      "Epoch 42:  64%|██████▎   | 7/11 [00:06<00:03,  1.16it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5656738877296448\n",
      "Epoch 42:  73%|███████▎  | 8/11 [00:06<00:02,  1.18it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0364, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5247887372970581\n",
      "Epoch 42:  82%|████████▏ | 9/11 [00:07<00:01,  1.20it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5714945793151855\n",
      "Epoch 42:  91%|█████████ | 10/11 [00:08<00:00,  1.21it/s, v_num=18]y.shape:  torch.Size([139, 1])\n",
      "y_hat.shape:  torch.Size([139, 1])\n",
      "loss:  tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.5822261571884155\n",
      "Epoch 42: 100%|██████████| 11/11 [00:08<00:00,  1.27it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.1182, device='cuda:0')\n",
      "r2_score:  -0.3645329475402832\n",
      "y.shape:  torch.Size([44, 1])\n",
      "y_hat.shape:  torch.Size([44, 1])\n",
      "loss:  tensor(0.1102, device='cuda:0')\n",
      "r2_score:  -0.34228360652923584\n",
      "Epoch 43:   0%|          | 0/11 [00:00<?, ?it/s, v_num=18]         y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.6737277507781982\n",
      "Epoch 43:   9%|▉         | 1/11 [00:00<00:09,  1.02it/s, v_num=18]y.shape:  torch.Size([256, 1])\n",
      "y_hat.shape:  torch.Size([256, 1])\n",
      "loss:  tensor(0.0304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "r2_score:  0.6180700063705444\n",
      "Epoch 43:  18%|█▊        | 2/11 [00:01<00:07,  1.18it/s, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  18%|█▊        | 2/11 [28:12<2:06:54,  0.00it/s, v_num=1]\n",
      "                                                                   \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[47], line 16\u001b[0m, in \u001b[0;36mAudioDS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# First, check if the image has already been generated\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mspectrogram_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(spectrogram_path)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\pathlib.py:1235\u001b[0m, in \u001b[0;36mPath.exists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.stat\u001b[1;34m(self, follow_symlinks)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;124;03mReturn the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;124;03mos.stat() does.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(\u001b[38;5;28mself\u001b[39m, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model_ld, result_ld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoencoder_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 22\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(latent_dim, num_classes, max_epochs, encoder_model_checkpoint)\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m Classifier(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, encoder_model_checkpoint\u001b[38;5;241m=\u001b[39mencoder_model_checkpoint)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jayor\\miniconda3\\envs\\synth-reconstruct\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "autoencoder_checkpoint = \"C:\\\\Users\\\\jayor\\\\Documents\\\\repos\\\\synth-reconstruct\\\\demo\\\\autoencoder\\\\saved_models\\\\audio_test\\\\final_model_dim_128\\\\lightning_logs\\\\version_0\\\\checkpoints\\\\epoch=199-step=2200.ckpt\"\n",
    "num_classes = 1\n",
    "latent_dim = 128\n",
    "model_ld, result_ld = train_classifier(latent_dim, num_classes, max_epochs=200, encoder_model_checkpoint=autoencoder_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, preset_name):\n",
    "    # Put model in evaluation mode\n",
    "    model.to(device).eval()\n",
    "\n",
    "    image = torchvision.io.read_image(f'C:\\\\Users\\\\jayor\\\\Documents\\\\repos\\\\synth-reconstruct\\\\demo\\\\spectrograms_small\\\\{preset_name}.png')\n",
    "    image = image.float() / 255\n",
    "    # put on cuda\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    print('image shape: ', image.shape)\n",
    "\n",
    "    # Make prediction\n",
    "    outputs = model(image)\n",
    "\n",
    "    true = pd.read_csv('../presets.csv')\n",
    "    # add column names\n",
    "    true.columns = ['preset'] + [f'param_{i}' for i in range(89)]\n",
    "    # Get the true values for the preset\n",
    "    true = true.loc[true['preset'] == preset_name].values[0][1:-1]\n",
    "    # convert eachv value to a float\n",
    "    true = [float(i) for i in true]\n",
    "    print('true: ', true)\n",
    "\n",
    "    # remove outputs dimension\n",
    "    \n",
    "    print('outputs: ', outputs)\n",
    "    outputs = outputs.squeeze().cpu().detach().numpy()\n",
    "    # Compare difference between true and predicted values\n",
    "    diff = numpy.subtract(true, outputs)\n",
    "    print('diff: ', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  torch.Size([1, 4, 128, 128])\n",
      "true:  [0.1128138127399065, 0.5200412343305395, 0.5048691076285318, 0.0098827374404064, 0.4772178795809715, 0.3488458046957011, 0.183505796322272, 0.3968369440561221, 0.6945592819389794, 0.9378262666849476, 0.6411993774368219, 0.5091637260096223, 0.1697929968357902, 0.9824074326248594, 0.0395036241251579, 0.542074222745996, 0.1334788097457471, 0.6815959430836447, 0.5460944800568336, 0.2981876381147958, 0.4996742123917477, 0.4040111580053628, 0.2657535266387455, 0.3810113867874748, 0.9467943466948706, 0.8345796400867546, 0.6186174672661281, 0.1138328860685606, 0.288204690520314, 0.5296385118662379, 0.1404540087475387, 0.3565403072834314, 0.4598743762441755, 0.8915992751517887, 0.405066379256584, 0.0039768757249234, 0.6975559617956802, 0.5242822477626187, 0.2416376932942164, 0.6048245719420646, 0.2560140778332006, 0.6419355024530109, 0.0380844677095567, 0.6717362138548425, 0.8425245601629546, 0.1023571953117167, 0.8196847660378422, 0.1055024568401072, 0.1385104500946979, 0.1948652709561426, 0.6219107363269863, 0.2352478111063053, 0.4687706152290036, 0.0153224911882089, 0.5829675610722876, 0.4647076719238115, 0.0135782366812172, 0.3649272893450538, 0.5005393053063596, 0.1834076206320888, 0.3995366236650095, 0.6973480127498931, 0.1457826062839496, 0.9966454524271958, 0.8750027186463544, 0.7171260121053862, 0.9114859759242184, 0.9386964923570282, 0.0966728437632671, 0.4133690173276103, 0.7063439681448845, 0.0755762309560894, 0.0988570441765144, 0.3618769222184634, 0.4116829698110327, 0.9606583416526044, 0.0895490524849732, 0.2699997767670847, 0.5626761249354777, 0.6621147647908799, 0.0244535458216597, 0.4346045425662926, 0.6525732501094379, 0.8742473199349984, 0.0073750196498799, 0.5262040044251731, 0.3890294014651541, 0.4555963565269478]\n",
      "outputs:  tensor([[0.1119]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "diff:  [ 0.00089402  0.40812144  0.39294932 -0.10203705  0.36529809  0.23692601\n",
      "  0.07158601  0.28491715  0.58263949  0.82590648  0.52927959  0.39724394\n",
      "  0.05787321  0.87048764 -0.07241617  0.43015443  0.02155902  0.56967615\n",
      "  0.43417469  0.18626785  0.38775442  0.29209137  0.15383374  0.2690916\n",
      "  0.83487456  0.72265985  0.50669768  0.0019131   0.1762849   0.41771872\n",
      "  0.02853422  0.24462052  0.34795459  0.77967948  0.29314659 -0.10794291\n",
      "  0.58563617  0.41236246  0.1297179   0.49290478  0.14409429  0.53001571\n",
      " -0.07383532  0.55981642  0.73060477 -0.0095626   0.70776498 -0.00641733\n",
      "  0.02659066  0.08294548  0.50999095  0.12332802  0.35685082 -0.0965973\n",
      "  0.47104777  0.35278788 -0.09834155  0.2530075   0.38861951  0.07148783\n",
      "  0.28761683  0.58542822  0.03386282  0.88472566  0.76308293  0.60520622\n",
      "  0.79956619  0.8267767  -0.01524695  0.30144923  0.59442418 -0.03634356\n",
      " -0.01306275  0.24995713  0.29976318  0.84873855 -0.02237074  0.15807999\n",
      "  0.45075633  0.55019497 -0.08746624  0.32268475  0.54065346  0.76232753\n",
      " -0.10454477  0.41428421  0.27710961  0.34367657]\n"
     ]
    }
   ],
   "source": [
    "predict(model_ld, 'preset_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-reconstruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
